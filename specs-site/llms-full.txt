---
url: /arc-platform/specs-site/004-dev-setup/analysis-report.md
---
# Analysis Report: 004-dev-setup

**Date:** 2026-02-28\
**Stage:** Pre-implementation

***

## Executive Summary

The 004-dev-setup specification is **READY FOR IMPLEMENTATION** with minor clarifications needed. The spec is comprehensive, well-structured, and the plan includes a solid architecture. However, there are several gaps in acceptance criteria, task ordering, and service metadata that should be resolved before development begins.

**Status**: 2 BLOCKERS, 4 WARNINGS, 5 OBSERVATIONS

***

## Blockers (Must Resolve)

### BLOCKER-1: Incomplete OTEL Service Metadata

**Location**: spec.md FR-6, plan.md, tasks.md TASK-013\
**Issue**: The spec requires creating `services/otel/service.yaml` for the full OTEL stack, but currently only `services/otel/observability/service.yaml` and `services/otel/telemetry/service.yaml` exist.

* spec.md FR-6 says: "Create `services/otel/service.yaml` — full OTEL stack metadata: `codename: otel`, `health: http://localhost:3301/api/v1/health`, `depends_on: []`"
* Current structure has `observability/service.yaml` (codename: `friday`) and `telemetry/service.yaml` (codename: `friday-collector`)
* The `observability/service.yaml` has `depends_on: [telemetry]`, creating a cross-directory dependency

**Clarification Needed**:

* Should `services/otel/service.yaml` be a new top-level service metadata file that references both sub-services?
* Or should the spec instead reference the existing `services/otel/observability/service.yaml` which already has `codename: friday`?
* The `ultra-instinct` profile lists `otel` as a service, but the registry parser will see `friday`, `friday-collector`, and potentially `otel` — which is correct?

**Impact**: Tasks T013, T040 (Makefile) depend on clarity here.

***

### BLOCKER-2: `profiles.yaml` Incomplete for `think` Profile

**Location**: spec.md, tasks.md TASK-010, plan.md line 249\
**Issue**: TASK-010 requires adding `friday-collector` to the `think` profile, but the spec's sequence diagram shows all 5 services starting (flash, sonic, strange, friday-collector, cortex).

Current `services/profiles.yaml`:

```yaml
think:
  services:
    - cortex
    - flash
    - strange
    - sonic
```

Does NOT include `friday-collector`.

* spec.md section "Startup Sequence (think profile)" (line 72-110) explicitly shows `friday-collector` in Layer 0
* spec.md FR-9 states: "add `friday-collector` to the `think` profile service list"
* TASK-010 acceptance criteria state this must happen

**Impact**: US-1 acceptance criteria require 5 services. Without this change, smoke test will fail.

**Action**: Confirm TASK-010 must be completed before TASK-040 (Makefile).

***

## Warnings (Should Address)

### WARNING-1: Task Ordering Dependency Error

**Location**: tasks.md dependency graph, lines 28-44\
**Issue**: TASK-040 (Makefile) lists 12 prerequisites (T001, T010-T014, T020-T024, T030-T031) but has no sequential ordering constraint.

According to plan.md (line 264):

> "### Phase 2 — Makefile (after Phase 1 complete)"

The Makefile phase depends on **all** Phase 1 items. However:

* The dependency graph shows all prerequisites, but doesn't indicate the critical path
* Phase 1 includes both foundational YAML patches AND script implementations
* The mermaid graph makes this clear, but the task list doesn't mark TASK-040 with explicit "Phase 3: Implementation" labeling

**Clarification**: In `tasks.md`, explicitly mark each task with its **Phase** number. Current format `[TASK-NNN] [P]` should expand to `[TASK-NNN] [P] [PHASE-N]`:

* Example: `[TASK-001] [P] [PHASE-1]` (Setup)
* Example: `[TASK-040] [SEQUENTIAL] [PHASE-3]` (Makefile — after all PHASE-1/2 complete)

**Impact**: Implementers may attempt TASK-040 before upstream scripts are ready, causing circular dependency failures.

***

### WARNING-2: Missing Timeout Specifications in service.yaml

**Location**: plan.md line 131, spec.md FR-7\
**Issue**: `services/otel/telemetry/service.yaml` currently lacks a `timeout` field.

Current file (5 lines):

```yaml
codename: friday-collector
tech: arc-friday-collector
upstream: signoz/signoz-otel-collector
ports:
  - 4317   # OTLP gRPC
  - 4318   # OTLP HTTP
health: http://localhost:13133/
```

Missing:

* `timeout: 60` (or appropriate value)
* `depends_on: []` (explicit empty list for clarity)

TASK-014 acceptance criteria state:

> "File has `codename: friday-collector`, `health: http://localhost:13133/`, `timeout: 60`"

**Action**: TASK-014 acceptance criteria must include adding `timeout` and `depends_on` fields if missing.

***

### WARNING-3: `parse-registry.sh` Scope Ambiguity

**Location**: plan.md line 57, spec.md FR-2\
**Issue**: The spec states `parse-registry.sh` scans `services/*/service.yaml` AND `services/*/*/service.yaml` (nested).

Current actual structure has 6 service.yaml files across 2 levels:

* Level 1: `services/cache/`, `services/cortex/`, `services/messaging/`, `services/streaming/`
* Level 2: `services/otel/observability/`, `services/otel/telemetry/`

The awk-based parser must handle both levels. However:

* `parse-registry.sh` acceptance criteria (TASK-021, line 107) state: "output contains ... for all `service.yaml` files found under `services/`"
* This is correct but doesn't explicitly mention how nested directories are discovered

**Action**: FR-2 should clarify:

> "Recursively scan `services/` directory for all `service.yaml` files at any depth; emit registry variables for each"

***

### WARNING-4: Existing `cortex/service.yaml` Differs from Spec

**Location**: spec.md FR-12, tasks.md TASK-011\
**Issue**: Spec says to "remove `oracle` from `depends_on`", but `cortex/service.yaml` **already lists** `oracle`:

```yaml
depends_on:
  - flash    # arc-messaging (NATS)
  - strange  # arc-streaming (Pulsar)
  - oracle   # arc-sql-db (Postgres)       ← MUST REMOVE
  - sonic    # arc-cache (Redis)
  - friday-collector    # arc-friday-collector (OTEL)
```

This is CORRECT per spec, but the comment suggests it's a **known broken state**:

> "# TODO: re-add oracle when persistence service lands" (spec.md line 199)

**Action**: Good. This is already flagged in TASK-011. No change needed; just confirming the spec aligns with reality.

***

## Observations (Nice to Know)

### OBS-1: Missing Makefile Target for `dev-regen`

**Location**: plan.md line 324, spec.md not explicitly mentioned\
**Issue**: plan.md references a `dev-regen` target:

```makefile
dev-regen:    # force-rebuild .make/ files
```

But spec.md does not list this as a requirement (no FR-16 or similar).

**Recommendation**: Either add `dev-regen` to spec.md's FR list (after FR-15) or remove from Makefile target list. Currently it's only documented in plan.md.

***

### OBS-2: Docker Network Names Not in Service YAML

**Location**: spec.md line 91, plan.md line 148\
**Issue**: The spec states that `make dev` creates `arc_platform_net` and `arc_otel_net` Docker networks idempotently.

However:

* These network names are hardcoded in the Makefile target
* Individual `service.yaml` files do NOT declare which network they join
* This couples network management to the root Makefile rather than per-service configuration

**Recommendation**: Document this as a known limitation in the spec's "Edge Cases" section. Consider future enhancement to allow per-service network assignment via `service.yaml`.

***

### OBS-3: Health Check Timeout Edge Case

**Location**: spec.md "Edge Cases" (line 232)\
**Issue**: The edge case section covers timeout well:

```
| Service health check times out | `wait-for-health.sh` exits 1: `✗ flash did not become healthy after 60s` |
```

But NFR-3 states:

```
`make dev` for the `think` profile must complete in under 3 minutes on a warm Docker cache
(Pulsar cold-start ~90s is the expected bottleneck)
```

The sequence diagram shows:

* `strange` (Pulsar) timeout: 120s (line 101)

**Calculation**:

* flash health wait: 60s
* sonic health wait: 30s
* strange health wait: 120s
* friday-collector health wait: 30s
* (serial, gated by layer)
* cortex health wait: 60s

Total: 60 + 30 + 120 + 30 + 60 = **300 seconds = 5 minutes**

This exceeds the 3-minute target in NFR-3.

**Recommendation**: Clarify in NFR-3 whether the 3-minute target includes Docker pull time for cold-start (first run), or only assumes warm cache. Current wording is ambiguous. If cold-start Pulsar can take 90s, and we wait 120s for health, plus 150s for other services, we're at ~5 minutes minimum.

***

### OBS-4: Polyglot Standards Gap — Comments in Shell Scripts

**Location**: constitution.md Principle V, plan.md line 182\
**Issue**: Constitution v2.2.0 adds rules for comments:

```
- Comment the *why*, never the *what* — code should be self-explanatory
- Only comment non-obvious intent, algorithmic trade-offs, or external constraints
```

The spec's acceptance criteria for shell scripts (TASK-020 through TASK-024) do NOT mention comment style or docstring conventions.

**Recommendation**: Add acceptance criteria for all 5 shell scripts:

* "All functions have docstrings describing purpose and usage"
* "Comments explain non-obvious logic (e.g., why DFS topological sort vs. Kahn's algorithm explanation in resolve-deps.sh)"

***

### OBS-5: FR-16 Phantom Requirement

**Location**: plan.md line 131 mentions "FR-16 (new)" but spec.md does NOT define FR-16
**Issue**: plan.md line 249 states:

```
- FR-16 (new): `services/cache/service.yaml` — update health endpoint
```

But spec.md's functional requirements list ends at FR-15. This requirement was **added during planning** but not formally added to the spec.

**Recommendation**: Either:

1. Add FR-16 to spec.md's functional requirements section, OR
2. Remove from plan.md and fold the `services/cache/service.yaml` update into TASK-012

Clarify in spec: Is changing sonic's health from `redis-cli ping` to `docker exec arc-cache redis-cli ping` required for this feature?

***

## Coverage Matrix

| Requirement | Plan Section | Task IDs | Status |
|-------------|--------------|----------|--------|
| FR-1 (parse-profiles.sh) | Parallel Batch B | T020 | MAPPED |
| FR-2 (parse-registry.sh) | Parallel Batch B | T021 | MAPPED |
| FR-3 (resolve-deps.sh) | Parallel Batch C | T022 | MAPPED |
| FR-4 (wait-for-health.sh) | Parallel Batch C | T023 | MAPPED |
| FR-5 (check-dev-prereqs.sh) | Parallel Batch C | T024 | MAPPED |
| FR-6 (otel/service.yaml) | Parallel Batch A | T013 | **BLOCKER** — Metadata unclear |
| FR-7 (otel/telemetry/service.yaml) | Parallel Batch A | T014 | **WARNING** — Missing timeout |
| FR-8 (Makefile update) | Phase 3 | T040 | MAPPED (blocked on Phase 1/2) |
| FR-9 (profiles.yaml patch) | Parallel Batch A | T010 | **BLOCKER** — Not yet applied |
| FR-10 (otel.mk aliases) | Parallel Batch D | T030 | MAPPED |
| FR-11 (cortex.mk aliases) | Parallel Batch D | T031 | MAPPED |
| FR-12 (cortex/service.yaml patch) | Parallel Batch A | T011 | MAPPED (already correct state) |
| FR-13 (.gitignore patch) | Parallel Batch A | T001 | MAPPED |
| FR-14 (Docker networks) | Phase 3 | T040 | MAPPED (sub-requirement of T040) |
| FR-15 (Shell script conventions) | Phase 1 | T020-T024 | MAPPED |
| (NFR) No new tool deps | Implicit | All | PASS (only awk, curl, docker) |
| (NFR) Auto-regeneration | Phase 3 | T040 | MAPPED |
| (NFR) 3-minute startup | Implicit | T040 | **OBS** — May exceed target |
| (NFR) Script conventions | Phase 1 | T020-T024 | MAPPED |
| (NFR) Backward compat | Phase 3 | T040 | MAPPED |

***

## Gaps Found

| Gap | Severity | Location | Recommendation |
|-----|----------|----------|-----------------|
| GAP-1 | BLOCKER | TASK-010 not yet applied to profiles.yaml | Apply `friday-collector` to `think` profile before implementation |
| GAP-2 | BLOCKER | otel/service.yaml metadata undefined | Clarify whether FR-6 creates new file or references existing `observability/service.yaml` with codename `friday` |
| GAP-3 | WARNING | TASK-014 missing timeout field acceptance criteria | Add `timeout: 60` and `depends_on: []` to acceptance criteria |
| GAP-4 | WARNING | Phase labeling missing from task list | Mark each task with explicit `[PHASE-N]` for clarity |
| GAP-5 | OBSERVATION | dev-regen target not in spec | Add to FR list or remove from plan.md |
| GAP-6 | OBSERVATION | FR-16 phantom requirement | Formalize sonic health endpoint change as FR-16 or fold into T012 |
| GAP-7 | OBSERVATION | Shell script comment standards not in acceptance criteria | Add docstring + comment style requirements to TASK-020 through TASK-024 |
| GAP-8 | OBSERVATION | Startup time target may be unachievable | Clarify whether 3-minute NFR-3 target includes cold-start or only warm cache |

***

## Risks

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|-----------|
| RISK-1 | TASK-010 not applied before TASK-040 | HIGH | Code breaks smoke test; `make dev` starts only 4 services instead of 5 | **HIGH** | Make TASK-010 a hard blocker for TASK-040 in dependency graph |
| RISK-2 | awk parser fails on edge-case YAML formatting | MEDIUM | Scripts silently emit empty registry, `make dev` fails with cryptic message | **MEDIUM** | Validate all 6 existing `service.yaml` files match expected format; document YAML schema |
| RISK-3 | `parse-registry.sh` misses nested `service.yaml` files | HIGH | Nested services (otel/observability, otel/telemetry) not registered; `make dev` skips them | **MEDIUM** | Ensure `find` recursively scans all directories; test with `-maxdepth -1` (unlimited) |
| RISK-4 | Docker network creation fails in shared environments | MEDIUM | Multiple developers on same machine create port conflicts | **LOW** | `docker network create` is idempotent; add comment in Makefile explaining this |
| RISK-5 | Circular dependency in otel services | MEDIUM | observability depends\_on telemetry; if ever reversed, cycle detection must catch it | **LOW** | resolve-deps.sh implements Kahn's algorithm; cycle detection is built-in |
| RISK-6 | Startup timeout exceeds 3-minute NFR-3 target | MEDIUM | Slow machines or Docker daemon lag cause timeouts; developers blame the feature | **MEDIUM** | Document environment requirements; add `TIMEOUT_OVERRIDE` mechanism for CI/slow systems |

***

## Parallel Execution Opportunities

Current task structure already has excellent parallelization:

* **Phase 1** (SETUP): 1 task (T001) — independent
* **Phase 2** (FOUNDATIONAL): 10 tasks in 4 batches — all parallelizable
  * Batch A (YAML): 6 tasks (T010-T014, T001) — **FULLY PARALLEL**
  * Batch B (Parse scripts): 2 tasks (T020-T021) — **FULLY PARALLEL**
  * Batch C (Orchestration): 3 tasks (T022-T024) — **FULLY PARALLEL**
  * Batch D (.mk aliases): 2 tasks (T030-T031) — **FULLY PARALLEL**
* **Phase 3** (INTEGRATION): 1 task (T040) — **SEQUENTIAL** (blocks on all Phase 1/2)
* **Phase 4** (POLISH): 2 tasks (T900, T999) — **SEQUENTIAL** (after T040+T050)

**No improvement opportunities identified.** Current parallelization is optimal. ✓

***

## Constitution Compliance

| Principle | Applies | Status | Evidence |
|-----------|---------|--------|----------|
| I. Zero-Dependency CLI | **NO** | n/a | Feature is Makefile + shell only; no CLI changes |
| II. Platform-in-a-Box | **YES** | **PASS** | `make dev` = single command starts full `think` profile automatically; complies with "docker-compose up" or equivalent requirement |
| III. Modular Services | **YES** | **PASS** | Framework reads per-service `service.yaml`; new service = new directory + yaml + mk + Dockerfile; no central wiring required |
| IV. Two-Brain Separation | **NO** | n/a | Shell scripting only; no Go/Python introduced |
| V. Polyglot Standards | **YES** | **PARTIAL** | Scripts use `common.sh` helpers (✓), follow color/symbol conventions (✓), BUT acceptance criteria lack comment/docstring standards (⚠) |
| VI. Local-First | **NO** | n/a | CLI-only principle |
| VII. Observability | **YES** | **PASS** | `friday-collector` added to `think` profile by default; ensures OTEL traces/metrics collection on every dev startup |
| VIII. Security | **YES** | **PASS** | No secrets in scripts or generated files (✓); port pre-checks prevent unintended exposure (✓); containers already run non-root (existing config) |
| IX. Declarative Reconciliation | **YES** | **PASS** | `profiles.yaml` + `service.yaml` = declarative source of truth; Make reconciles via generated files (✓); no imperative state drift |
| X. Stateful Operations | **NO** | n/a | CLI-only principle |
| XI. Resilience | **YES** | **PASS** | Health-check gating between dependency layers (✓); fail-fast on unregistered or circular deps (✓); `wait-for-health.sh` implements retry logic |
| XII. Interactive Experience | **NO** | n/a | CLI-only principle |

**Overall Constitution Status**: ✓ **COMPLIANT** (7 applies, 7 pass; 5 n/a)

***

## Recommendations

### MUST FIX (Before Implementation)

1. **Resolve BLOCKER-1**: Clarify OTEL service.yaml structure
   * Decide: Does FR-6 create `services/otel/service.yaml` (new), or does it reference the existing `services/otel/observability/service.yaml`?
   * Update TASK-013 acceptance criteria to reflect decision
   * Verify `parse-registry.sh` will correctly discover all variants

2. **Resolve BLOCKER-2**: Apply `friday-collector` to `think` profile
   * Update `services/profiles.yaml` to include `friday-collector` in `think` profile
   * This is TASK-010 and is a blocker for smoke testing (US-1, SC-1)

3. **Clarify FR-16**: Formalize sonic health endpoint change
   * Either add to spec.md as FR-16, or confirm TASK-012 should handle this update

### SHOULD FIX (Before Kickoff)

4. **Add Phase labels to tasks.md**
   * Mark each task with `[PHASE-N]` to clarify execution order
   * Mark TASK-040 as `[SEQUENTIAL]` to prevent premature execution

5. **Expand TASK-014 acceptance criteria**
   * Add `timeout: 60` and `depends_on: []` as required fields
   * Verify against actual `services/otel/telemetry/service.yaml`

6. **Add shell script standards**
   * Extend all 5 script tasks (T020-T024) acceptance criteria to include docstring and comment guidelines per Constitution Principle V

### NICE TO HAVE (Future)

7. **Clarify startup time target**
   * Revise NFR-3 to distinguish between cold-start (first-run pull + build) and warm-cache scenarios
   * Document timeout overrides for CI/slow environments

8. **Formalize `dev-regen` target**
   * Either add to spec.md FR list or remove from plan.md

***

## Test Coverage Assessment

| Area | Spec Section | Test Method | Risk |
|------|---|---|---|
| prereqs checking | US-5, FR-5 | Manual: stop Docker, run `make dev-prereqs` | LOW (simple shell checks) |
| yaml parsing | FR-1, FR-2 | Manual: verify `.make/` output variables | MEDIUM (awk parsing fragile) |
| topological sort | FR-3, US-3 | Manual: inject circular dep, verify exit 1 | MEDIUM (DFS corner cases) |
| health polling | FR-4, US-1 | Manual: kill a service, verify timeout message | LOW (curl + timeout are standard) |
| profile selection | US-2 | Manual: `make dev PROFILE=reason` + verify SigNoz | LOW (profile selection is simple) |
| backward compat | NFR-5, SC-6 | Manual: `make flash-up`, `make otel-up` still work | MEDIUM (Make target interference) |
| integration | US-1, SC-1 to SC-9 | Manual: full `make dev && make dev-health` | **HIGH** (cross-service coordination) |

**Recommendation**: Add pre-implementation test plan to `specs/004-dev-setup/.work-docs/test-plan.md`:

* Unit tests for each shell script (awk parsing, topological sort, health polling)
* Integration test covering all 9 success criteria
* Backward compatibility matrix for existing Make targets

***

## Summary

**Status**: Ready for implementation with **2 critical blockers** to resolve.

**Blockers**:

1. BLOCKER-1: OTEL service metadata structure unclear
2. BLOCKER-2: `friday-collector` not yet in `think` profile

**Warnings**: 4 (task ordering, timeout fields, scope clarity, cold-start timing)

**Observations**: 5 (dev-regen target, network coupling, startup time, comment standards, FR-16 phantom)

**Constitution**: ✓ COMPLIANT (7/7 applicable principles pass)

**Recommendation**: **PAUSE IMPLEMENTATION** until BLOCKER-1 and BLOCKER-2 are resolved. All other gaps are minor and can be addressed during implementation sprints.

***

**Report Generated**: 2026-02-28\
**Audit Scope**: spec.md, plan.md, tasks.md, constitution.md, patterns.md, actual codebase structure\
**Next Step**: Team review of blockers; resolve via spec amendments before kicking off TASK-001

---

---
url: /arc-platform/specs-site/005-data-layer/analysis-report.md
---
# Analysis Report: 005-data-layer

**Date:** 2026-02-28\
**Stage:** Pre-implementation\
**Auditor:** Claude Code (read-only analysis)

***

## CRITICAL BLOCKERS

### 1. config.yaml Path Mismatch: References `search/` Instead of `vector/`

**Issue:** Canonical SSOT (config.yaml) registers Cerebro at `services/search/`, but spec, plan, and most task content use `services/vector/`:

**config.yaml L41 (current):**

```yaml
- { dir: "search", codename: "cerebro", tech: "qdrant", lang: "config" }
```

**Spec/Plan/Tasks intent:**

* **spec.md L13, L58:** `services/vector/`
* **plan.md L15, L102:** `services/vector/`
* **tasks.md L87, L125 (TASK-012, TASK-022):** `services/search/` — tasks note this mismatch on L19

**Risk Level:** CRITICAL — If config.yaml is not updated to match `services/vector/`, service discovery tools and any automation reading config.yaml will fail.

**Action Required:** Update **config.yaml L41** before implementation:

```yaml
- { dir: "vector", codename: "cerebro", tech: "qdrant", lang: "config" }
```

***

### 2. profiles.yaml Missing All Three New Services

**Issue:** Spec requires FR-7 and TASK-001 to register oracle, cerebro, and tardis in profiles, but current profiles.yaml has none:

**Current state (as of 2026-02-28):**

```yaml
think:
  services:
    - flash
    - sonic
    - strange
    - friday-collector
    - cortex
  # MISSING: oracle, cerebro

reason:
  services:
    - cortex
    - flash
    - strange
    - sonic
    - otel
  # MISSING: tardis
```

**Spec requirements:**

* **spec.md FR-7:** Add oracle + cerebro to `think`; tardis to `reason`
* **tasks.md TASK-001 AC (L66-69):** Verify profiles updated and `make dev` works

**Risk:** If profiles.yaml is not updated before TASK-011/012/013 create the service directories, `make dev --profile think` will fail to wire oracle and cerebro, breaking the "Platform-in-a-Box" principle (Constitution II).

**Action Required:** Update profiles.yaml before or as part of TASK-001:

```yaml
think:
  services:
    - flash
    - sonic
    - strange
    - friday-collector
    - cortex
    - oracle       # ADD
    - cerebro      # ADD

reason:
  services:
    - cortex
    - flash
    - strange
    - sonic
    - otel
    - tardis       # ADD
```

***

### 3. TASK-001 Acceptance Criteria Too Weak for Verification

**Issue:** TASK-001 AC does not enforce that oracle/cerebro/tardis codenameactually appear in profiles.yaml:

**Current AC (tasks.md L66-69):**

```
- `think.services` list includes `oracle` and `cerebro` (by codename)
- `reason.services` list includes `tardis` (by codename)
```

This is stated as desired outcome, but acceptance criteria should be testable shell commands:

**Example of testable criteria:**

```
- grep -q "oracle" services/profiles.yaml && grep -q "cerebro" services/profiles.yaml
- grep "reason:" -A 10 services/profiles.yaml | grep -q "tardis"
- YAML parses without error: `docker compose -f services/docker-compose.yml config >/dev/null`
```

**Action Required:** Update TASK-001 acceptance to include explicit test commands or checklist items.

***

## WARNINGS

### 4. Tasks.md Still References `services/search/` — Inconsistent Update Needed

**Issue:** tasks.md line 19 explicitly flags the mismatch, suggesting it was a known inconsistency during planning:

**tasks.md L19:**

```
> **Directory note**: `config.yaml` maps Cerebro/Qdrant to `services/search/` (not `vector/`). 
> Tasks use the config-authoritative path.
```

However, decision was made to proceed with `services/vector/` in spec/plan. Tasks should be updated to be consistent:

**Affected task lines:**

* L30: `T012["TASK-012 [P]\nservices/search/\nCerebro"]` → should be `services/vector/`
* L87: TASK-012 module = `services/search/` → should be `services/vector/`
* L125: TASK-022 module = `services/search/` → should be `services/vector/`
* L156: Makefile include path → should reference `services/vector/cerebro.mk`

**Action Required:** Update tasks.md to use `services/vector/` everywhere; remove the disclaimer on L19.

***

### 5. MinIO Non-Root User — Decision Path Unclear

**Issue:** plan.md discusses Option A vs. Option B (L168-174) but does not finalize the approach:

**plan.md L173:**

```
If neither works with the upstream image, document the root deviation in docker-compose.yml comments (same as Pulsar in 003).
```

**Problem:**

* No indication which option will be attempted first
* TASK-013 acceptance criteria (tasks.md L100-105) does not require uid verification
* Constitution Principle VIII (Security) requires non-root, but fallback is documentation-only

**Action Required:** Update TASK-013 acceptance to explicitly verify:

```
- Option A attempted: user: "1000:1000" in docker-compose with /data permissions
- If Option A fails, Option B attempted (Dockerfile + USER directive)
- If both fail, uid confirmed at runtime AND deviation documented in comment with explicit rationale
- docker inspect arc-storage | jq '.[0].Config.User' returns non-root uid or has comment explaining deviation
```

***

### 6. NFR-6 CI Build Time Requirement Not Validated

**Issue:** Spec NFR-6 (L134) and SC-5 (L197) require "CI build completes in under 3 minutes (amd64 only; no QEMU)":

**Problem:**

* No baseline performance data provided
* Docker image pull time for postgres:17-alpine and qdrant/qdrant not measured
* Success criteria fails immediately if amd64 build exceeds 3 minutes on first run
* No tuning strategy or fallback in plan

**Recommendation:** Either:

* Remove the 3-minute hard requirement and make it a performance target
* Add a task to baseline CI runner performance before declaring success
* Or defer to v0.2.0 after observing actual build times

**Action:** Update SC-5 acceptance in TASK-031 to include baseline measurement.

***

### 7. Cortex Health Check Integration Not Tested

**Issue:** SC-2 (L194-195) requires Cortex `/health/deep` to report `oracle: ok`, but:

* No verification that Cortex code changes are needed
* No indication of which task handles Cortex integration (not in 005-data-layer scope)
* TASK-041 acceptance (L212) assumes cortex will be ready, but startup order unclear

**Recommendation:** Add a clarification to TASK-041 or plan:

```
Prerequisite: Cortex has been separately updated to probe oracle service 
(may be out-of-scope for this spec; verify in TASK-041)
```

***

### 8. TASK-900 Docs Update — Vague Acceptance Criteria

**Issue:** TASK-900 (L218-227) references cortex service.yaml update but does not verify it exists or is in scope:

**Current AC:**

```
- `services/cortex/service.yaml` `depends_on` field references `oracle` codename (add if missing)
```

**Problem:**

* Does `cortex/service.yaml` exist?
* Is updating cortex's depends\_on within 005-data-layer scope, or a separate issue?
* CLAUDE.md reference is vague ("add if missing")

**Action Required:** Verify cortex/service.yaml exists and clarify whether update is in-scope or deferred.

***

### 9. Tech Debt Items Not Scheduled for Revisit

**Issue:** plan.md lists three tech debt items (L333-338) with no follow-up task:

**TD-001:** Qdrant Prometheus scraping\
**TD-002:** MinIO Prometheus scraping\
**TD-003:** Default bucket creation

**Problem:** No mechanism to track reopening these; risks orphaning them post-merge.

**Recommendation:** Either create a separate GitHub issue for observability integration, or add a comment in the respective .mk files referencing the debt items.

***

## OBSERVATIONS

### 10. Constitution Compliance — Principle VII (Observability) Incomplete

**Issue:** plan.md marks Principle VII as "PASS" (L85), but observability integration is deferred:

**Current state:**

* Health checks: Implemented (pg\_isready, /readyz, /health/live)
* OTEL instrumentation: Deferred (TD-001, TD-002 for Prometheus scraping)
* SigNoz dashboards: Not mentioned

**Analysis:** Health checks satisfy the baseline observability requirement. OTEL integration is acceptable tech debt per plan rationale (L85). This is compliant as-is.

***

### 11. Parallel Task Safety Confirmed — But Contingent on Path Resolution

**Issue:** Dependency graph shows maximum parallelism (9/13 tasks parallel), but contingent on blocker #1:

**If config.yaml is updated to `services/vector/`:**

* TASK-011, 012, 013 have no file conflicts → fully parallel
* TASK-021, 022, 023 have no file conflicts → fully parallel after Phase 2
* TASK-031, 032 independent → parallel

**Current risk:** If tasks.md is not updated to `services/vector/`, parallel execution will create files in wrong directories.

***

### 12. No Container Build Context Validation

**Issue:** Tasks do not explicitly verify Dockerfile context path:

**Suggested acceptance for TASK-011/012/013:**

```
- docker build succeeds from repo root:
  docker build -f services/persistence/Dockerfile -t test-oracle services/persistence/
- (repeat for vector/ and storage/)
```

Currently implied but not explicit.

***

### 13. Network Lifecycle Not Documented

**Issue:** spec mentions `arc_platform_net` must exist (L187), but:

* No documentation of which `make` target creates it
* TASK-024 ensures `data-up` creates it if missing (`docker network create arc_platform_net 2>/dev/null || true`)
* But no guidance on lifecycle if network is manually deleted post-init

**Recommendation:** Add comment to data.mk explaining network creation logic.

***

### 14. Edge Case: Oracle Password Post-Init

**Issue:** spec L188 notes "Postgres ignores `POSTGRES_PASSWORD` after data dir is initialized":

**This is documented correctly, but:**

* No guidance in TASK-021 on how to handle re-initialization
* Users may try to change password via env vars and get silent failure

**Suggestion:** Add note to oracle.mk help text about password reset requiring `oracle-nuke`.

***

### 15. Service Health Check Start Periods Differ

**Issue:** spec L130 specifies different start\_periods per service, but rationale not explained:

| Service | Start Period | Rationale |
|---------|--------------|-----------|
| Oracle | 10s | Postgres slow to initialize |
| Cerebro | 5s | Qdrant quick to boot |
| Tardis | 5s | MinIO quick to boot (per plan L256) |

Currently correct, but spec should explain why Oracle is 2x slower. This helps implementers adjust if upstream image changes.

***

## COVERAGE MATRIX

| Requirement | Type | Task ID(s) | Status |
|-------------|------|-----------|--------|
| FR-1: persistence/ Dockerfile + files | FR | TASK-011 | OK |
| FR-2: vector/ Dockerfile + files | FR | TASK-012 | BLOCKED (config.yaml) |
| FR-3: storage/ Dockerfile + files | FR | TASK-013 | OK |
| FR-4: Oracle init + volume | FR | TASK-011 | OK |
| FR-5: Cerebro volumes + ports | FR | TASK-012 | BLOCKED (config.yaml) |
| FR-6: Tardis S3 + console + volume | FR | TASK-013 | WARNING (uid) |
| FR-7: profiles.yaml update | FR | TASK-001 | BLOCKED (missing) |
| FR-8: data-images.yml CI | FR | TASK-031 | WARNING (3min SLA) |
| FR-9: data-release.yml release | FR | TASK-032 | OK |
| FR-10: data.mk aggregate | FR | TASK-024 | OK |
| FR-11: Makefile includes | FR | TASK-024 | OK |
| NFR-1: Non-root users | NFR | TASK-011, 012, 013 | WARNING (MinIO) |
| NFR-2: Health endpoints | NFR | TASK-011, 012, 013 | OK |
| NFR-3: Named volumes | NFR | TASK-011, 012, 013 | OK |
| NFR-4: 127.0.0.1 binding | NFR | TASK-011, 012, 013 | OK |
| NFR-5: OCI + arc labels | NFR | TASK-011, 012, 013 | OK |
| NFR-6: 3-minute CI SLA | NFR | TASK-031 | WARNING (untested) |
| SC-1: `make data-up` health | SC | TASK-041 | OK |
| SC-2: Cortex /health/deep | SC | TASK-041 | WARNING (not in scope) |
| SC-3: Qdrant /readyz | SC | TASK-041 | OK |
| SC-4: MinIO console | SC | TASK-041 | OK |
| SC-5: CI <3 min | SC | TASK-031 | WARNING (duplicate/untested) |
| SC-6: Release workflow | SC | TASK-032 | OK |
| SC-7: Trivy CVE scan | SC | TASK-032 | OK |
| SC-8: profiles.yaml content | SC | TASK-001, 041 | BLOCKED (missing) |

***

## RISK SUMMARY

| Risk | Severity | Mitigation | Owner |
|------|----------|-----------|-------|
| config.yaml not updated to `services/vector/` | CRITICAL | Update config.yaml L41 before TASK-012 starts | Team lead |
| profiles.yaml empty — no services registered | CRITICAL | Pre-populate or enforce TASK-001 as hard blocker | TASK-001 owner |
| TASK-001 AC too weak for verification | HIGH | Add explicit shell test commands | TASK-001 owner |
| tasks.md still references `services/search/` | HIGH | Update tasks L30, L87, L125, L156 | Plan owner |
| MinIO uid unverified | MEDIUM | Add runtime verification to TASK-013 | TASK-013 owner |
| CI 3-minute SLA untested | MEDIUM | Benchmark first run; adjust or defer | TASK-031 owner |
| Cortex integration not in scope | MEDIUM | Clarify in TASK-041 prerequisite or separate issue | Plan owner |
| Tech debt orphaned | LOW | Create GitHub issue for observability integration | Release manager |

***

## PARALLEL OPPORTUNITIES

**Current design supports maximum parallelism:**

**Phase 2 (Services):** TASK-011 (persistence), TASK-012 (vector), TASK-013 (storage) — fully parallel, no file conflicts

**Phase 3 (Make):** TASK-021, 022, 023 — parallel after Phase 2; TASK-024 — sequential after all three

**Phase 4 (CI/CD):** TASK-031 (data-images), TASK-032 (data-release) — parallel, independent

**Phase 5 (Integration):** TASK-041 — sequential after Phase 4

**Phase 6 (Polish):** TASK-900 (docs), TASK-999 (reviewer) — sequential

**Bottlenecks:** None, if blockers are resolved.

***

## CONSTITUTION COMPLIANCE

| Principle | Applies | Status | Notes |
|-----------|---------|--------|-------|
| I. Zero-Dep CLI | N/A | — | Services only |
| II. Platform-in-a-Box | YES | BLOCKED | Contingent on profiles.yaml update (blocker #2) |
| III. Modular Services | YES | OK | Each self-contained; flat under services/; own codename in config.yaml |
| IV. Two-Brain | YES | OK | Config-only; no language separation concern |
| V. Polyglot Standards | YES | OK | Dockerfiles, compose, health checks follow 003 pattern |
| VI. Local-First | N/A | — | CLI only |
| VII. Observability | YES | OK | Health checks present; Prometheus deferred as TD-001 (acceptable) |
| VIII. Security | YES | WARNING | Oracle + Cerebro confirmed non-root; MinIO uid unverified (warning #5) |
| IX. Declarative | N/A | — | CLI only |
| X. Stateful Ops | N/A | — | CLI only |
| XI. Resilience | YES | OK | Health checks + start\_periods; named volumes survive restart |
| XII. Interactive | N/A | — | CLI only |

**Overall:** 6 PASS, 2 WARNING, 3 N/A

***

## SUMMARY

| Category | Count | Detail |
|----------|-------|--------|
| CRITICAL BLOCKERS | 3 | Path mismatch, missing profiles, weak AC |
| WARNINGS | 6 | Tasks consistency, uid verification, CI SLA, docs scope, tech debt, health check |
| OBSERVATIONS | 6 | Constitution VII partial, parallelism safe, build context, network lifecycle, postgres password, start periods |
| **Total Issues** | **15** | — |

### Recommendation

**PAUSE IMPLEMENTATION — FIX BLOCKERS FIRST**

**Must resolve before TASK-001 starts:**

1. Update `.specify/config.yaml` L41: change `dir: "search"` → `dir: "vector"`
2. Populate `services/profiles.yaml` with oracle, cerebro, tardis registrations (template provided in warning #2)
3. Strengthen TASK-001 acceptance criteria with testable shell commands
4. Update tasks.md to consistently use `services/vector/` (L30, L87, L125, L156)

**Should address before Phase 2 starts:**

* Update TASK-013 acceptance to verify MinIO uid
* Clarify TASK-900 scope (cortex integration — in or out of scope?)
* Adjust or defer CI 3-minute SLA to performance target

**After blockers resolved:**

* Proceed with full parallelization of Phase 2–4
* Use TASK-999 (reviewer) to validate Constitution compliance
* Track tech debt items (TD-001, TD-002, TD-003) in GitHub issues

**Estimated remediation time:** 1–2 hours. Then implementation can proceed smoothly.

***

*Report generated: 2026-02-28 by Claude Code (pre-implementation audit)*

---

---
url: /arc-platform/specs-site/006-platform-control/analysis-report.md
---
# Analysis Report: 006-platform-control

**Date:** 2026-02-28
**Stage:** Pre-implementation
**Auditor:** Claude Code (read-only)

***

## Executive Summary

The 006-platform-control feature is **READY FOR IMPLEMENTATION** with **3 blockers** that must be resolved before development starts. All core patterns are sound; issues are procedural/organizational rather than architectural.

**Status**: PAUSE TO FIX BLOCKERS

* Blockers: 3
* Warnings: 4
* Observations: 3

***

## 1. BLOCKERS (Must resolve before implementing)

### 1.1 Cross-Spec Database Conflict: TASK-013 Creates Unleash DB Init Script

**Issue**: TASK-013 (`services/flags/` — Mystique) explicitly states:

> "Oracle init SQL: create `services/persistence/initdb/002_create_unleash_db.sql` containing `CREATE DATABASE unleash;`"

**Problem**: This modifies `services/persistence/initdb/`, which is **owned by spec 005-data-layer** (completed). TASK-013 has:

* Module: `services/flags/`, `services/persistence/initdb/`
* Two separate module paths that cross specification boundaries

**Compliance**: Constitution **Principle III (Modular Services)** requires each service self-contained. Mystique depends on Oracle, but **Mystique's tasks should not create files in Oracle's directory structure**.

**Root Cause**: Mystique's `DATABASE_URL=postgresql://arc:arc@arc-sql-db:5432/unleash` expects an `unleash` database. The init SQL must exist, but who owns it?

**Recommendation**:

* **Option A** (Preferred): Add a task **TASK-014** to 005-data-layer to retroactively create `002_create_unleash_db.sql` as a dependency for 006. This keeps database initialization in 005.
* **Option B**: Move `002_create_unleash_db.sql` creation to a post-005, pre-006 coordination task.
* **Option C**: Document that Mystique's Dockerfile/entrypoint script must create the database on first boot if it doesn't exist (upstream Unleash capability).

**Action**: Update plan.md and tasks.md to clarify ownership of the init SQL file before implementation.

***

### 1.2 Profiles.yaml Missing Existing Services in Updated Sections

**Issue**: Current `services/profiles.yaml` (from 005-data-layer implementation):

```yaml
think:
  services:
    - flash
    - sonic
    - strange
    - friday-collector
    - cortex
    - oracle
    - cerebro

reason:
  services:
    - cortex
    - flash
    - strange
    - sonic
    - otel
    - tardis
```

**Task TASK-001 (Spec 006) states**:

* Add `heimdall` to `think.services`
* Add `nick-fury`, `mystique` to `reason.services`

**Problem**: Current profiles.yaml is **incomplete**:

* `think` is missing `friday-collector` (it's there but let's verify the full list)
* `reason` is missing any reference to `friday` (should it be `friday` or `otel`?)

**Actual Issue**: Line 13 in current profiles.yaml shows `friday-collector`, not just `friday`. But TASK-001 acceptance doesn't validate the **full state of both profiles**, only that the three new services are added.

**Recommendation**:

* **Update TASK-001 acceptance criteria** to verify the full list of services in each profile after the update (not just grep for presence of the three new ones).
* Ensure `friday-collector` and `otel` are correctly named in reason profile.

***

### 1.3 Missing Reviewer Verification Task (TASK-999) Acceptance Criteria

**Issue**: TASK-999 (Reviewer) references "plan.md Reviewer Checklist" but that section:

* ✓ Exists in plan.md (line 357–381)
* ✗ Is **not cross-linked** in tasks.md acceptance

**Problem**: The reviewer agent needs to know which file to check. Currently:

```markdown
[TASK-999] Acceptance (reviewer runs all items from plan.md Reviewer Checklist):
```

This assumes the reviewer knows to open `plan.md` and find line 357. If TASK-999 is in a separate agent context, the checklist won't be visible.

**Recommendation**:

* Copy the **full reviewer checklist from plan.md into TASK-999 acceptance criteria** in tasks.md
* OR: Add an explicit reference: "See plan.md lines 357–381: Reviewer Checklist"
* This ensures the reviewer agent has all criteria in one place without context switching.

***

## 2. WARNINGS (Should address)

### 2.1 Constitution Principle VIII (Security by Default) — Nick Fury Root Deviation

**Issue**: Nick Fury (OpenBao) runs as root in dev mode. Constitution **Principle VIII** states:

> "Non-root containers (all Dockerfiles)"

Plan.md addresses this at lines 127–141:

```
# openbao/openbao runs as root by default.
# Nick Fury uses -dev mode (in-memory, auto-unsealed, known root token).
# This is a DEVELOPMENT-ONLY service. Root is acceptable for dev-mode OpenBao.
```

**Assessment**:

* ✓ Documented in Dockerfile
* ✓ Justified in plan.md (development-only, intentional)
* ✓ Marked in Constitution Check table (line 276) as `PASS†` with footnote

**Warning (not blocker)**: The footnote at line 282 states:

> "Nick Fury root deviation is intentional — OpenBao `-dev` mode is by-design insecure."

This is **correct reasoning** but relies on external context (dev-mode semantics). If a reviewer unfamiliar with OpenBao reads this, they might flag it as a violation.

**Recommendation**:

* Add a **REQUIRED** line to TASK-012 acceptance criteria:
  > "Dockerfile includes comment explaining root deviation: 'OpenBao dev mode is inherently insecure and stateless. This service is development-only.'"
* Ensures every implementation artifact (not just plan.md) carries the justification.

***

### 2.2 Profiles.yaml Update Not in TASK-001 Acceptance Criteria (Detail)

**Issue**: TASK-001 updates `services/profiles.yaml` but acceptance criteria only verify presence via grep:

```
- `grep "heimdall" services/profiles.yaml` matches a line under `think.services`
- `grep "nick-fury" services/profiles.yaml` matches a line under `reason.services`
```

**Problem**: `grep` matches anywhere. If someone adds `heimdall` to a comment instead of the YAML key, grep passes. Acceptance should be stricter.

**Recommendation**:

* Add: `python3 -c "import yaml; p = yaml.safe_load(open('services/profiles.yaml')); assert 'heimdall' in p['think']['services']"`
* Ensures the values are in the correct YAML structure, not just text matches.

***

### 2.3 Missing \[TASK-9XX] Documentation/Links Update Task Reference

**Issue**: Spec 006 includes a "Docs & Links Update" section (spec.md lines 211–217):

```markdown
## Docs & Links Update

- [ ] Update `services/profiles.yaml` — heimdall → `think`; nick-fury + mystique → `reason`
- [ ] Update `CLAUDE.md` monorepo layout to reference `gateway/`, `secrets/`, `flags/` directories
- [ ] Update `CLAUDE.md` Service Codenames table — Nick Fury: Infisical → OpenBao
- [ ] Update `.specify/config.yaml` `secrets` entry — change tech from `infisical` to `openbao`
- [ ] Verify `services/flags/service.yaml` `depends_on` lists `oracle` and `sonic`
```

But in tasks.md:

* ✓ TASK-900 exists (line 230–239) covering docs updates
* ✓ Acceptance criteria are clear

**However**: TASK-900 acceptance does NOT reference CLAUDE.md service names table update explicitly. It says:

```
- `CLAUDE.md` monorepo layout section references `gateway/`, `secrets/`, `flags/` directories
- `CLAUDE.md` Service Codenames table updated: Nick Fury row shows `OpenBao` (not `Infisical`)
```

This is fine, but the parallel spec.md list also mentions `.specify/config.yaml` update, which TASK-900 **does** include. Just verify alignment.

**Recommendation**:

* ✓ Current state is acceptable (both spec.md and tasks.md mention it)
* Cross-reference them in a comment if unsure.

***

### 2.4 Mystique Database Migration Start Period (30s) — Risk Underspecified

**Issue**: Plan.md (line 181) states:

```
start_period: 30s   # Unleash runs DB migrations on first boot — needs longer start_period
```

And TASK-013 acceptance specifies `start_period 30s` but does NOT specify:

* What happens if Oracle is not ready? (Mentioned in edge cases, but not acceptance.)
* What if migration takes >30s? (Documented as acceptable risk in plan.md line 391, but not in task acceptance.)

**Assessment**:

* ✓ Documented in plan.md risks
* ✓ Health check timeout is independent (5s) so continuous retries will eventually succeed
* \~ Task acceptance could be more explicit about expected behavior

**Recommendation**:

* Add to TASK-013 acceptance:
  > "If Mystique fails to start, logs must show DB migration in progress; `make mystique-health` retries for 30s before final health check timeout at 10s retries; migration typically completes in 10-20s."
* Clarifies expected startup delay and debugging approach.

***

## 3. OBSERVATIONS (Nice to know)

### 3.1 Heimdall Port Binding (127.0.0.1:80) Requires Root Escalation

**Issue**: Spec.md and plan.md specify:

```
ports:
  - "127.0.0.1:80:8080"     # host:80 → container proxy (non-privileged inside)
```

Port 80 is privileged (< 1024). Even with `127.0.0.1` binding, the Docker host must have privilege to bind the host port.

**Context**:

* Plan.md (lines 59–68) explains the workaround: internal Traefik entrypoint runs on `:8080` (non-privileged), host binding is handled by Docker (which typically runs as root or with CAP\_NET\_BIND\_SERVICE).
* This is correct and documented.

**Observation** (not a blocker):

* On some systems (e.g., rootless Docker), port 80 binding may fail with "Permission denied".
* Users with non-standard Docker configs may need to change the host port.

**Recommendation**:

* Add to `heimdall-help` output: "Note: Port 80 binding requires Docker privilege. If startup fails with 'permission denied', override in docker-compose override or change to a higher port."

***

### 3.2 CI Pipeline Amd64-Only (Not Multi-Platform in Images Build)

**Issue**: TASK-031 (control-images.yml) specifies:

```
platforms: linux/amd64
```

But TASK-032 (control-release.yml) specifies:

```
platforms: linux/amd64,linux/arm64
```

**Assessment**:

* ✓ Mirrors data-images.yml pattern (amd64 CI, multi-platform releases)
* ✓ Documented and intentional (plan.md line 145: "amd64 only in CI"; release multiplatform at line 237)

**Observation**: CI builds are faster (amd64 only, no QEMU), releases are portable. This is **correct** but adds complexity for multi-architecture testing.

**Recommendation**:

* If a future task requires testing on ARM64, add a separate workflow (e.g., `control-images-multiplatform.yml`) that runs on manual trigger only.

***

### 3.3 No Persistent Volume for Nick Fury — Stateless by Design

**Issue**: Plan.md (line 30) and TASK-012 acceptance state:

```
no volume (stateless dev); health fails until Oracle is healthy
```

And TASK-012 explicitly notes:

```
No volume declared (Nick Fury is stateless in dev — data lost on restart is expected and documented)
```

**Assessment**:

* ✓ Correct for dev mode
* ✓ Documented in plan.md risks (line 389)
* ✓ Acceptance includes verification: `no `arc-vault-\*` volumes`

**Observation**: This is appropriate for a dev-only service, but if a future spec wants to support persistent secrets in dev (e.g., for multi-session testing), a volume will need to be added. The current design is correct but inflexible.

**Recommendation**:

* Document in CLAUDE.md under "Nick Fury" codename:
  > "Dev mode only — all secrets lost on restart. For persistent secrets, a future spec will add Raft-backed storage."

***

## 4. REQUIREMENTS → PLAN → TASKS COVERAGE MATRIX

| ID | Title | Applies to | Plan Section | Task(s) | Status |
|----|-------|-----------|--------------|---------|--------|
| FR-1 | Create `services/gateway/` with Traefik v3 | Heimdall | Arch § Heimdall | TASK-011, TASK-021 | ✓ |
| FR-2 | Create `services/secrets/` with OpenBao | Nick Fury | Arch § Nick Fury | TASK-012, TASK-022 | ✓ |
| FR-3 | Create `services/flags/` with Unleash | Mystique | Arch § Mystique | TASK-013, TASK-023 | ✓† |
| FR-4 | Heimdall port binding 80 + 8090 | Heimdall | Arch § Heimdall | TASK-011, TASK-021 | ✓ |
| FR-5 | Nick Fury dev mode, root token | Nick Fury | Arch § Nick Fury | TASK-012, TASK-022 | ✓ |
| FR-6 | Mystique DATABASE\_URL + REDIS connection | Mystique | Arch § Mystique | TASK-013, TASK-023 | ✓† |
| FR-7 | Update profiles.yaml | Profiles | Arch § 7 | TASK-001 | ✓ |
| FR-8 | Create control-images.yml CI | CI | CI pattern § | TASK-031 | ✓ |
| FR-9 | Create control-release.yml | CI | CI pattern § | TASK-032 | ✓ |
| FR-10 | Create services/control.mk | Make | Aggregate § control.mk | TASK-024 | ✓ |
| FR-11 | Include .mk files in root Makefile | Makefile | Aggregate § | TASK-024 | ✓ |
| NFR-1 | Docker socket read-only (Heimdall) | Heimdall | Arch § Heimdall | TASK-011 | ✓ |
| NFR-2 | Nick Fury stateless (documented) | Nick Fury | Arch § Nick Fury | TASK-012 | ✓ |
| NFR-3 | All ports 127.0.0.1 only | All | Network § | TASK-011, TASK-012, TASK-013 | ✓ |
| NFR-4 | Non-root user verification | All | Arch § Non-root | TASK-011, TASK-012, TASK-013 | ✓† |
| NFR-5 | OCI + arc labels | All | Arch § File pattern | TASK-011, TASK-012, TASK-013 | ✓ |
| NFR-6 | Mystique depends\_on oracle + sonic | Mystique | Arch § Mystique | TASK-013 | ✓ |
| NFR-7 | CI build < 3 minutes (amd64) | CI | CI pattern § | TASK-031 | ✓ |

Legend:

* ✓ = Coverage found and adequate
* ✓† = Coverage found but with conditional success (edge case documented)
* ✗ = Missing coverage

***

## 5. PARALLEL EXECUTION OPPORTUNITIES

Current task DAG (tasks.md lines 21–45) identifies:

**Phase 2 (Parallel)**:

* TASK-011 \[P] services/gateway/ (Heimdall)
* TASK-012 \[P] services/secrets/ (Nick Fury)
* TASK-013 \[P] services/flags/ (Mystique)

**Phase 3 (Parallel, depends on Phase 2)**:

* TASK-021 \[P] heimdall.mk
* TASK-022 \[P] nick-fury.mk
* TASK-023 \[P] mystique.mk

**Phase 4 (Parallel, depends on TASK-024)**:

* TASK-031 \[P] control-images.yml
* TASK-032 \[P] control-release.yml

**Opportunity**: All tasks marked \[P] are safe to spawn in parallel. Total span (critical path):

```
TASK-001 (1) → TASK-011/012/013 (2) → TASK-021/022/023 (1)
            → TASK-024 (1) → TASK-031/032 (2) → TASK-041 (1) → TASK-900 (1) → TASK-999 (1)
```

**Critical path**: 10 units (TASK-001, 011, 021, 024, 031, 041, 900, 999 sequential; 012, 013, 022, 023, 032 parallel inside their phases).

Parallelization is **well-optimized**. No improvements needed.

***

## 6. CONSTITUTION COMPLIANCE CHECK

Cross-reference against `.specify/memory/constitution.md` v2.2.0:

| # | Principle | Applies | Status | Evidence | Issue |
|---|-----------|---------|--------|----------|-------|
| I | Zero-Dep CLI | n/a | n/a | No CLI changes | none |
| II | Platform-in-a-Box | YES | PASS | `make control-up` boots all 3; heimdall in think | none |
| III | Modular Services | YES | PASS† | Each self-contained, but Mystique's init SQL in Oracle dir | **Blocker 1.1** |
| IV | Two-Brain | YES | PASS | Config-only images; no language concern | none |
| V | Polyglot Standards | YES | PASS | Matches 003/005 patterns (Dockerfile, compose, .mk, CI) | none |
| VI | Local-First | n/a | n/a | CLI only | none |
| VII | Observability | YES | PASS | Health endpoints; Traefik dashboard; metrics available | none |
| VIII | Security | YES | WARN | Non-root attempts (Heimdall uid 1000, Mystique uid 1000) but Nick Fury root documented | **Warning 2.1** |
| IX | Declarative | n/a | n/a | CLI only | none |
| X | Stateful Ops | n/a | n/a | CLI only | none |
| XI | Resilience | YES | PASS | Health checks, start\_period, Mystique depends\_on oracle+sonic | none |
| XII | Interactive | n/a | n/a | CLI only | none |

**Constitution Status**: PASS with one documented deviation (Nick Fury root, justified for dev-only).

***

## 7. SERVICE PATH VALIDATION

Cross-reference module paths against `services/profiles.yaml` and actual directories:

| Service | Path | Exists? | Profile | Status |
|---------|------|---------|---------|--------|
| arc-gateway | `services/gateway/` | N (to-be-created) | think | ✓ |
| arc-vault | `services/secrets/` | N (to-be-created) | reason | ✓ |
| arc-flags | `services/flags/` | N (to-be-created) | reason | ✓ |
| arc-sql-db | `services/persistence/` | Y (existing) | think | ✓ |
| arc-cache | `services/cache/` | Y (existing) | think | ✓ |

All paths are **valid and follow the pattern** from spec 005-data-layer.

***

## 8. UNDEFINED/UNTESTABLE ACCEPTANCE CRITERIA

Scan tasks.md for vague or untestable criteria:

| Task | Criterion | Issue | Recommendation |
|------|-----------|-------|-----------------|
| TASK-011 | "if `USER 1000` causes Traefik to fail at runtime, remove `USER 1000` and document" | Vague — how to test? | Add explicit test: `docker run --rm ghcr.io/arc-framework/arc-gateway:latest traefik version` must succeed |
| TASK-012 | "if `wget` absent in openbao image, fall back to bash `/dev/tcp`" | Untestable in acceptance — error handling is conditional | Add: "Test both wget and /dev/tcp patterns; CI will verify which is available" |
| TASK-013 | "if Unleash fails to start, remove and add comment" | Conditional acceptance — implies retry loop | Add: "If uid 1000 fails, document root deviation + justification in compose comments; verify startup logs" |
| TASK-024 | "all three health targets; exits non-zero if any fails" | Testable but needs explicit command | Add: "`make control-health` and verify exit code via `echo $?` is 0" |

**Recommendation**: Update tasks.md acceptance for TASK-011, TASK-012, TASK-013 to include explicit fallback testing procedures.

***

## 9. DANGEROUS PATTERNS & RISKS

### 9.1 Mystique Creates Unleash DB (BLOCKER 1.1 — Covered Above)

### 9.2 Nick Fury Root — Needs Explicit Sign-Off

Currently marked as acceptable for dev-only, but reviewers may challenge. Recommend adding explicit **approval gate** in TASK-999.

### 9.3 Port 80 Binding on Unprivileged Systems

Heimdall requires privilege to bind port 80. If Docker is rootless, this will fail. Not a blocker but should be documented in a `heimdall-help` note.

***

## 10. MISSING TASKS / GAPS

### Potential Missing Tasks

| Gap | Spec Impact | Suggested Task |
|-----|-------------|-----------------|
| Unleash default admin credentials setup | US-4 (Mystique starts and serves UI) | Add post-startup task to retrieve auto-generated credentials from Unleash logs |
| Traefik dashboard access verification | US-2 (Heimdall auto-discovers services) | Implicit in E2E (TASK-041); no separate task needed |
| OpenBao secret backend initialization | US-3 (Nick Fury dev mode) | Implicit in health check; no task needed (stateless dev) |
| Mystique feature flag seeding | Not in scope | Future spec (post-launch data population) |

**Assessment**: No critical missing tasks. All user stories have corresponding tasks.

***

## FINAL ASSESSMENT

**Recommendation**: PAUSE TO FIX BLOCKERS

### Must Resolve Before Implementation:

1. **Blocker 1.1**: Clarify ownership of `services/persistence/initdb/002_create_unleash_db.sql`
   * Action: Update plan.md and TASK-013 to reference a coordination task or document Unleash's built-in DB creation capability.

2. **Blocker 1.2**: Verify profiles.yaml completeness in TASK-001 acceptance
   * Action: Update TASK-001 acceptance to validate full profile structure (not just grep).

3. **Blocker 1.3**: Copy reviewer checklist into TASK-999 acceptance criteria
   * Action: Embed the full checklist from plan.md into tasks.md TASK-999 section.

### Should Address Before Implementation:

4. **Warning 2.1**: Add explicit Dockerfile comment requirement for Nick Fury root deviation to TASK-012.
5. **Warning 2.2**: Strengthen TASK-001 YAML validation (use Python YAML parser, not grep).
6. **Warning 2.3** (already addressed): Verify .specify/config.yaml update is in scope.
7. **Observation 3.1**: Add heimdall-help note about port 80 privilege requirement.
8. **Observation 3.2**: Clarify Mystique startup delay expectations in TASK-013 acceptance.

### Timeline

* **Fix blockers**: 15–20 minutes (mostly rewording and cross-linking)
* **Address warnings**: 10 minutes (minor acceptance criterion updates)
* **Implementation**: Ready after blockers resolved

***

## APPENDIX: Detailed Task Dependencies

```mermaid
graph TD
    T001["TASK-001\n1 unit\nprofiles.yaml"] 
    
    T011["TASK-011 [P]\n2 units\ngateway/"]
    T012["TASK-012 [P]\n2 units\nsecrets/"]
    T013["TASK-013 [P]\n2 units\nflags/"]
    
    T021["TASK-021 [P]\n1 unit\nheimdall.mk"]
    T022["TASK-022 [P]\n1 unit\nnick-fury.mk"]
    T023["TASK-023 [P]\n1 unit\nmystique.mk"]
    
    T024["TASK-024\n1 unit\ncontrol.mk + Makefile"]
    
    T031["TASK-031 [P]\n2 units\ncontrol-images.yml"]
    T032["TASK-032 [P]\n2 units\ncontrol-release.yml"]
    
    T041["TASK-041\n1 unit\nE2E verification"]
    
    T900["TASK-900 [P]\n1 unit\nDocs & links"]
    T999["TASK-999\n1 unit\nReviewer"]
    
    T001 --> T011
    T001 --> T012
    T001 --> T013
    
    T011 --> T021
    T012 --> T022
    T013 --> T023
    
    T021 --> T024
    T022 --> T024
    T023 --> T024
    
    T024 --> T031
    T024 --> T032
    
    T031 --> T041
    T032 --> T041
    
    T041 --> T900
    T041 --> T999
    T900 --> T999
    
    style T001 fill:#e1f5ff
    style T041 fill:#fff9c4
    style T999 fill:#f3e5f5
```

**Critical Path** (longest dependency chain):
TASK-001 → TASK-011 → TASK-021 → TASK-024 → TASK-031 → TASK-041 → TASK-900 → TASK-999 = **10 units**

**Parallelizable units**:

* TASK-011, TASK-012, TASK-013 (Phase 2): 6 units in parallel
* TASK-021, TASK-022, TASK-023 (Phase 3): 3 units in parallel
* TASK-031, TASK-032 (Phase 4): 4 units in parallel

**Total effort** (critical path only, sequential): 10 units
**With parallelization** (realistic wall-clock time): ~6 units (Phase 1: 1, Phase 2 parallel: 2, Phase 3 parallel: 1, Phase 4 parallel: 2, Phase 5: 1 sequential)

---

---
url: /arc-platform/specs-site/007-voice-stack/analysis-report.md
---
# Analysis Report: 007-voice-stack

**Date:** 2026-03-01\
**Stage:** Pre-implementation\
**Auditor:** Arc Analysis Agent (Read-Only)

***

## Executive Summary

The 007-voice-stack feature specification is **well-structured and ready for implementation** with no critical blockers. The spec, plan, and tasks demonstrate strong alignment with the A.R.C. Platform architecture and constitution. All 11 functional requirements, 7 non-functional requirements, and 8 success criteria have clear mappings to implementation tasks. The design follows established patterns from specs 003 (messaging), 005 (data-layer), and 006 (platform-control).

**Status:** PROCEED TO IMPLEMENTATION\
**Blockers:** 0\
**Warnings:** 3 (all addressable)\
**Observations:** 4

***

## Section 1: Coverage Matrix

### Requirements → Plan → Tasks Traceability

#### Functional Requirements (FR-1 through FR-11)

| FR | Requirement | Plan Section | Task ID | Coverage | Status |
|----|-------------|--------------|---------|----------|--------|
| FR-1 | Create 3 Dockerfiles (thin wrappers) | Key Decisions #1-3 | TASK-011, TASK-012, TASK-013 | 100% | ✓ |
| FR-2 | Create docker-compose.yml (all 3 services) | Key Decisions #1-3 | TASK-021 | 100% | ✓ |
| FR-3 | Create livekit.yaml config | Key Decisions #1 | TASK-014 | 100% | ✓ |
| FR-4 | Create ingress.yaml config | Key Decisions #2 | TASK-014 | 100% | ✓ |
| FR-5 | Create egress.yaml config | Key Decisions #3 | TASK-014 | 100% | ✓ |
| FR-6 | Create service.yaml (metadata) | Key Decisions #8 | TASK-015 | 100% | ✓ |
| FR-7 | Create realtime.mk (make targets) | Key Decisions #5 | TASK-022 | 100% | ✓ |
| FR-8 | Update services/profiles.yaml | Project Structure | TASK-001 | 100% | ✓ |
| FR-9 | Create .github/workflows/realtime-images.yml | Key Decisions #6 | TASK-041 | 100% | ✓ |
| FR-10 | Create .github/workflows/realtime-release.yml | Key Decisions #6 | TASK-042 | 100% | ✓ |
| FR-11 | Include realtime.mk in root Makefile + publish-all | Parallel Execution | TASK-031 | 100% | ✓ |

**FR Coverage: 100% (11/11)**

#### Non-Functional Requirements (NFR-1 through NFR-7)

| NFR | Requirement | Plan Section | Task ID | Acceptance Criteria | Status |
|-----|-------------|--------------|---------|-------------------|--------|
| NFR-1 | TCP 127.0.0.1 only; UDP 0.0.0.0 exception | Network Strategy | TASK-021, TASK-051 | Verified via docker-compose config + integration test | ✓ |
| NFR-2 | Static dev API keys in configs | Key Decisions #1, #2, #3 | TASK-014 | `devkey`/`devsecret` in all three YAMLs; production vault deferred (TD-001) | ✓ |
| NFR-3 | Non-root containers | Non-root handling | TASK-011, TASK-012, TASK-013 | Verify upstream image user; document if root required | ✓ |
| NFR-4 | OCI + arc.service.\* labels on Dockerfiles | Key Decisions #1-3 | TASK-011, TASK-012, TASK-013 | `org.opencontainers.*` + `arc.service.codename` | ✓ |
| NFR-5 | livekit.yaml mounted read-only | Key Decisions #1 | TASK-021 | Volume mount with `:ro` flag in compose | ✓ |
| NFR-6 | LIVEKIT\_NODE\_IP documented | Key Decisions #1 | TASK-022 | `realtime-help` target includes env var doc | ✓ |
| NFR-7 | CI build < 5 min (amd64 only, no QEMU) | Key Decisions #6 | TASK-041 | CI path: linux/amd64 only; mirror control-images.yml | ✓ |

**NFR Coverage: 100% (7/7)**

#### Success Criteria (SC-1 through SC-8)

| SC | Criterion | Task ID | Verification Method |
|----|-----------|---------|---------------------|
| SC-1 | `make realtime-up && make realtime-health` exits 0; all three healthy | TASK-051 | Docker compose ps + curl probes |
| SC-2 | `curl http://localhost:7880` returns LiveKit response | TASK-051 | HTTP 2xx/3xx from :7880 |
| SC-3 | `curl http://localhost:7888` returns 200 (Sentry) | TASK-051 | HTTP 200 from :7888 |
| SC-4 | `curl http://localhost:7889` returns 200 (Scribe) | TASK-051 | HTTP 200 from :7889 |
| SC-5 | `make dev` (think profile) includes realtime; `make dev-health` exits 0 | TASK-001 + TASK-051 | Profile composition verified |
| SC-6 | `realtime-images.yml` completes < 5 min (3 images, amd64) | TASK-041 | CI build time logged |
| SC-7 | `git tag realtime/v0.1.0` triggers multi-platform release | TASK-042 | Tag → GitHub release with GHCR links |
| SC-8 | All three Dockerfiles pass `trivy` scan (zero CRITICAL) | TASK-011, TASK-012, TASK-013 | Security scan in CI (via security-\* jobs) |

**SC Coverage: 100% (8/8)**

***

## Section 2: Task Dependency Analysis

### Dependency Graph Validation

```
TASK-001 (profiles.yaml) → all Phase 2 tasks (TASK-011-015)
  ↓
Phase 2 (parallel) → TASK-021, TASK-022
  ↓
TASK-031 (Makefile wiring) → TASK-041, TASK-042
  ↓
TASK-051 (E2E) → TASK-900 (docs) → TASK-999 (reviewer)
```

**DAG Status:** Valid (no cycles, clean topological ordering)

### Parallelization Analysis

**Phase 1 (Sequential):**

* TASK-001: Update profiles.yaml (dependency for all downstream)

**Phase 2 (Fully Parallel - 5 tasks):**

* TASK-011, TASK-012, TASK-013: Three Dockerfiles (independent files)
* TASK-014: Three YAML configs (independent files)
* TASK-015: service.yaml (independent file)
* **Safety:** No file conflicts; each task writes to distinct path in `services/realtime/`

**Phase 3 (Parallel - 2 tasks):**

* TASK-021: docker-compose.yml
* TASK-022: realtime.mk
* **Safety:** Different files; TASK-022 depends on TASK-021 in execution order but files are independent

**Phase 4 (Sequential):**

* TASK-031: Root Makefile + scripts updates (writes to 3 files in root + scripts/)

**Phase 5 (Parallel - 2 tasks):**

* TASK-041, TASK-042: CI workflows (independent files)

**Phase 6 (Sequential):**

* TASK-051: E2E verification (depends on all Phases 1-5)

**Phase 7 (Sequential):**

* TASK-900: Docs update
* TASK-999: Reviewer verification

**Verdict:** Current task structure is sound. 9 of 14 tasks marked `[P]` for parallel execution correctly identify independent work.

***

## Section 3: Requirement → Success Criteria → Task Acceptance

All user stories and acceptance criteria from spec.md have verifiable task acceptance criteria:

### US-1: Stack startup

* **Spec:** All three start with `make realtime-up`
* **Task:** TASK-051 acceptance criterion: `docker compose ps` shows all three `healthy`
* **Status:** ✓ Full traceability

### US-2: LiveKit API at :7880

* **Spec:** HTTP 200/redirect on `curl http://localhost:7880`
* **Task:** TASK-051 acceptance criterion: `curl -s http://localhost:7880` returns LiveKit response
* **Status:** ✓ Full traceability

### US-3: Sentry at :7888 and :1935

* **Spec:** `:7888` and `:1935` available; `curl http://localhost:7888` returns 200
* **Task:** TASK-021 (docker-compose.yml) acceptance: ports specified; TASK-051: curl probe
* **Status:** ✓ Full traceability

### US-4: Scribe at :7889

* **Spec:** `:7889` available; `curl http://localhost:7889` returns 200
* **Task:** TASK-021 + TASK-051 (same pattern as US-3)
* **Status:** ✓ Full traceability

### US-5: Dependency ordering in `make dev`

* **Spec:** cache starts before realtime; `make dev && make dev-health` exits 0
* **Task:** TASK-001 (add realtime to think profile); TASK-051 (integration test with think profile)
* **Status:** ✓ Full traceability

### US-6: CI image builds

* **Spec:** All three images built + pushed on main merge
* **Task:** TASK-041 (realtime-images.yml workflow)
* **Status:** ✓ Full traceability

### US-7: Release versioning

* **Spec:** Tag format `realtime/vX.Y.Z` → multi-platform images
* **Task:** TASK-042 (realtime-release.yml workflow)
* **Status:** ✓ Full traceability

### US-8: service.yaml for CLI discovery

* **Spec:** `service.yaml` contains role, codename, image, ports, health, depends\_on
* **Task:** TASK-015 (create service.yaml)
* **Status:** ✓ Full traceability

### US-9: `make realtime-logs` (P3 — nice-to-have)

* **Spec:** Tail all three services with service-prefixed output
* **Task:** TASK-022 (realtime.mk includes `realtime-logs` target)
* **Status:** ✓ Full traceability

***

## Section 4: Constitution Compliance Audit

Reviewing all 12 principles against the feature:

### I. Zero-Dependency CLI

* **Applies:** No (services only)
* **Status:** N/A

### II. Platform-in-a-Box

* **Applies:** Yes
* **Evidence:**
  * `make realtime-up` brings all three services online as a unit ✓
  * Joined to `think` profile (minimal viable platform) ✓
  * Depends on cache (pre-existing in think profile) ✓
  * TASK-051 verifies: `make dev PROFILE=think` includes realtime ✓
* **Status:** **PASS**

### III. Modular Services Architecture

* **Applies:** Yes
* **Evidence:**
  * Self-contained in `services/realtime/` ✓
  * Own Dockerfile (3 variants), service.yaml, compose, .mk ✓
  * Profiles (`think`, `reason`) select inclusion ✓
  * Sentry/Scribe co-located but logically grouped (rationale: can never run without Daredevil) ✓
  * TASK-015 creates service.yaml with sidecars metadata ✓
* **Status:** **PASS**

### IV. Two-Brain Separation

* **Applies:** Yes
* **Evidence:**
  * Go (zero custom code) — only config files and Dockerfiles ✓
  * Python (zero code) — upstream LiveKit images ✓
  * No application logic in this feature ✓
* **Status:** **PASS**

### V. Polyglot Standards

* **Applies:** Yes
* **Evidence:**
  * Follows 003 (messaging), 005 (data), 006 (control) thin-wrapper patterns exactly ✓
  * Consistent Dockerfile structure with OCI + arc.service.\* labels ✓
  * Same healthcheck pattern (CMD-SHELL with curl/wget) ✓
  * Makefile targets mirror control.mk / data.mk structure ✓
  * CI pattern mirrors control-images.yml / data-images.yml ✓
* **Status:** **PASS**

### VI. Local-First Architecture

* **Applies:** No (services only)
* **Status:** N/A

### VII. Observability by Default

* **Applies:** Yes
* **Evidence:**
  * HTTP health endpoints on all three: :7880 (Daredevil), :7888 (Sentry), :7889 (Scribe) ✓
  * Docker healthchecks with appropriate intervals/retries/start\_periods ✓
  * TASK-022 defines `realtime-health` target probing all three ✓
  * Plan.md notes: "No structured logging/OTEL inside LiveKit" but public health endpoints sufficient for dev ✓
* **Status:** **PASS**

### VIII. Security by Default

* **Applies:** Yes
* **Evidence:**
  * TCP ports bind `127.0.0.1` only (secure by default) ✓
  * UDP `50100-50200` binds `0.0.0.0` (documented exception, required for WebRTC NAT) ✓
  * Non-root containers: TASK-011/012/013 verify upstream image user; document if root ✓
  * No secrets in git: static dev keys in config files (NFR-2, TD-001 tracks vault production work) ✓
  * OCI labels required in all Dockerfiles ✓
* **Status:** **PASS† (exception well-documented)**

### IX. Declarative Reconciliation

* **Applies:** No (CLI only)
* **Status:** N/A

### X. Stateful Operations

* **Applies:** No (CLI only)
* **Status:** N/A

### XI. Resilience Testing

* **Applies:** Yes
* **Evidence:**
  * `depends_on` with `condition: service_healthy` ensures ordering ✓
  * Healthchecks on all three services ✓
  * Start periods (10s, 15s) allow service startup time ✓
  * Edge cases documented (spec.md lines 195-206): cache unavailable → single-node; storage unavailable → graceful egress fallback ✓
* **Status:** **PASS**

### XII. Interactive Experience

* **Applies:** No (CLI only)
* **Status:** N/A

**Constitution Summary:** 7/12 applicable principles; all 7 **PASS**

***

## Section 5: Cross-Module Dependencies & Risks

### Direct Dependencies

| Dependency | Introduced By | Risk Level | Mitigation |
|------------|---------------|-----------|-----------|
| `arc-cache` (Redis) | TASK-021 compose depends\_on | Medium | Listed in TASK-051 pre-condition; `make realtime-up` after `make cache-up` |
| `arc-storage` (MinIO) | egress.yaml S3 config | Low | Optional for dev; gracefully degrades per spec edge case |
| `arc_platform_net` (Docker network) | docker-compose.yml | Medium | TASK-022 realtime-up creates network if missing (`docker network create ... \|\| true`) |

### Cross-File Consistency Risks

| Risk | Scope | Detection | Mitigation |
|------|-------|-----------|-----------|
| API key mismatch (devkey/devsecret) | livekit.yaml, ingress.yaml, egress.yaml | TASK-014 acceptance: all three use identical keys | All three created in same task; values sourced from spec |
| Config mount path mismatch | docker-compose.yml vs actual files | TASK-021 acceptance: mount paths verified vs file structure | Compose spec template in plan.md; integration test |
| Image name consistency | 3 Dockerfiles vs docker-compose.yml vs .mk vs CI | TASK-011/012/013/021/041/042 all reference standardized names | Names follow pattern: arc-realtime, arc-realtime-ingress, arc-realtime-egress |
| Health endpoint divergence | realtime.mk vs actual service ports | TASK-022 acceptance: curl probes hardcoded to :7880/:7888/:7889 | Port reference section in spec (lines 164-174) is single source of truth |

***

## Section 6: Gaps & Missing Pieces

### Potential Gaps Identified

| Gap | Category | Severity | Rationale |
|-----|----------|----------|-----------|
| No explicit LIVEKIT\_NODE\_IP testing for remote clients | Integration | Low | Documented in TASK-022 (realtime-help); TD-002 tracks cloud deploy work |
| LiveKit Egress requires Chrome (~1GB) | Architecture | Low | Noted in TASK-013 acceptance; TD-005 tracks optimization |
| MinIO `recordings` bucket pre-creation | Infrastructure | Low | Documented in TASK-014 acceptance; TD-004 tracks init script |
| Vault integration for production | Security | Low | TD-001 explicitly deferred to security hardening spec |

**Verdict:** No gaps blocking implementation. All deferred items have corresponding `TD-*` tech debt entries in plan.md.

***

## Section 7: Service Path Validation

### Directory Structure

✓ `services/realtime/` — single directory (matches `.specify/config.yaml` entry)
✓ No reference to stale paths (`platform/core/`, `platform/plugins/`)
✓ All file paths in tasks reference `services/realtime/` correctly

### Module Cross-Reference (`.specify/config.yaml`)

```yaml
services:
  - { dir: "realtime", codename: "daredevil", tech: "livekit", lang: "config" }
```

**Status:** Already present in config.yaml (line 39) ✓

### Profiles Validation

**Current state:**

```yaml
think:
  services:
    - messaging
    - cache
    - streaming
    - friday-collector
    - cortex
    - sql-db
    - vector-db
    - gateway
    # TASK-001 adds: realtime
```

**Status:** Ready for update ✓

***

## Section 8: CI/CD Pattern Consistency

### Workflows Structure Validation

Checking against existing patterns (control-images.yml, data-images.yml):

| Pattern Element | control-images.yml | data-images.yml | Expected for realtime-images.yml |
|-----------------|-------------------|-----------------|----------------------------------|
| Trigger paths | `services/control/**` | `services/data/**` | `services/realtime/**` |
| dorny/paths-filter | Yes | Yes | TASK-041 includes |
| Parallel build jobs | 3 (net, smtp, email) | 3 (sql, vector, otel) | 3 (realtime, ingress, egress) — TASK-041 |
| amd64-only in CI | Yes (`linux/amd64`) | Yes (`linux/amd64`) | TASK-041 specifies amd64 only |
| Release tag format | `control/v*` | `data/v*` | `realtime/v*` — TASK-042 |
| Multi-platform in release | Yes (amd64, arm64) | Yes (amd64, arm64) | TASK-042 specifies |

**Status:** ✓ Full alignment with existing patterns

***

## Section 9: Parallel Execution Safety

### File-Level Conflict Analysis

**Phase 2 (TASK-011 through TASK-015)** — all parallel ✓

| Task | Creates File(s) | Conflicts with |
|------|-----------------|----------------|
| TASK-011 | `services/realtime/Dockerfile` | None |
| TASK-012 | `services/realtime/Dockerfile.ingress` | None |
| TASK-013 | `services/realtime/Dockerfile.egress` | None |
| TASK-014 | `services/realtime/{livekit,ingress,egress}.yaml` | None |
| TASK-015 | `services/realtime/service.yaml` | None |

**Safety:** ✓ Zero conflicts; safe for concurrent execution

**Phase 3 (TASK-021, TASK-022)** — parallel ✓

| Task | Creates File(s) | Conflicts with |
|------|-----------------|----------------|
| TASK-021 | `services/realtime/docker-compose.yml` | None |
| TASK-022 | `services/realtime/realtime.mk` | None |

**Safety:** ✓ Different files; safe for concurrent execution

**Phase 5 (TASK-041, TASK-042)** — parallel ✓

| Task | Creates File(s) | Conflicts with |
|------|-----------------|----------------|
| TASK-041 | `.github/workflows/realtime-images.yml` | None |
| TASK-042 | `.github/workflows/realtime-release.yml` | None |

**Safety:** ✓ Different files; safe for concurrent execution

***

## Section 10: Documentation & Links Coverage

### Docs & Links Update (TASK-900) Alignment with Spec.md Section

From spec.md "Docs & Links Update" (lines 219-227):

1. Update `services/profiles.yaml` — add `realtime` to `think`, `reason`, `ultra-instinct`
   * **Task:** TASK-001 + TASK-900 acceptance criteria line 328

2. Update `CLAUDE.md` monorepo layout
   * **Task:** TASK-900 acceptance line 322

3. Update `CLAUDE.md` Service Codenames table
   * **Task:** TASK-900 acceptance lines 323-326

4. Update `.specify/config.yaml`
   * **Task:** TASK-900 acceptance line 327 (verify already present)

5. Update `scripts/lib/check-dev-prereqs.sh`
   * **Task:** TASK-031 (prereq checks)
   * **Task:** TASK-900 acceptance (docs link verification)

6. Track vault integration as follow-on
   * **Task:** Plan.md TD-001 (tech debt entry)

7. Verify `services/realtime/service.yaml` `depends_on` lists `cache`
   * **Task:** TASK-015 acceptance criteria

**Status:** ✓ All items in spec "Docs & Links Update" section have task coverage

### Docs Links Broken-Reference Risk

Cross-referencing TASK-900 acceptance criteria:

* `CLAUDE.md` → exists, has Monorepo Layout and Service Codenames sections ✓
* `.specify/config.yaml` → exists, `realtime` entry already added (line 39) ✓
* `services/profiles.yaml` → exists, will be modified by TASK-001 ✓

**Risk:** Low — all target files exist and are accessible

***

## Section 11: Reviewer Task (TASK-999) Completeness

TASK-999 acceptance criteria (lines 335-377 in tasks.md) comprehensively cover:

* \[ ] All 14 tasks marked complete (lines 336)
* \[ ] Stack health (pre-condition: cache running) (lines 337-341)
* \[ ] Individual service health (lines 342-345)
* \[ ] Port binding validation (lines 346-348)
* \[ ] Config mount verification (lines 349)
* \[ ] API key consistency (line 350)
* \[ ] Dockerfile compliance (lines 351-354)
* \[ ] service.yaml structure (line 355)
* \[ ] realtime.mk functionality (lines 356-359)
* \[ ] Makefile integration (line 360)
* \[ ] scripts/scripts.mk publish-all (line 361)
* \[ ] check-dev-prereqs.sh updates (line 362)
* \[ ] profiles.yaml final state (line 363)
* \[ ] CI workflows (lines 364-366)
* \[ ] Documentation updates (line 367)
* \[ ] Constitution compliance (lines 368-375)
* \[ ] No stray TODOs (line 376)

**Status:** ✓ All spec success criteria covered; comprehensive validation scope

***

## Section 12: Identified Warnings

### Warning 1: Service Co-Location Pattern — Execution Risk

**Severity:** Medium\
**Issue:** Three related services (Daredevil, Sentry, Scribe) in one directory with shared compose. If one task is deferred or blocked, all three could stall.\
**Evidence:** Task structure depends on TASK-021 (compose) before most integration tests run.\
**Mitigation:**

* TASK-051 integration test is sequential; no parallel risk to deliverables
* Plan.md Rationale (lines 65-71) justifies co-location: "Prevents accidental partial-stack deploys"
* Each Dockerfile independently buildable if needed
  **Recommendation:** Proceed; co-location is intentional architectural choice.

### Warning 2: Edge Case on `LIVEKIT_NODE_IP` Environment Expansion

**Severity:** Low\
**Issue:** Plan.md line 464 raises concern about LiveKit YAML env var expansion syntax `${LIVEKIT_NODE_IP:-127.0.0.1}`.\
**Evidence:** Spec line 104, Plan line 104, TASK-014 acceptance (line 140) all reference this pattern.\
**Mitigation:**

* TASK-014 acceptance: "LiveKit supports env var expansion in YAML config — verify..."
* Fallback documented: "fallback to compose `environment:` injection if not"
* Plan.md Risk row (line 464) explicitly calls this out
  **Recommendation:** TASK-014 should include explicit verification step (try build + startup with default LIVEKIT\_NODE\_IP).

### Warning 3: Non-Root Verification Deferred Per-Image

**Severity:** Low\
**Issue:** TASK-011/012/013 acceptance criteria defer non-root check to per-image verification (lines 103-105, 116, 127).\
**Evidence:** No hardcoded user; depends on upstream image inspection.\
**Mitigation:**

* Plan.md section "Non-root handling" (lines 221-232) provides exact docker commands
* If root detected, documented in Dockerfile comment (pattern from 006-platform-control)
* NFR-3 explicitly addresses this
  **Recommendation:** Good; upstream image inspection is appropriate for thin-wrapper pattern.

***

## Section 13: Identified Observations

### Observation 1: Strong Alignment with Established Thin-Wrapper Pattern

**Note:** 007-voice-stack follows the exact structure of 003 (messaging), 005 (data), 006 (control). This is excellent — reusing proven patterns. No action needed; flag as positive precedent for future specs.

### Observation 2: Co-Location Rationale is Well-Documented

**Note:** Plan.md lines 65-71 provide clear architectural rationale for why Sentry/Scribe share one directory with Daredevil. This is deliberate, not an accident. Implementation should preserve this group structure.

### Observation 3: Edge Cases & Tech Debt Comprehensively Listed

**Note:** Spec.md edge cases table (lines 195-206) is thorough. Plan.md tech debt (lines 416-424) clearly defers non-blocking items (vault, remote clients, TURN, MinIO bucket init, Chrome size). All align with NFR-2 and constitution Principle VIII (Security).

### Observation 4: Reviewer Verification Scope is Comprehensive

**Note:** TASK-999 acceptance criteria (42 lines) is unusually detailed — this is appropriate given the multi-container, multi-port, multi-config nature of the feature. Ensures high-quality integration testing.

***

## Section 14: Task Breakdown Quality

### Task Granularity Assessment

| Phase | Task Count | Independence | Status |
|-------|-----------|--------------|--------|
| Setup | 1 | Sequential (profiles.yaml blocks downstream) | ✓ Correct |
| Core Files | 5 | Fully parallel (different files) | ✓ Correct |
| Assembly | 2 | Parallel-safe (different files) | ✓ Correct |
| Wiring | 1 | Sequential (modifies multiple root files) | ✓ Correct |
| CI/CD | 2 | Parallel (different workflows) | ✓ Correct |
| Integration | 1 | Sequential (depends on all prior) | ✓ Correct |
| Polish | 2 | Sequential (docs then review) | ✓ Correct |

**Verdict:** Tasks are appropriately granular. No over-fragmentation (e.g., separate tasks per Dockerfile) or under-fragmentation (e.g., all core files in one task).

### Acceptance Criteria Quality

All 14 tasks have concrete, testable acceptance criteria:

* ✓ TASK-001: Python YAML validation (lines 77-85)
* ✓ TASK-011-013: Docker build success + label verification (lines 100-130)
* ✓ TASK-014: YAML syntax validation + config content checks (lines 136-153)
* ✓ TASK-015: service.yaml structure validation (lines 160-167)
* ✓ TASK-021: Compose config validation + port/volume/depends\_on verification (lines 180-203)
* ✓ TASK-022: Make target dry-run + help output (lines 210-224)
* ✓ TASK-031: Root Makefile include + publish-all updates (lines 235-247)
* ✓ TASK-041: YAML parsing + 3 parallel image builds (lines 262-271)
* ✓ TASK-042: YAML parsing + multi-platform setup (lines 280-287)
* ✓ TASK-051: Docker health checks + curl probes + port bindings (lines 298-312)
* ✓ TASK-900: Docs file updates (lines 322-328)
* ✓ TASK-999: 42-line comprehensive verification checklist (lines 335-376)

**Verdict:** Acceptance criteria are specific, measurable, and verifiable.

***

## Section 15: Dependency Chain Verification

### Critical Path Analysis

```
TASK-001 (1 min)
  → TASK-011/012/013/014/015 parallel (15 min combined)
    → TASK-021/TASK-022 parallel (10 min combined)
      → TASK-031 (5 min)
        → TASK-041/TASK-042 parallel (20 min combined, includes build time)
          → TASK-051 (10 min integration test)
            → TASK-900 (5 min docs)
              → TASK-999 (10 min reviewer verification)

Critical Path Duration: ~75 minutes (single-threaded equivalent)
Parallelization Opportunity: 5 + 5 + 2 + 5 = 17 tasks × time, but 9 marked [P]
Estimated Actual Time: ~40 minutes with 2-3 parallel agents
```

**Verdict:** Good parallelization strategy; critical path is clear.

***

## Section 16: Constitution Compliance Detailed Evidence

### Principle II: Platform-in-a-Box

**Full Evidence:**

* spec.md US-5: "make dev (think profile) includes realtime in dependency-ordered boot"
* plan.md profiles.yaml update (lines 280-305): adds `realtime` to `think` and `reason`
* TASK-001: updates profiles.yaml with realtime entry
* TASK-051 acceptance: verifies "make dev PROFILE=think" starts realtime
* result: Single `make dev` bootstraps working platform with voice infrastructure

**Verdict:** PASS — Feature achieves Principle II

### Principle III: Modular Services Architecture

**Full Evidence:**

* spec.md line 9: "Target Modules — services/realtime/"
* plan.md Project Structure (lines 354-377): self-contained directory
* TASK-015: service.yaml defines `role: realtime`, `codename: daredevil`, lists `depends_on: [cache]`
* TASK-015 sidecars: arc-realtime-ingress (sentry), arc-realtime-egress (scribe) registered as metadata
* profiles.yaml update: single `realtime` entry enables all three services

**Verdict:** PASS — Feature follows modular architecture

### Principle IV: Two-Brain Separation

**Full Evidence:**

* spec.md line 19: "thin-wrapper Dockerfiles over official LiveKit images"
* spec.md line 129: "No Python or Go custom code"
* All tasks: only create Dockerfiles (FROM upstream), config files, Makefiles
* Zero custom application code

**Verdict:** PASS — Go/Python boundary respected

### Principle V: Polyglot Standards

**Full Evidence:**

* Dockerfile pattern: matches 003, 005, 006 exactly (FROM, OCI labels, arc.service.\* labels, healthchecks)
* docker-compose.yml: matches messaging, data, control patterns (services, networks, depends\_on, healthchecks, volumes, ports)
* realtime.mk: matches control.mk, data.mk structure (color vars, phony targets, compose shortcuts)
* CI workflows: mirror control-images.yml / data-images.yml (dorny filter, parallel jobs, amd64 CI, multi-platform release)
* Git conventions: plan.md line 72 notes "No AI attribution in commit messages" (Principle V.2)

**Verdict:** PASS — Consistent patterns across all languages/tools

### Principle VII: Observability by Default

**Full Evidence:**

* spec.md lines 239-240: "HTTP health endpoints :7880/:7888/:7889; Docker healthchecks"
* plan.md section Technical Context (line 27): "Testing: HTTP GET :7880 (Daredevil), :7888 (Sentry), :7889 (Scribe)"
* TASK-021 acceptance: healthchecks with intervals/timeouts/retries/start\_periods defined
* TASK-022 acceptance: `realtime-health` target probes all three endpoints
* Plan.md Observability Pattern note: LiveKit images don't include OTEL but public health endpoints sufficient for dev

**Verdict:** PASS — Observable from day one; health endpoints on all three services

### Principle VIII: Security by Default

**Full Evidence:**

* spec.md lines 144-150: TCP 127.0.0.1 only; UDP 0.0.0.0 documented exception
* spec.md line 145: NFR-3 "Non-root containers"
* spec.md line 147: NFR-4 "OCI + arc.service.\* labels"
* spec.md line 145: NFR-2 "dev static keys; production vault deferred"
* TASK-011/012/013: verify non-root per image; document if root required
* Plan.md line 351: "Security note: UDP 0.0.0.0 is documented, intentional exception"
* Spec.md edge case (line 202): UDP blocking → client connection fails but documented

**Verdict:** PASS† — Secure defaults; exception documented and intentional

### Principle XI: Resilience Testing

**Full Evidence:**

* spec.md edge cases (lines 195-206): all documented failure modes
* plan.md Key Decisions line 130: `depends_on: {arc-cache: {condition: service_healthy}}`
* plan.md line 130-136: healthchecks with start\_periods for gradual readiness
* TASK-021 acceptance: `depends_on` ordering cache → realtime → ingress/egress
* TASK-051 acceptance: integration test validates all three services healthy before proceeding

**Verdict:** PASS — Healthy-state dependency ordering + comprehensive edge case handling

***

## Section 17: Final Checklist

### Pre-Implementation Gate Checks

* \[x] All 11 FR + 7 NFR + 8 SC have task coverage (100%)
* \[x] No circular dependencies in task DAG (valid topological sort)
* \[x] 9 of 14 tasks correctly marked \[P] for parallelization (Phase 2, 3, 5)
* \[x] Parallel tasks have zero file conflicts (safe for concurrent execution)
* \[x] All 7 applicable constitution principles PASS
* \[x] Service paths valid (services/realtime/ matches config.yaml)
* \[x] CI pattern mirrors existing (control, data, messaging)
* \[x] docs & links task (TASK-900) covers all spec items
* \[x] Reviewer task (TASK-999) comprehensive (42-line checklist)
* \[x] Tech debt items (TD-001 through TD-005) deferred appropriately
* \[x] Edge cases documented (spec.md table; plan.md mitigations)
* \[x] Cross-module dependencies identified (cache, storage, arc\_platform\_net)

### Quality Gates Met

| Gate | Status | Evidence |
|------|--------|----------|
| spec\_complete | ✓ | All 9 sections complete; no TODO/TBD |
| plan\_aligned | ✓ | Every FR/NFR/SC mapped to plan section |
| tasks\_coverage | ✓ | 14 tasks cover all requirements |
| constitution\_compliance | ✓ | 7/7 applicable principles PASS |
| patterns\_compliance | ✓ | Thin-wrapper pattern matches 003/005/006 |

***

## Recommendation

**PROCEED TO IMPLEMENTATION**

This feature is well-specified, comprehensively planned, and ready for parallel agent execution. No blockers exist. The three minor warnings (co-location execution risk, LIVEKIT\_NODE\_IP verification, non-root deferred check) are all mitigated by acceptance criteria and documentation.

**Suggested Execution Strategy:**

1. Run TASK-001 sequentially (profiles.yaml baseline)
2. Spin up 2 agents for Phase 2 (TASK-011-015 parallel)
3. Spin up 1 agent for Phase 3 (TASK-021-022 parallel, ~fast)
4. Continue sequentially through Phase 6
5. Spawn reviewer agent for TASK-999 after TASK-900 complete

**Estimated Implementation Time:** 1.5-2 hours with 2-3 parallel agents; review 20-30 minutes

***

## Appendix: File Manifest

All artifacts reviewed:

* `/Users/dgtalbug/Workspace/arc/arc-platform/specs/007-voice-stack/spec.md` (257 lines)
* `/Users/dgtalbug/Workspace/arc/arc-platform/specs/007-voice-stack/plan.md` (465 lines)
* `/Users/dgtalbug/Workspace/arc/arc-platform/specs/007-voice-stack/tasks.md` (393 lines)
* `/Users/dgtalbug/Workspace/arc/arc-platform/.specify/memory/constitution.md` (177 lines)
* `/Users/dgtalbug/Workspace/arc/arc-platform/.specify/memory/patterns.md` (134 lines)
* `/Users/dgtalbug/Workspace/arc/arc-platform/.specify/config.yaml` (310 lines)
* `/Users/dgtalbug/Workspace/arc/arc-platform/services/profiles.yaml` (38 lines)
* `/Users/dgtalbug/Workspace/arc/arc-platform/Makefile` (checked; service includes verified)

***

**Analysis Complete**\
**Report Generated:** 2026-03-01\
**Status:** Ready for team review and implementation sign-off

---

---
url: /arc-platform/specs-site/008-specs-site/analysis-report.md
---
# Analysis Report: 008-Specs-Site

**Date:** 2026-03-01
**Stage:** Pre-implementation audit
**Status:** Ready to implement with noted observations

***

## Coverage Matrix

### Functional Requirements

| Requirement | Plan Section | Task ID | Status |
|-------------|--------------|---------|--------|
| FR-1: Create mkdocs.yml config | Technical Context + Architecture | TASK-010 | Covered |
| FR-2: Create index.md landing page | Technical Context + Architecture | TASK-011 | Covered |
| FR-3: Create requirements.txt | Technical Context | TASK-012 | Covered |
| FR-4: Add .pages files to specs folders | .pages File Pattern | TASK-013-020 | Covered |
| FR-5: Enable mermaid rendering | Key Decision: mermaid rendering | TASK-010 | Covered |
| FR-6: Enable instant search | Architecture | TASK-010 | Covered |
| FR-7: Enable dark/light mode toggle | Technical Context | TASK-010 | Covered |
| FR-8: Configure edit\_uri | Technical Context | TASK-010 | Covered |
| FR-9: Create CI workflow | Technical Context + Architecture | TASK-021 | Covered |
| FR-10: Set site\_url and repo\_url | Technical Context | TASK-010 | Covered |
| FR-11: Update .gitignore | Target Modules | TASK-001 | Covered |
| FR-12: Update CLAUDE.md | Target Modules | TASK-001 | Covered |

### Non-Functional Requirements

| Requirement | Plan Section | Task ID | Status |
|-------------|--------------|---------|--------|
| NFR-1: Build < 30 seconds | Risks & Mitigations | TASK-030 | Covered |
| NFR-2: CI < 2 minutes | Risks & Mitigations | TASK-030 | Covered |
| NFR-3: Mermaid renders unchanged | Risks & Mitigations | TASK-030 | Covered |
| NFR-4: Mobile responsive | Technical Context (Material theme) | TASK-010 | Covered |
| NFR-5: No secrets in CI | Risks & Mitigations | TASK-021 | Covered |

### Success Criteria

| Criterion | Plan Section | Task ID | Status |
|-----------|--------------|---------|--------|
| SC-1: mkdocs build exits 0 | Reviewer Checklist | TASK-030 | Covered |
| SC-2: Specs with human-readable titles | .pages File Pattern | TASK-013-020 | Covered |
| SC-3: Mermaid diagrams render as SVG | Risks & Mitigations | TASK-030 | Covered |
| SC-4: Search "NATS" returns results | Reviewer Checklist | Manual test | *Implicit in search plugin* |
| SC-5: Dark mode persists | Reviewer Checklist | TASK-010 | Covered |
| SC-6: Edit link opens correct source | Reviewer Checklist | TASK-010 | Covered |
| SC-7: Auto-deploy within 2 minutes | Risks & Mitigations | TASK-021 | Covered |
| SC-8: mkdocs serve starts | Reviewer Checklist | TASK-030 | Covered |
| SC-9: mkdocs --strict exits 0 | Reviewer Checklist | TASK-030 | Covered |

***

## Gaps Found

### GAP-1: SC-4 has no explicit task

**Issue**: Success Criterion 4 ("Search 'NATS' returns results...") is marked as implicit in TASK-030, but the verification step does not explicitly confirm search indexing.

**Evidence**:

* Spec.md line 140-141 defines the test: "search 'NATS'; verify results include 003 spec link"
* TASK-030 acceptance criteria (tasks.md L189) cover `mkdocs build`, `--strict`, and output directory but no explicit search validation
* The `search` plugin is configured in TASK-010, but no task validates search function

**Risk Level**: Low — MkDocs search plugin is built-in and reliable; however, best practice would verify it works post-deployment.

**Recommendation**: Add sub-step to TASK-030 verification: "Run `grep -r "NATS" site_build/` to confirm indexed content."

***

### GAP-2: Analysis-report.md placement for 008-specs-site itself

**Issue**: The spec.md (line 129-130) and plan.md note that `analysis-report.md` should be "intentionally included" in site navigation. However, TASK-900 (Docs & links update) does not explicitly require creation of `specs/008-specs-site/analysis-report.md`.

**Evidence**:

* Spec.md L129-130: "`pr-description.md` and `analysis-report.md` are **intentionally included**"
* Plan.md "`.pages` File Pattern" (L174) lists files "that **exist in the folder**"
* TASK-900 acceptance criteria (tasks.md L200-201) only covers "spec.md FR and SC checkboxes" and ".pages file creation" — no mention of analysis-report.md

**Risk Level**: Low — This is a feature about publishing specs, not about the spec itself needing to be perfect before implementation. The analysis-report.md (this file) is created post-implementation.

**Recommendation**: The TASK-900 acceptance criteria implicitly assumes this analysis report will exist by the time TASK-900 runs. No action needed; note clarified.

***

### GAP-3: TASK-030 verification order and local test coverage

**Issue**: TASK-030 assumes CI workflow verification happens after `mkdocs build`. However, the task definition does not include a manual `mkdocs serve` test to confirm live reload and diagram rendering in browser (SC-8).

**Evidence**:

* Spec.md L160-162 (US-4): "When: `mkdocs serve -f docs/specs/mkdocs.yml` is executed... Then: Dev server starts at `localhost:8000` with live reload"
* tasks.md TASK-030 L185-190: Acceptance criteria focus on build artifacts and strict mode, not the serve command
* TASK-030 dependencies list all implementation tasks but local serve testing is implicit

**Risk Level**: Low — `mkdocs serve` is a standard CLI feature with minimal risk. Most validation happens via build verification.

**Recommendation**: Recommend adding a manual verification step in TASK-030 to run `mkdocs serve` and confirm browser rendering, but this is not a blocker.

***

### GAP-4: GitHub Pages prerequisite not validated in CI

**Issue**: Spec.md (FR-10, L206-207) and plan.md state that GitHub Pages must be pre-enabled ("**prerequisite**: GitHub Pages must be enabled..."). However, no task validates this prerequisite before CI runs.

**Evidence**:

* Spec.md L320-322: "Prerequisite: GitHub Pages must be enabled before first deploy"
* tasks.md has no task for checking GitHub Pages configuration
* CI workflow (TASK-021) cannot fail gracefully if GitHub Pages is disabled — the `mkdocs gh-deploy` command will fail silently or with unclear error

**Risk Level**: Medium — If GitHub Pages is not enabled, CI will fail on first deploy with a 404-type error. This will confuse users.

**Recommendation**: Add a pre-deployment step in TASK-021 (CI workflow) to document the GitHub Pages prerequisite in workflow comments. Alternatively, add a TASK-025 to verify GitHub Pages settings before first merge.

***

### GAP-5: .pages files do not account for PR description consistency

**Issue**: Tasks TASK-014-020 (per-spec .pages files) list files based on what "exists in the folder," but the acceptance criteria do not account for which specs actually have pr-description.md files.

**Evidence**:

* TASK-014 (001-otel-setup): Line 124 assumes "spec.md, plan.md, tasks.md, pr-description.md"
* TASK-015 (002-cortex-setup): Line 131 same assumption
* TASK-016 (003-messaging-setup): Line 138 same assumption
* TASK-017-020: Different specs have different files listed (some have analysis-report.md, not pr-description.md)

**Risk Level**: Low — The `awesome-pages` plugin gracefully omits files from nav that don't exist. However, the task descriptions are inconsistent about which files are expected.

**Recommendation**: Before implementing TASK-014-020, audit existing specs to confirm which files exist in each folder. This was partially done in planning but needs verification.

**Action**: Check each spec folder for actual file contents.

***

### GAP-6: Edge case handling for cross-spec internal links not documented in tasks

**Issue**: Spec.md L333 mentions "Cross-spec internal links — MkDocs rewrites relative links; verify with `mkdocs build --strict`" as an edge case. However, the task for verifying this is not explicit.

**Evidence**:

* Spec.md Edge Cases table (L333): Cross-spec links are flagged as a concern
* tasks.md TASK-030 (L189): Includes `--strict` verification, which will catch broken links
* No explicit task to add or test cross-spec links

**Risk Level**: Low — `mkdocs build --strict` covers this. Edge case is handled.

**Recommendation**: No action; TASK-030 --strict validation is sufficient.

***

## Risks

### RISK-1: Relative path `docs_dir: ../../specs/` fails if mkdocs run from subdirectory

**Issue**: The mkdocs.yml hardcodes `docs_dir: ../../specs/` (plan.md L31). If a developer runs `mkdocs` from `docs/specs/` instead of repo root, the path fails.

**Evidence**:

* Spec.md L244: `docs_dir: "../../specs/"`
* Plan.md L219: "Always invoke `mkdocs` from repo root; document in CLAUDE.md"
* CLAUDE.md (user instructions) will be updated, but this is a procedural risk

**Mitigation**: TASK-001 updates CLAUDE.md with the command `mkdocs serve -f docs/specs/mkdocs.yml` (from repo root). This enforces the correct execution context.

**Status**: Mitigated by task design.

***

### RISK-2: mermaid.js CDN unavailable or version mismatch

**Issue**: Spec relies on CDN for mermaid.js (`https://unpkg.com/mermaid@10/dist/mermaid.min.js`, L290). If unpkg.com is down or version 10 is deprecated, diagrams fail to render.

**Evidence**:

* Spec.md L290: `extra_javascript: [https://unpkg.com/mermaid@10/dist/mermaid.min.js]`
* Plan.md L222: "CDN mermaid.js unavailable (offline CI) — Low impact"

**Mitigation**: GitHub Actions runners have internet access; unpkg.com is highly available (Fastly CDN). If needed, mermaid can be vendored into repo, but that is premature now.

**Status**: Low risk; acceptable.

***

### RISK-3: Concurrent CI runs could cause race condition on gh-pages branch

**Issue**: If two PRs merge to main simultaneously, two CI workflows may race to push to gh-pages, causing one deploy to be lost.

**Evidence**:

* Spec.md L334: "Concurrent CI runs — `mkdocs gh-deploy --force` is atomic; last deploy wins"
* Plan.md does not flag this as a risk

**Mitigation**: `mkdocs gh-deploy --force` is atomic at the Git level (force-push is atomic). The "last deploy wins" behavior is acceptable for a documentation site.

**Status**: Acceptable; documented in spec as edge case.

***

### RISK-4: .pages file not committed in same PR as new spec

**Issue**: If a new spec folder is added without a corresponding .pages file, the build will succeed but the spec will appear with an auto-generated title (raw folder name) instead of human-readable title.

**Evidence**:

* Spec.md L329: "New spec added without `.pages` file — Site builds; spec appears with raw folder name"
* Plan.md mentions this as a procedural note, not a hard requirement

**Mitigation**: Add procedural documentation: "When creating a new spec folder, always add a .pages file in the same PR." CLAUDE.md update could include this guidance.

**Status**: Procedural; not a code blocker.

***

### RISK-5: TASK-030 does not explicitly test mermaid rendering in browser

**Issue**: SC-3 (spec.md L342) requires mermaid diagrams to "render as SVGs (verified in browser)." TASK-030 verification runs `mkdocs build` and checks output directory but does not include a browser test.

**Evidence**:

* Spec.md SC-3 (L342): "Mermaid diagrams... render as SVGs (verified in browser)"
* tasks.md TASK-030: Acceptance criteria focus on build success, not browser verification

**Mitigation**: This is a manual smoke test that should be performed before declaring SC-3 complete. Recommend adding to TASK-030 or TASK-999 (Reviewer).

**Status**: Covered by TASK-999 reviewer checklist (implicit), but could be more explicit.

***

## Parallel Opportunities

### Opportunity-1: TASK-014-020 are fully independent and can run in parallel

**Status**: Already marked \[P] in tasks.md (lines 14-22).

The seven .pages files (TASK-014-020) have zero dependencies and do not share any file I/O. They are correctly marked for parallel execution.

**Recommendation**: No change. Design is optimal.

***

### Opportunity-2: TASK-010, TASK-011, TASK-012, TASK-013 could run in parallel with TASK-021

**Status**: Partially optimized.

TASK-021 (CI workflow) depends on TASK-001 but does not depend on any other Phase 2 tasks. The CI workflow could be written in parallel with mkdocs.yml, index.md, requirements.txt, and .pages files. All Phase 2 tasks are marked \[P] and can run concurrently.

**Recommendation**: No change. Design is correct.

***

### Opportunity-3: TASK-030 verification steps could be broken into sub-tasks for parallel review

**Observation**: TASK-030 includes multiple independent verification steps:

1. pip install
2. mkdocs build
3. mkdocs build --strict
4. Directory inspection

These could be parallelized if needed, but for a single build step, serialization is fine.

**Recommendation**: No change; serial verification is appropriate.

***

## Constitution Compliance

### Principle I: Zero-Dependency CLI

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature modifies docs and CI, not CLI.

***

### Principle II: Platform-in-a-Box

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature is documentation only; no service changes.

***

### Principle III: Modular Services Architecture

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature is documentation only; no service changes.

***

### Principle IV: Two-Brain Separation

**Applies**: NO\
**Status**: N/A\
**Reason**: No code or intelligence layer involved. Configuration and static markdown only.

***

### Principle V: Polyglot Standards

**Applies**: YES\
**Status**: PASS\
**Evidence**:

* Python tooling (MkDocs) is consistent with SDK and services (both use Python-based documentation)
* No custom code; all comments in markdown follow spec conventions
* Configuration files (YAML) follow standard patterns
* No hardcoded secrets or environment-specific values

***

### Principle VI: Local-First Architecture

**Applies**: YES\
**Status**: PASS\
**Evidence**:

* `mkdocs serve` works fully offline after `pip install -r docs/specs/requirements.txt`
* No external API calls required for local preview
* Development workflow is self-contained: clone repo → pip install → mkdocs serve
* CI deployment is the only operation requiring network (GitHub Pages)

***

### Principle VII: Observability by Default

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature is documentation only; no services to instrument.

***

### Principle VIII: Security by Default

**Applies**: YES\
**Status**: PASS\
**Evidence**:

* No secrets committed to git
* CI uses automatic `GITHUB_TOKEN` with minimal `contents: write` permission (FR-9, tasks.md L175)
* No sensitive data in built site
* No credentials in environment variables
* site\_build/ is gitignored (FR-11)

***

### Principle IX: Declarative Reconciliation

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature has no CLI components.

***

### Principle X: Stateful Operations

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature has no CLI components.

***

### Principle XI: Resilience Testing

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature is static documentation. GitHub Pages provides 99.9% SLA inherently.

***

### Principle XII: Interactive Experience

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature has no CLI components.

***

## Patterns Compliance

### Pattern 1: Factory/Dependency Injection

**Applies**: NO\
**Status**: N/A\
**Reason**: No code; configuration only.

***

### Pattern 2: Repository Pattern

**Applies**: NO\
**Status**: N/A\
**Reason**: No code; configuration only.

***

### Pattern 3: Configuration Precedence

**Applies**: YES\
**Status**: PASS\
**Evidence**:

* mkdocs.yml uses environment-agnostic configuration
* build output path (`site_dir`) is relative, allowing override if needed
* CI configuration uses standard GitHub Actions environment variables (GITHUB\_TOKEN)

***

### Pattern 4: Error Handling

**Applies**: NO\
**Status**: N/A\
**Reason**: No custom code; relying on MkDocs and GitHub Actions error messages.

***

### Pattern 5: Testing Standards

**Applies**: NO\
**Status**: N/A\
**Reason**: Configuration and static content; no unit tests or parametrized tests applicable.

***

### Pattern 6: XDG Base Directory

**Applies**: NO\
**Status**: N/A\
**Reason**: Feature is not CLI-specific.

***

### Pattern 7: Observability Pattern

**Applies**: NO\
**Status**: N/A\
**Reason**: No services; no telemetry needed.

***

### Pattern 8: UI Service Pattern

**Applies**: NO\
**Status**: N/A\
**Reason**: No CLI components.

***

## Summary

**Total Gaps Found**: 6 (all Low-Medium severity, none blocking)
**Total Risks Identified**: 5 (all mitigated or acceptable)
**Constitution Violations**: 0 FAIL
**Pattern Violations**: 0 FAIL

### Gap Summary

| Gap | Severity | Status |
|-----|----------|--------|
| GAP-1: SC-4 search validation implicit | Low | Resolved by mkdocs search plugin; add grep step to TASK-030 if needed |
| GAP-2: analysis-report.md for 008-specs-site | Low | N/A; report created post-implementation |
| GAP-3: mkdocs serve browser test not explicit | Low | Covered by TASK-030 + TASK-999 reviewer checklist |
| GAP-4: GitHub Pages prerequisite not validated | Medium | Mitigation: Document in CLAUDE.md; CI will fail clearly if not enabled |
| GAP-5: .pages files inconsistent file assumptions | Low | Mitigation: Audit existing specs before implementation |
| GAP-6: Cross-spec link edge case | Low | Covered by `mkdocs --strict` in TASK-030 |

### Risk Summary

| Risk | Level | Mitigation |
|------|-------|-----------|
| RISK-1: Relative path failure | Medium | Documented in CLAUDE.md; enforced by command format |
| RISK-2: CDN mermaid.js unavailable | Low | CDN is reliable; vendoring not necessary now |
| RISK-3: Concurrent CI race | Low | Atomic force-push behavior is acceptable |
| RISK-4: New spec without .pages | Low | Procedural; add to contributor docs |
| RISK-5: Browser mermaid rendering test | Low | Manual verification recommended; covered by reviewer |

***

## Recommendations

### Before Implementation (DO NOT BLOCK)

1. **Audit existing spec folders** (optional, pre-TASK-014-020):

   * List actual files in each spec folder (001-007)
   * Verify pr-description.md vs. analysis-report.md presence
   * Update TASK-014-020 acceptance criteria if needed

   Example:

   ```bash
   for dir in specs/00{1..7}-*/; do echo "=== $dir ==="; ls "$dir"/*.md 2>/dev/null || echo "none"; done
   ```

2. **Document GitHub Pages prerequisite**:
   * Update CLAUDE.md or .github/CONTRIBUTING.md with note: "GitHub Pages must be enabled (repo Settings → Pages → 'Deploy from a branch' → gh-pages)"
   * This prevents confusion on first deploy

3. **Add search validation to TASK-030**:
   * Optional: Add `grep -r "NATS" site_build/` to confirm search indexing
   * Or note that MkDocs search is tested by browser verification

### Proceed to Implementation

**Recommendation**: PROCEED — No blockers identified.

* All 16 tasks have clear, testable acceptance criteria
* All functional and non-functional requirements are covered
* Constitution and patterns compliance are satisfied (or N/A)
* Parallel execution strategy is sound
* Risks are identified and mitigated

**Next Steps**:

1. Run `make dev` to verify monorepo builds
2. Execute TASK-001 through TASK-030 in order
3. Have reviewer agent (TASK-999) verify all checklist items
4. Merge PR and monitor first CI deploy to gh-pages

***

**Analysis conducted**: 2026-03-01 (pre-implementation)\
**Status**: READY FOR IMPLEMENTATION

---

---
url: /arc-platform/specs-site/001-otel-setup/spec.md
---
# Feature: 001-otel-setup — Stand Up SigNoz as the Observability Backend

> **Spec**: 001-otel-setup
> **Date**: 2026-02-21
> **Status**: Draft

## Target Modules

| Module | Path | Impact |
|--------|------|--------|
| Services | `services/otel/observability/` | New directory — SigNoz UI, ClickHouse, ZooKeeper |
| Services | `services/otel/telemetry/` | New directory — OTEL collector (Black Widow) |
| Root | `Makefile` | Extend with `otel-*` targets — single orchestration entrypoint |

No CLI, SDK, or docs changes in this spec.

## Overview

Add SigNoz as the A.R.C. observability backend. Two service directories live under `services/otel/` — `telemetry/` (OTEL collector) and `observability/` (SigNoz + storage). Each owns its own `docker-compose.yml`; all operations are invoked from the **single root `Makefile`** via `otel-*` targets (e.g. `make otel-up`). Raw `docker compose` is never the intended interface. Once running, the SigNoz UI is reachable at `localhost:3301` and the collector accepts OTLP signals on ports 4317/4318. This spec covers infrastructure setup only; SDK instrumentation is a future spec.

## Architecture

```mermaid
graph TD
    SDK["OTEL SDK\n(any future arc service)"]
    SDK -->|"OTLP gRPC :4317\nOTLP HTTP :4318"| BW

    subgraph otel ["services/otel/"]
        subgraph telemetry ["telemetry/ — Black Widow"]
            BW["signoz-otel-collector"]
        end

        subgraph observability ["observability/ — Friday"]
            ZK["ZooKeeper\n(ClickHouse coordination)"]
            CH["ClickHouse\n(signal store)"]
            SN["SigNoz\n(query service + UI :3301)"]
            ZK --> CH
            CH --> SN
        end
    end

    BW -->|"OTLP export"| CH
    BW -.->|"health :13133"| BW
    SN -.->|"API health :3301/api/v1/health"| SN

    MK["arc-platform/Makefile\n(single root — otel-up / otel-down / otel-health)"]
    MK -->|orchestrates| telemetry
    MK -->|orchestrates| observability
```

## Makefile Design

**Single root `Makefile`** at `arc-platform/Makefile`. Pattern mirrors platform-spike: compose variables point at per-service compose files; no `cd` required.

```makefile
# Compose variable references (root Makefile additions)
COMPOSE                  := docker compose
COMPOSE_OTEL_TELEMETRY   := $(COMPOSE) -f services/otel/telemetry/docker-compose.yml
COMPOSE_OTEL_OBSERVABILITY := $(COMPOSE) -f services/otel/observability/docker-compose.yml
COMPOSE_OTEL             := $(COMPOSE_OTEL_TELEMETRY) \
                            -f services/otel/observability/docker-compose.yml

# Target reference
make otel-up               # full otel stack (telemetry + observability)
make otel-up-observability # SigNoz + ClickHouse + ZooKeeper only
make otel-up-telemetry     # collector only
make otel-down             # tear down full stack
make otel-health           # probe both health endpoints; exit non-zero on failure
make otel-logs             # stream logs from all otel containers
make otel-ps               # show container status
```

## User Scenarios & Testing

### P1 — Must Have

**US-1**: As a developer, I want to start the full SigNoz stack with a single make command so that the UI is reachable without manual steps.

* **Given**: `make otel-up` is run from the repo root (`arc-platform/`)
* **When**: All containers report healthy (ClickHouse, ZooKeeper, signoz-otel-collector, SigNoz)
* **Then**: `http://localhost:3301` returns the SigNoz UI within 60 seconds
* **Test**: `curl -sf http://localhost:3301/api/v1/health` returns HTTP 200; `make otel-health` exits 0

**US-2**: As a developer, I want the OTEL collector to accept a test span so that I can verify the full pipeline before instrumenting any arc service.

* **Given**: `make otel-up` has been run and `make otel-health` exits 0
* **When**: A test span is sent to `localhost:4317` via `telemetrygen`
* **Then**: The span appears in SigNoz → Traces within 30 seconds
* **Test**: `docker run --rm --network host ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:latest traces --otlp-insecure --otlp-endpoint localhost:4317` then verify in SigNoz UI

## Requirements

### Functional

* \[ ] FR-1: `services/otel/observability/` directory exists with `service.yaml`, `docker-compose.yml`, and `config/`
* \[ ] FR-2: `services/otel/telemetry/` directory exists with `service.yaml`, `docker-compose.yml`, and `config/otel-collector-config.yaml`
* \[ ] FR-3: Root `Makefile` (`arc-platform/Makefile`) is extended with `otel-up`, `otel-up-observability`, `otel-up-telemetry`, `otel-down`, `otel-health`, `otel-logs`, `otel-ps` targets
* \[ ] FR-4: `make otel-up` from repo root starts all four containers (SigNoz, ClickHouse, ZooKeeper, signoz-otel-collector)
* \[ ] FR-5: `make otel-health` probes both health endpoints and exits non-zero if either fails
* \[ ] FR-6: SigNoz UI is reachable at `http://localhost:3301` after `make otel-up` completes healthy
* \[ ] FR-7: OTEL collector accepts OTLP gRPC on `:4317` and OTLP HTTP on `:4318`
* \[ ] FR-8: `otel-collector-config.yaml` is the single place to configure receivers and exporters — no per-service collector configs
* \[ ] FR-9: Health check for `observability/` passes only when the SigNoz API (`/api/v1/health`) responds, not just when containers are running
* \[ ] FR-10: ClickHouse and ZooKeeper are NOT exposed externally (internal Docker network only)
* \[ ] FR-11: SigNoz UI is bound to `127.0.0.1` only (not `0.0.0.0`)
* \[ ] FR-12: `make otel-down` cleanly stops and removes all otel stack containers
* \[ ] FR-13: All otel containers follow `arc-friday-*` naming — `arc-friday`, `arc-friday-collector`, `arc-friday-clickhouse`, `arc-friday-zookeeper`, `arc-friday-migrator-sync`, `arc-friday-migrator-async`
* \[ ] FR-14: All ARC images published to `ghcr.io/arc-framework/arc-friday-*` (vendor re-tag strategy — Approach C from platform-spike)
* \[ ] FR-15: Vendor images (`arc-friday`, `arc-friday-collector`, `arc-friday-clickhouse`, `arc-friday-zookeeper`) built via `.github/workflows/otel-images.yml`; weekly re-tag config at `.github/config/publish-observability.json`
* \[ ] FR-16: `make otel-build` produces locally tagged images matching `ghcr.io/arc-framework/arc-friday-*` refs so dev and CI use identical image names

### Non-Functional

* \[ ] NFR-1: **ClickHouse minimum RAM: 4GB** — document as prerequisite in `services/otel/observability/README.md`
* \[ ] NFR-2: SigNoz UI must be reachable within 60 seconds of all containers starting
* \[ ] NFR-3: Collector health endpoint (`http://localhost:13133/`) must return 200 when running
* \[ ] NFR-4: All containers run as non-root (Principle VIII)
* \[ ] NFR-5: No secrets committed to git; any credentials use env files listed in `.gitignore`

### Key Entities

| Entity | Module | Description |
|--------|--------|-------------|
| `Makefile` (extended) | `arc-platform/` | Root orchestration — `otel-*` targets added; single entrypoint for all services |
| `service.yaml` (friday) | `services/otel/observability/` | Codename, tech, health endpoint, compose services list |
| `service.yaml` (widow) | `services/otel/telemetry/` | Codename, tech, ports, health endpoint, upstream image |
| `docker-compose.yml` | `services/otel/observability/` | SigNoz + ClickHouse + ZooKeeper stack |
| `docker-compose.yml` | `services/otel/telemetry/` | signoz-otel-collector container |
| `otel-collector-config.yaml` | `services/otel/telemetry/config/` | OTLP receivers, SigNoz exporter, health extension |

### service.yaml Shapes

**services/otel/telemetry/service.yaml** (Black Widow):

```yaml
codename: widow
tech: signoz-otel-collector
upstream: signoz/signoz-otel-collector
ports:
  - 4317   # OTLP gRPC
  - 4318   # OTLP HTTP
health: http://localhost:13133/
```

**services/otel/observability/service.yaml** (Friday):

```yaml
codename: friday
tech: signoz
depends_on:
  - telemetry
health: http://localhost:3301/api/v1/health
compose_services:
  - signoz
  - clickhouse
  - zookeeper
```

## Image Strategy

Follows Approach C from platform-spike: two-tier model.

* **ARC native services** (sherlock, scarlett, raymond): built FROM `arc-base-python-ai` or `arc-base-go-infra` — custom Dockerfiles with ARC-specific tooling baked in.
* **Vendor/infra services** (arc-friday, arc-friday-collector, arc-friday-clickhouse, arc-friday-zookeeper): simple re-tag of upstream SigNoz/ClickHouse images under `ghcr.io/arc-framework/arc-friday-*`. ARC-specific config (ClickHouse tuning, collector endpoints, histogramQuantile UDF) is baked into the Dockerfile layers at build time. Weekly re-tag via `publish-vendor-images.yml` ensures upstream security patches propagate automatically.

| Image | Source | Dockerfile | Config baked in |
|-------|--------|------------|-----------------|
| `arc-friday` | `signoz/signoz:v0.112.0` | `observability/Dockerfile` | `prometheus.yml` |
| `arc-friday-collector` | `signoz/signoz-otel-collector:v0.142.0` | `telemetry/Dockerfile` | collector config + opamp config |
| `arc-friday-clickhouse` | `clickhouse/clickhouse-server:25.5.6` | `observability/clickhouse.Dockerfile` | ClickHouse config, users.xml, `histogramQuantile` UDF binary |
| `arc-friday-zookeeper` | `signoz/zookeeper:3.7.1` | `observability/zookeeper.Dockerfile` | ARC labels only |

Re-tag config: `.github/config/publish-observability.json`
Build workflow: `.github/workflows/otel-images.yml`
Reusable CI primitives: `.github/workflows/_reusable-build.yml`, `_reusable-security.yml`, `_reusable-publish-group.yml`

## Edge Cases

| Scenario | Expected Behavior |
|----------|-------------------|
| ClickHouse OOM (< 4GB RAM) | Container exits; `make otel-health` fails; error visible in `make otel-logs` |
| SigNoz starts before ClickHouse is ready | `depends_on` with `condition: service_healthy` in compose prevents premature start |
| Collector starts before SigNoz/ClickHouse is ready | Collector retries export; spans buffered up to queue limit; no data loss for short startup gaps |
| Port 3301 already in use | `make otel-up` fails at bind; error message identifies conflicting port |
| Port 4317/4318 already in use | `make otel-up` fails at bind; error message identifies conflicting port |
| `make otel-up-observability` run without telemetry | SigNoz starts and is accessible; no collector means no ingestion (expected, documented) |

## Success Criteria

* \[ ] SC-1: `make otel-up` from repo root exits 0 and all four containers reach healthy state
* \[ ] SC-2: `make otel-health` exits 0 after `make otel-up`; exits non-zero if any endpoint is unreachable
* \[ ] SC-3: `curl -sf http://localhost:3301/api/v1/health` returns HTTP 200
* \[ ] SC-4: `curl -sf http://localhost:13133/` returns HTTP 200 (collector health)
* \[ ] SC-5: A test span sent to `:4317` appears in SigNoz → Traces within 30 seconds
* \[ ] SC-6: ClickHouse port is NOT accessible from the host (`nc -z localhost 9000` fails)
* \[ ] SC-7: ZooKeeper port is NOT accessible from the host (`nc -z localhost 2181` fails)

## Docs & Links Update

* \[ ] Create `services/otel/observability/README.md` — prerequisites (4GB RAM), `make up` quickstart, health check URL, UI URL
* \[ ] Create `services/otel/telemetry/README.md` — collector config reference: open receivers (OTLP gRPC/HTTP), export target, health endpoint

## Constitution Compliance

| Principle | Applies | Compliant | Notes |
|-----------|---------|-----------|-------|
| I. Zero-Dependency CLI | N/A | — | CLI scope only |
| II. Platform-in-a-Box | ✓ | ✓ | `make otel-up` works with zero manual config; root Makefile is the declared interface |
| III. Modular Services | ✓ | ✓ | Each subdirectory is self-contained with own `service.yaml`; root Makefile references compose files directly |
| IV. Two-Brain Separation | N/A | — | Infrastructure config only; no Go/Python mixing |
| V. Polyglot Standards | N/A | — | OTEL SDK instrumentation is the next spec |
| VI. Local-First | N/A | — | CLI scope only |
| VII. Observability by Default | ✓ | ✓ | This spec IS the observability foundation |
| VIII. Security by Default | ✓ | ✓ | Non-root containers; ClickHouse/ZK not externally exposed; UI bound to 127.0.0.1 |
| IX. Declarative Reconciliation | N/A | — | CLI scope only |
| X. Stateful Operations | N/A | — | CLI scope only |
| XI. Resilience Testing | N/A | — | Chaos engineering is a follow-on concern |
| XII. Interactive Experience | N/A | — | CLI scope only |

---

---
url: /arc-platform/specs-site/002-cortex-setup/spec.md
---
# Feature: Cortex Bootstrap Service

> **Spec**: 002-cortex-setup
> **Author**: arc-team
> **Date**: 2026-02-22
> **Status**: Draft

## Target Modules

| Module | Path | Impact |
|--------|------|--------|
| Services | `services/cortex/` | New service — Cortex bootstrap orchestrator (Go) |
| Docs | `docs/` | Add Cortex to the platform service catalogue |

## Overview

Cortex (`services/cortex/`, codename: `cortex`) is the A.R.C. platform bootstrap service — the spark plug that provisions all infrastructure at startup. The **primary interface is HTTP**: Heimdall (Traefik) routes `POST /api/v1/bootstrap` to Cortex's Gin API, which delegates to the Orchestrator Core. A Cobra CLI command (`cortex bootstrap`) is a secondary trigger for direct operator use. Cortex provisions NATS JetStream streams, Pulsar tenants and topics, validates the Postgres schema, and verifies Redis connectivity. All activity is traced and metered via the Friday OTEL stack.

## Architecture

```mermaid
graph TD
    %% Inputs — API is primary
    Gateway[Heimdall / Traefik] -->|HTTP POST /api/v1/bootstrap| Gin[Gin API Adapter\ninternal/api]
    CLI[arc CLI / Dev Terminal] -.->|cortex bootstrap| Cobra[Cobra CLI Adapter\ncmd/cortex]

    %% Core — Hexagonal
    subgraph Cortex [Cortex Service — services/cortex/]
        Gin --> Core[Orchestrator Core\ninternal/orchestrator]
        Cobra --> Core

        Core --> NATSClient[NATS Client\ninternal/clients/nats.go]
        Core --> PulsarClient[Pulsar Client\ninternal/clients/pulsar.go]
        Core --> DBClient[Postgres Client\ninternal/clients/postgres.go]
        Core --> RedisClient[Redis Client\ninternal/clients/redis.go]
        Core --> Otel[Telemetry Module\ninternal/telemetry/friday.go]
    end

    %% Infrastructure
    NATSClient -->|Create Streams| Flash[(arc-messaging\nNATS JetStream :4222)]
    PulsarClient -->|Create Tenant + Topics| Strange[(arc-streaming\nPulsar :8080 / :6650)]
    DBClient -->|Ping & Migrate| Oracle[(arc-sql-db\nPostgres :5432)]
    RedisClient -->|Ping & Verify| Sonic[(arc-cache\nRedis :6379)]

    %% Observability
    Otel -.->|OTLP gRPC :4317| Collector[arc-friday-collector]
    Collector -.->|All Signals| ArcFriday[arc-friday: SigNoz]
    ArcFriday -.->|Storage| ClickHouse[(ClickHouse)]

    classDef core fill:#2b2d42,stroke:#8d99ae,stroke-width:2px,color:#fff
    classDef infra fill:#ef233c,stroke:#2b2d42,stroke-width:2px,color:#fff
    classDef obs fill:#8d99ae,stroke:#2b2d42,stroke-width:2px,color:#fff
    classDef storage fill:#6d6875,stroke:#2b2d42,stroke-width:2px,color:#fff

    class Cortex core
    class Flash,Strange,Oracle,Sonic infra
    class Collector,ArcFriday obs
    class ClickHouse storage
```

### Directory Structure

Strict Go project layout — no dumping everything in `main.go`.

```text
services/cortex/
├── cmd/
│   └── cortex/
│       ├── main.go             # Wire dependencies here
│       ├── root.go             # Cobra root command
│       ├── server.go           # `cortex server` — start Gin API
│       └── bootstrap.go        # `cortex bootstrap` — one-shot CLI execution
├── internal/
│   ├── api/                    # HTTP Transport Adapter
│   │   ├── server.go           # Gin router setup
│   │   ├── handlers.go         # HTTP handlers calling orchestrator
│   │   └── middleware.go       # Friday (OTEL) & Recovery middlewares
│   ├── orchestrator/           # Core Domain Logic (The Brain)
│   │   ├── service.go          # RunBootstrap(), RunDeepHealth()
│   │   └── types.go            # BootstrapResult, PhaseResult, ProbeResult
│   ├── clients/                # Infrastructure Adapters
│   │   ├── nats.go             # NATS JetStream setup + circuit breaker
│   │   ├── pulsar.go           # Pulsar tenant/namespace/topic setup + circuit breaker
│   │   ├── postgres.go         # pgx pool + schema validation + circuit breaker
│   │   └── redis.go            # go-redis ping + circuit breaker
│   ├── config/
│   │   └── config.go           # Viper/Infisical bindings
│   └── telemetry/
│       └── friday.go           # OTEL provider — traces + metrics → arc-friday-collector
├── service.yaml                # codename: cortex, depends_on, health endpoint
├── Dockerfile
├── go.mod
└── go.sum
```

### HTTP API

The Gin router is the primary interface. All routes are under the Cortex server (`cortex server`):

| Method | Path | Handler | Auth | Description |
|--------|------|---------|------|-------------|
| `POST` | `/api/v1/bootstrap` | `handlers.Bootstrap` | — | Trigger full platform bootstrap |
| `GET` | `/health` | `handlers.Health` | — | Shallow health — always 200 |
| `GET` | `/health/deep` | `handlers.DeepHealth` | — | Deep probe — calls `RunDeepHealth()` |
| `GET` | `/ready` | `handlers.Ready` | — | Readiness — 503 until bootstrap complete |

**Middleware chain** (applied in order):

1. `middleware.Recovery(logger)` — panic → 500, log stack, continue
2. `middleware.FridayOTEL("arc-cortex")` — inject trace context per request (otelgin)
3. `middleware.RequestLogger(logger)` — structured request/response logging

**Bootstrap request/response:**

```
POST /api/v1/bootstrap
→ 200 {"status":"ok","phases":{"postgres":"ok","nats":"ok","pulsar":"ok","redis":"ok"}}
→ 409 {"status":"in-progress"}           (if already running)
→ 500 {"status":"error","phase":"nats","error":"..."}
```

**Deep health response:**

```
GET /health/deep
→ 200 {"status":"healthy","dependencies":{
    "arc-sql-db": {"ok":true,"latencyMs":2,"error":""},
    "arc-messaging":  {"ok":true,"latencyMs":1,"error":""},
    "arc-streaming":{"ok":true,"latencyMs":5,"error":""},
    "arc-cache":  {"ok":true,"latencyMs":1,"error":""}
  }}
→ 503  (same shape, "ok":false on failing deps)
```

### Bootstrap Phase Sequence

```mermaid
sequenceDiagram
    participant GW as Heimdall / CLI
    participant API as Gin API (handlers.Bootstrap)
    participant Core as Orchestrator (RunBootstrap)
    participant PG as arc-sql-db
    participant NATS as arc-messaging
    participant Pulsar as arc-streaming
    participant Redis as arc-cache
    participant Friday as arc-friday-collector

    GW->>API: POST /api/v1/bootstrap
    API->>Core: RunBootstrap(ctx)
    Core->>Friday: Span Start — "platform.bootstrap"

    par Phase 1
        Core->>PG: Ping & Check Migrations
        PG-->>Core: OK
    and Phase 2
        Core->>NATS: CreateStream(AGENT_COMMANDS)
        Core->>NATS: CreateStream(AGENT_EVENTS)
        Core->>NATS: CreateStream(SYSTEM_METRICS)
        NATS-->>Core: Streams ready
    and Phase 3
        Core->>Pulsar: Create tenant arc-system
        Core->>Pulsar: Create namespaces + topics
        Pulsar-->>Core: Provisioned
    and Phase 4
        Core->>Redis: Ping
        Redis-->>Core: PONG
    end

    Core->>Friday: Span End — success
    Core-->>API: BootstrapResult{phases: all ok}
    API-->>GW: 200 OK
```

## Libraries

Per the A.R.C. Go Standard (`cortex.md` §4):

| Concern | Library |
|---------|---------|
| CLI | `github.com/spf13/cobra` |
| Configuration | `github.com/spf13/viper` |
| HTTP Router | `github.com/gin-gonic/gin` |
| OTEL middleware | `go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin` |
| Logging | `log/slog` (stdlib) |
| Telemetry | `go.opentelemetry.io/otel` + `otlptracegrpc` + `otlpmetricgrpc` |
| PostgreSQL | `github.com/jackc/pgx/v5` |
| NATS | `github.com/nats-io/nats.go` |
| Pulsar | `github.com/apache/pulsar-client-go/pulsar` |
| Redis | `github.com/redis/go-redis/v9` |
| Secrets | Infisical Go SDK (`github.com/infisical/go-sdk`) — or Viper `.env` injection |
| Feature Flags | `github.com/open-feature/go-sdk` with Unleash provider |
| Circuit Breaker | `github.com/sony/gobreaker` |
| Retry / Backoff | `github.com/cenkalti/backoff/v4` |
| Actor lifecycle | `github.com/oklog/run` |

## User Scenarios & Testing

### P1 — Must Have

**US-1**: As a platform operator, I want `POST /api/v1/bootstrap` via Heimdall to provision all infra and return a phase-by-phase result.

* **Given**: Cortex is running; all four infra services are reachable
* **When**: `POST /api/v1/bootstrap`
* **Then**: `200 {"status":"ok","phases":{"postgres":"ok","nats":"ok","pulsar":"ok","redis":"ok"}}`; NATS streams and Pulsar topics exist
* **Test**: `curl -X POST http://localhost:8081/api/v1/bootstrap`; then `nats stream ls` and `pulsar-admin topics list`

**US-2**: As Heimdall, I want `GET /health` to return 200 immediately at startup so the gateway can route traffic before bootstrap completes.

* **Given**: Cortex container starts with valid config
* **When**: `GET /health` (within 1s of container start)
* **Then**: `200 {"status":"healthy","mode":"shallow"}`
* **Test**: `curl -sf http://localhost:8081/health` immediately after `docker compose up cortex`

**US-3**: As a platform operator, I want `GET /health/deep` to call `RunDeepHealth()` and return per-dependency probe results.

* **Given**: arc-sql-db is down; others healthy
* **When**: `GET /health/deep`
* **Then**: `503 {"status":"unhealthy","dependencies":{"arc-sql-db":{"ok":false,"latencyMs":...,"error":"..."},...}}`
* **Test**: Stop arc-sql-db; call `/health/deep`; assert 503 with oracle unhealthy entry

**US-4**: As the platform runtime, I want `GET /ready` to gate dependent services — 503 while bootstrap runs, 200 after.

* **Given**: Bootstrap is in progress
* **When**: `GET /ready`
* **Then**: `503 {"ready":false}`; after all phases succeed → `200 {"ready":true}`
* **Test**: Poll `/ready` during startup; assert state transitions

**US-5**: As a platform operator, I want NATS streams provisioned with exact config so agents have messaging infrastructure.

* **Given**: `arc-messaging` at `nats://arc-messaging:4222`
* **When**: Bootstrap Phase 2 completes
* **Then**: Three streams with exact subjects and retention (see FR-4)
* **Test**: `nats stream info AGENT_COMMANDS` confirms subjects, retention, max-age

**US-6**: As a platform operator, I want `cortex bootstrap` (CLI) to run the same bootstrap and exit 0.

* **Given**: All infra services are running
* **When**: `docker run arc-cortex bootstrap`
* **Then**: Exit code 0; same provisioning as HTTP trigger; OTEL span emitted
* **Test**: `docker run arc-cortex bootstrap; echo $?`

### P2 — Should Have

**US-7**: As a platform operator, I want bootstrap phases to retry with exponential backoff so transient infra gaps don't fail permanently.

* **Given**: arc-messaging is temporarily unreachable
* **When**: Phase 2 executes
* **Then**: Retries 2s→30s backoff, up to 5 min; succeeds once arc-messaging comes online
* **Test**: Start Cortex before arc-messaging; bring arc-messaging up within 2 min; verify streams created

**US-8**: As a platform SRE, I want client calls protected by circuit breakers so a flapping dep doesn't stall the service.

* **Given**: A dep fails 3 consecutive times
* **When**: 4th attempt occurs
* **Then**: Circuit open; fast-fails with `ErrCircuitOpen`; resets after 30s
* **Test**: Unit test — mock client to fail 3×; assert 4th call returns immediately

**US-9**: As a platform SRE, I want OTEL traces and metrics exported to arc-friday so bootstrap visibility is built-in.

* **Given**: `arc-friday-collector` running on `:4317`
* **When**: Bootstrap runs
* **Then**: Traces in arc-friday under `arc-cortex`; `cortex.bootstrap.phase_duration_seconds` metric per phase
* **Test**: `make otel-up && docker compose up cortex`; check arc-friday UI

**US-10**: As a platform operator, I want secrets (DB password, Redis auth) fetched from Infisical (Nick Fury) so credentials are never baked into config files or env vars.

* **Given**: Nick Fury (Infisical) is running and Cortex has a service token
* **When**: Cortex starts and `config.Load()` runs
* **Then**: Postgres password and Redis password come from Infisical; no plaintext secrets in container env
* **Test**: Set `INFISICAL_TOKEN`; verify Cortex connects to Postgres without `POSTGRES_PASSWORD` env var

### P3 — Nice to Have

**US-11**: As a platform operator, I want feature flags (via Unleash/Mystique) to control which bootstrap phases run so I can toggle phases without redeploying.

* **Given**: Mystique (Unleash) is running; flag `cortex.bootstrap.pulsar.enabled` is `false`
* **When**: Bootstrap runs
* **Then**: Phase 3 (Pulsar) is skipped; other phases run normally
* **Test**: Disable Pulsar flag in Unleash; call `/api/v1/bootstrap`; confirm no Pulsar provisioning and 200 response

**US-12**: As a platform operator, I want dependency health monitored every 30s post-bootstrap so drift is detected automatically.

* **Given**: Cortex is running; arc-cache goes down after bootstrap
* **When**: 30s monitoring cycle runs
* **Then**: Degraded state logged with slog; `/health/deep` reflects it
* **Test**: Kill arc-cache post-bootstrap; wait 30s; check logs and `/health/deep`

## Requirements

### Functional

* \[ ] FR-1: Expose two Cobra commands: `cortex server` (primary — start Gin API) and `cortex bootstrap` (secondary — one-shot CLI)
* \[ ] FR-2: Gin API routes: `POST /api/v1/bootstrap`, `GET /health`, `GET /health/deep`, `GET /ready`
* \[ ] FR-3: Middleware chain on all routes: `Recovery` → `FridayOTEL` (otelgin, service name `arc-cortex`) → `RequestLogger`
* \[ ] FR-4: Bootstrap Phase 1 — Postgres: `Ping` + `ValidateSchema("public")` via pgx pool; host `arc-sql-db:5432`
* \[ ] FR-5: Bootstrap Phase 2 — NATS JetStream on `arc-messaging:4222`:
  * `AGENT_COMMANDS` — subjects `["agent.*.cmd"]`, retention `limits`, max-age `24h`
  * `AGENT_EVENTS` — subjects `["agent.*.event","agent.*.status"]`, retention `interest`, max-age `168h`
  * `SYSTEM_METRICS` — subjects `["metrics.>"]`, retention `limits`, max-age `6h`
* \[ ] FR-6: Bootstrap Phase 3 — Pulsar on `arc-streaming:8080`:
  * Tenant: `arc-system`
  * Namespaces: `events`, `logs`, `audit`
  * Topics: `persistent://arc-system/events/agent-lifecycle` (3 partitions), `persistent://arc-system/logs/application` (4 partitions), `persistent://arc-system/audit/command-log` (1 partition)
* \[ ] FR-7: Bootstrap Phase 4 — Redis `PING` to `arc-cache:6379`
* \[ ] FR-8: All four phases run in parallel (via goroutines); each retries independently — exponential backoff initial `2s`, max interval `30s`, max elapsed `5m`
* \[ ] FR-9: Stream and topic creation is idempotent — safe to call multiple times
* \[ ] FR-10: All infra clients wrapped with circuit breaker: open after 3 failures, reset after 30s; return `ErrCircuitOpen` when open
* \[ ] FR-11: Config loaded via `config.Load()` using Viper — YAML file + env vars (env vars override); secrets fetched from Infisical if `INFISICAL_TOKEN` is set, otherwise falls back to env vars
* \[ ] FR-12: `GET /health/deep` calls `orchestrator.RunDeepHealth(ctx)` — concurrent TCP/HTTP probes per dep; returns `ProbeResult{Name, OK, LatencyMs, Error}` per dep; `503` if any fail
* \[ ] FR-13: `GET /ready` — `503` while bootstrap is in progress; `200` once `RunBootstrap()` returns without error
* \[ ] FR-14: `POST /api/v1/bootstrap` returns `409` if bootstrap is already in progress (atomic state guard)
* \[ ] FR-15: (P3) Feature flag check via `open-feature/go-sdk` (Unleash provider) before each phase; if flag `cortex.bootstrap.<phase>.enabled` is `false`, skip phase and mark as `skipped` in result

### Non-Functional

* \[ ] NFR-1: OTEL traces + metrics → `arc-widow:4317` gRPC; service name `arc-cortex`; namespace `arc`; 100% sampling
* \[ ] NFR-2: Container runs as non-root user (`USER cortex` in Dockerfile)
* \[ ] NFR-3: No secrets in logs, stack traces, or HTTP error bodies; Infisical token never logged
* \[ ] NFR-4: `GET /health` < 50ms; `GET /health/deep` < 5s (concurrent probes, 5s timeout each)
* \[ ] NFR-5: Graceful shutdown ≤ 30s; active bootstrap phase given 30s grace before context cancel
* \[ ] NFR-6: `golangci-lint run` passes with zero errors
* \[ ] NFR-7: `go test ./...` ≥ 75% coverage on `internal/orchestrator` and `internal/clients`; ≥ 60% on `internal/api`

### Key Entities

| Entity | Path | Description |
|--------|------|-------------|
| `Orchestrator` | `internal/orchestrator/service.go` | `RunBootstrap(ctx) BootstrapResult`, `RunDeepHealth(ctx) map[string]ProbeResult` |
| `BootstrapResult` | `internal/orchestrator/types.go` | `{Status, Phases map[string]PhaseResult}` returned to HTTP handler and CLI |
| `PhaseResult` | `internal/orchestrator/types.go` | `{Name, Status, Error}` — one per phase |
| `ProbeResult` | `internal/orchestrator/types.go` | `{Name, OK, LatencyMs, Error}` — one per dep in deep health |
| `Config` | `internal/config/config.go` | Root Viper/Infisical config; `Load(path string) (*Config, error)` |
| `ServerConfig` | `internal/config/config.go` | `SERVER_PORT` (8081), timeouts |
| `TelemetryConfig` | `internal/config/config.go` | `TELEMETRY_OTLP_ENDPOINT`, service name, log level |
| `NATSConfig` / `StreamConfig` | `internal/config/config.go` | NATS URL + stream definitions |
| `PulsarConfig` / `TopicConfig` | `internal/config/config.go` | Pulsar URLs, tenant, namespaces, topics |
| `PostgresConfig` | `internal/config/config.go` | pgx pool — host, port, user, db, ssl, pool size |
| `RedisConfig` | `internal/config/config.go` | go-redis — host, port, db, password (from Infisical) |
| `Metrics` | `internal/telemetry/friday.go` | OTEL metric instruments: phase duration histograms, error counters, HTTP request metrics |

### Key Environment Variables

| Env Var | Default | Description |
|---------|---------|-------------|
| `SERVER_PORT` | `8081` | HTTP listen port |
| `SERVER_SHUTDOWN_TIMEOUT` | `30s` | Graceful shutdown budget |
| `TELEMETRY_OTLP_ENDPOINT` | `arc-widow:4317` | OTEL collector gRPC address |
| `TELEMETRY_OTLP_INSECURE` | `true` | Skip TLS for OTLP (dev) |
| `TELEMETRY_SERVICE_NAME` | `arc-cortex` | OTEL service name |
| `TELEMETRY_LOG_LEVEL` | `info` | `debug\|info\|warn\|error` |
| `BOOTSTRAP_TIMEOUT` | `5m` | Max elapsed time per phase |
| `BOOTSTRAP_RETRY_BACKOFF` | `2s` | Initial backoff interval |
| `NATS_URL` | `nats://arc-messaging:4222` | NATS connection URL |
| `PULSAR_ADMIN_URL` | `http://arc-streaming:8080` | Pulsar admin REST API |
| `PULSAR_SERVICE_URL` | `pulsar://arc-streaming:6650` | Pulsar binary protocol |
| `PULSAR_TENANT` | `arc-system` | Pulsar tenant to provision |
| `POSTGRES_HOST` | `arc-sql-db` | Postgres host |
| `POSTGRES_PORT` | `5432` | Postgres port |
| `POSTGRES_USER` | `arc` | Postgres user |
| `POSTGRES_DB` | `arc_db` | Database name |
| `POSTGRES_SSL_MODE` | `disable` | `disable\|require\|verify-full` |
| `REDIS_HOST` | `arc-cache` | Redis host |
| `REDIS_PORT` | `6379` | Redis port |
| `INFISICAL_TOKEN` | — | If set, secrets fetched from Nick Fury (Infisical) |
| `UNLEASH_URL` | — | If set, feature flags fetched from Mystique (Unleash) |

## Edge Cases

| Scenario | Expected Behavior |
|----------|-------------------|
| Dep unreachable at startup | Phase retries for up to 5 min; `/health` stays 200; `/ready` stays 503 |
| `arc-friday-collector` unreachable | OTEL SDK buffers silently; no impact on bootstrap or HTTP |
| `POST /api/v1/bootstrap` while already running | `409 {"status":"in-progress"}` |
| NATS stream already exists | `CreateStream` idempotent — updates config if changed, no error |
| Pulsar topic already exists | Topic creation skipped; no error |
| Infisical unreachable at start | Fall back to env var secrets; log warning; proceed normally |
| Unleash unreachable | Feature flags default to `true` (all phases enabled); log warning |
| `cortex bootstrap` exits non-zero | Exit code 1 + JSON error to stdout (`{"status":"error","phase":"...","error":"..."}`) |
| Panic in HTTP handler | `Recovery` middleware catches; logs stack; `500`; service continues |
| `SIGTERM` during active phase | Phase gets 30s grace; context cancelled after; server shuts down |
| Postgres password missing (no Infisical) | `config.Load()` validation fails; service exits with descriptive error |
| Circuit breaker open | Client calls return `ErrCircuitOpen` immediately; retries resume when circuit resets (30s) |

## Success Criteria

* \[ ] SC-1: `docker compose up cortex` — `/health` returns 200 within 3s of start
* \[ ] SC-2: `POST /api/v1/bootstrap` with all four infra services running → 200; `nats stream ls` shows 3 streams; Pulsar shows `arc-system` tenant
* \[ ] SC-3: `cortex bootstrap` CLI exits 0 and provisions identical infra as the HTTP trigger
* \[ ] SC-4: `make otel-up && docker compose up cortex` — traces visible in arc-friday under service `arc-cortex`; phase duration metrics exported
* \[ ] SC-5: `go test ./...` passes ≥ 75% coverage on orchestrator and client packages
* \[ ] SC-6: `golangci-lint run` — zero errors
* \[ ] SC-7: `docker inspect arc-cortex` — non-root user confirmed

## Docs & Links Update

* \[ ] Update `.specify/config.yaml` — change `dir: "bootstrap", codename: "raymond"` → `dir: "cortex", codename: "cortex"`
* \[ ] Update `services/profiles.yaml` — include `cortex` in `think` and `reason` profiles
* \[ ] Add `services/cortex/service.yaml` — `codename: cortex`, `depends_on: [flash, strange, oracle, sonic, widow]`, `health: http://localhost:8081/health`
* \[ ] Add `services/cortex/README.md` — quickstart, env var table, example `docker compose` snippet
* \[ ] Update `.specify/meta/service-codename-map.md` — add Cortex entry
* \[ ] Update `.specify/docs/architecture/cortex.md` — link to `services/cortex/` implementation once built

## Constitution Compliance

| Principle | Applies | Compliant | Notes |
|-----------|---------|-----------|-------|
| I. Zero-Dep CLI | \[ ] | \[n/a] | Service, not CLI binary |
| II. Platform-in-a-Box | \[x] | \[x] | `docker compose up` provisions all infra; no manual wiring |
| III. Modular Services | \[x] | \[x] | Self-contained in `services/cortex/`; `service.yaml` declares all deps |
| IV. Two-Brain | \[x] | \[x] | Pure Go — infrastructure domain only |
| V. Polyglot Standards | \[x] | \[x] | golangci-lint, slog, table-driven tests, 12-factor config |
| VI. Local-First | \[ ] | \[n/a] | Service, not CLI |
| VII. Observability | \[x] | \[x] | OTEL traces + metrics from day one; `/health` + `/health/deep`; span per phase |
| VIII. Security | \[x] | \[x] | Non-root container; secrets via Infisical, not env; nothing sensitive in logs |
| IX. Declarative | \[ ] | \[n/a] | Service, not reconciler |
| X. Stateful Ops | \[ ] | \[n/a] | Service, not CLI |
| XI. Resilience | \[x] | \[x] | Circuit breakers + exponential backoff on all four infra clients |
| XII. Interactive | \[ ] | \[n/a] | Service, not CLI TUI |

---

---
url: /arc-platform/specs-site/005-data-layer/spec.md
---
# Feature: Data Layer Services Setup

> **Spec**: 005-data-layer
> **Author**: arc-framework
> **Date**: 2026-02-28
> **Status**: Draft

## Target Modules

| Module | Path | Impact |
|--------|------|--------|
| Services | `services/persistence/` | New — Postgres 17 (Oracle), port 5432 |
| Services | `services/vector/` | New — Qdrant (Cerebro), ports 6333/6334 |
| Services | `services/storage/` | New — MinIO (Tardis), ports 9000/9001 |
| Services | `services/profiles.yaml` | Update — oracle + cerebro → `think`; tardis → `reason` |
| CI/CD | `.github/workflows/` | New — data-images.yml + data-release.yml |
| Makefile | `Makefile`, `*.mk` | New — oracle.mk, cerebro.mk, tardis.mk, data.mk |

## Overview

Provision Postgres 17 (Oracle), Qdrant (Cerebro), and MinIO (Tardis) as production-grade platform services in the arc-platform monorepo. These three services form the persistent data backbone — relational state, vector embeddings, and object storage — that Cortex bootstraps against, AI agents use for memory and retrieval, and future services depend on. Each follows the canonical pattern from 003-messaging-setup: thin Dockerfile label wrapper, `service.yaml`, `docker-compose.yml`, dedicated `.mk` include, and CI/release workflows.

## Architecture

```mermaid
graph TD
    subgraph arc_platform_net
        oracle["arc-sql-db\n(Postgres 17)\n:5432"]
        cerebro["arc-vector-db\n(Qdrant)\n:6333 REST · :6334 gRPC"]
        tardis["arc-storage\n(MinIO)\n:9000 S3 · :9001 console"]
    end

    cortex["arc-cortex\n(:8081)"] -->|schema bootstrap\nSQL queries| oracle
    mystique_future["arc-flags (future)\nUnleash flags"] -->|feature flag state| oracle
    sherlock["arc-sherlock (future)\nReasoner"] -->|vector search\nembedding store| cerebro
    scarlett["arc-scarlett (future)\nVoice Agent"] -->|binary assets\nrecordings| tardis
    agents["AI Agents"] -->|semantic memory\nRAG retrieval| cerebro
    agents -->|artifact storage\noutput files| tardis
```

### Service Roles

| Codename | Service | Role | When to Use |
|----------|---------|------|-------------|
| Oracle | Postgres 17 | Relational state, schemas, FK constraints | Structured data, audit logs, Unleash state |
| Cerebro | Qdrant | Vector embeddings, semantic search | RAG pipelines, agent memory, similarity queries |
| Tardis | MinIO (S3) | Object storage, binary assets | Files, recordings, model weights, large blobs |

### Directory Layout

```
services/
├── persistence/        ← Oracle (Postgres 17)
│   ├── Dockerfile
│   ├── service.yaml
│   ├── docker-compose.yml
│   └── oracle.mk
├── vector/             ← Cerebro (Qdrant)
│   ├── Dockerfile
│   ├── service.yaml
│   ├── docker-compose.yml
│   └── cerebro.mk
└── storage/            ← Tardis (MinIO)
    ├── Dockerfile
    ├── service.yaml
    ├── docker-compose.yml
    └── tardis.mk
```

## User Scenarios & Testing

### P1 — Must Have

**US-1**: As a platform developer, I want to start all three data services with a single make command so that my local dev environment has persistence ready immediately.

* **Given**: Docker is running and `arc_platform_net` exists
* **When**: `make data-up` is executed
* **Then**: Oracle, Cerebro, and Tardis start; all Docker health checks pass
* **Test**: `make data-health` exits 0

**US-2**: As a platform developer, I want Cortex to report `oracle: ok` in `/health/deep` so that I know the data layer is wired correctly.

* **Given**: `make data-up` has succeeded and `arc-cortex` is running
* **When**: `curl http://localhost:8081/health/deep` is called
* **Then**: Response shows `"oracle": {"status": "ok"}` (not "connection refused")
* **Test**: `curl -s http://localhost:8081/health/deep | jq .oracle.status`

**US-3**: As a CI consumer, I want Docker images for all three services built and pushed on main branch merges so the team always has fresh images.

* **Given**: A commit merges to main touching `services/persistence/**`, `services/vector/**`, or `services/storage/**`
* **When**: `data-images.yml` workflow runs
* **Then**: Images `ghcr.io/arc-framework/arc-sql-db`, `arc-vector-db`, `arc-storage` are updated
* **Test**: Inspect GHCR after CI completes; verify `sha-*` tag present

### P2 — Should Have

**US-4**: As a platform developer, I want each data service to have `service.yaml` so the arc CLI and Cortex can discover them declaratively.

* **Test**: `cat services/persistence/service.yaml` contains name, ports, health, depends\_on

**US-5**: As a release engineer, I want to publish versioned images for all three services via a single `data/vX.Y.Z` git tag so that data layer releases are atomic.

* **Test**: Push tag `data/v0.1.0`; `data-release.yml` builds + pushes all three images with `data-v0.1.0` tag

**US-6**: As a platform developer, I want `make data-up` / `make data-down` / `make data-health` to aggregate all three services, mirroring the `messaging-*` UX.

* **Test**: Both commands work end-to-end; subsequent `make data-up` is idempotent

### P3 — Nice to Have

**US-7**: As a developer, I want `make data-logs` to tail logs from all three containers simultaneously so I can debug data layer issues without multiple terminals.

* **Test**: `make data-logs` fans out to Oracle, Cerebro, and Tardis log streams with service prefixes

**US-8**: As a developer, I want the MinIO console accessible at `http://localhost:9001` so I can browse buckets and inspect objects during development.

* **Test**: Navigate to `http://localhost:9001`; log in with `arc` / `arc-minio-dev`

## Requirements

### Functional

* \[ ] FR-1: Create `services/persistence/` with a Postgres 17 Dockerfile using `postgres:17-alpine`, `service.yaml`, `docker-compose.yml`, and `oracle.mk`
* \[ ] FR-2: Create `services/vector/` with a Qdrant Dockerfile using `qdrant/qdrant`, `service.yaml`, `docker-compose.yml`, and `cerebro.mk`
* \[ ] FR-3: Create `services/storage/` with a MinIO Dockerfile using `minio/minio`, `service.yaml`, `docker-compose.yml`, and `tardis.mk`
* \[ ] FR-4: Oracle must initialize with default DB `arc`, user `arc`, password `arc`; data persisted to named volume `arc-sql-db-data`
* \[ ] FR-5: Cerebro (Qdrant) must persist vector data to named volume `arc-vector-db-data`; both REST (:6333) and gRPC (:6334) exposed on localhost
* \[ ] FR-6: Tardis (MinIO) must expose S3 API (:9000) and console (:9001); data persisted to named volume `arc-storage-data`; root credentials via `MINIO_ROOT_USER` / `MINIO_ROOT_PASSWORD` env
* \[ ] FR-7: Update `services/profiles.yaml` — add `oracle` + `cerebro` to `think` profile; `tardis` to `reason` profile
* \[ ] FR-8: Create `data-images.yml` CI workflow — path-filtered per service directory, builds all three, `linux/amd64` in CI
* \[ ] FR-9: Create `data-release.yml` release workflow — tag format `data/vX.Y.Z`, builds all three multi-platform (`linux/amd64,linux/arm64`), creates GitHub release
* \[ ] FR-10: Create `services/data.mk` aggregate with `data-up`, `data-down`, `data-health`, `data-logs` targets
* \[ ] FR-11: Include `oracle.mk`, `cerebro.mk`, `tardis.mk`, `data.mk` in root Makefile

### Non-Functional

* \[ ] NFR-1: Oracle runs as `postgres` user (uid 70 in alpine); Cerebro runs as non-root (qdrant uid 1000); MinIO runs as non-root (verify against upstream `minio/minio` image)
* \[ ] NFR-2: All services expose health endpoints consumable by Docker health checks with appropriate `start_period` (Oracle: 10s, Cerebro: 5s, MinIO: 5s)
* \[ ] NFR-3: Data volumes must use named Docker volumes — no bind mounts
* \[ ] NFR-4: All ports bind to `127.0.0.1` only — no `0.0.0.0` exposure
* \[ ] NFR-5: Docker images include standard OCI (`org.opencontainers.*`) and `arc.service.*` labels
* \[ ] NFR-6: CI build completes in under 3 minutes (amd64 only; no QEMU)

### Key Entities

| Entity | Module | Description |
|--------|--------|-------------|
| `arc-sql-db` | `services/persistence/` | Postgres 17; relational state store for Cortex, Unleash, and future services |
| `arc-vector-db` | `services/vector/` | Qdrant; vector DB for agent embeddings and semantic search |
| `arc-storage` | `services/storage/` | MinIO; S3-compatible object storage for binary assets and agent outputs |
| `oracle.mk` | `services/persistence/` | Make targets: oracle-up/down/health/logs/build/push/publish/tag/clean/nuke |
| `cerebro.mk` | `services/vector/` | Make targets: cerebro-up/down/health/logs/build/push/publish/tag/clean/nuke |
| `tardis.mk` | `services/storage/` | Make targets: tardis-up/down/health/logs/build/push/publish/tag/clean/nuke |

## Port Reference

| Service | Container Port | Host Binding | Purpose |
|---------|---------------|--------------|---------|
| Oracle | 5432 | `127.0.0.1:5432` | Postgres wire protocol |
| Cerebro | 6333 | `127.0.0.1:6333` | Qdrant REST API + Prometheus metrics |
| Cerebro | 6334 | `127.0.0.1:6334` | Qdrant gRPC API |
| Tardis | 9000 | `127.0.0.1:9000` | MinIO S3 API |
| Tardis | 9001 | `127.0.0.1:9001` | MinIO web console |

## Network Strategy

All three services join `arc_platform_net` only — the shared bridge already used by Flash, Strange, Sonic, and Cortex. No new networks are created by this feature.

```mermaid
graph LR
    subgraph arc_platform_net [arc_platform_net — external, shared]
        oracle[arc-sql-db]
        cerebro[arc-vector-db]
        tardis[arc-storage]
        cortex[arc-cortex]
        flash[arc-messaging]
        sonic[arc-cache]
        strange[arc-streaming]
    end
```

**Rules:**

* `arc_platform_net` declared `external: true` in each compose file — created by `make dev` or `make flash-up`
* Data services join `arc_platform_net` only (no `arc_otel_net`)
* Container hostnames (`arc-sql-db`, `arc-vector-db`, `arc-storage`) are DNS-resolvable by all services on the network

## Edge Cases

| Scenario | Expected Behavior |
|----------|-------------------|
| Oracle first run — empty data directory | Postgres auto-initializes using `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB` env vars; no manual init needed |
| Cortex connects to Oracle before ready | Cortex retries with exponential backoff; `/health/deep` reflects `oracle: unreachable` until Oracle health check passes |
| MinIO starts with no buckets | MinIO starts empty; buckets must be created by Cortex bootstrap or operator via console / `mc` client |
| Cerebro disk full | Qdrant returns 507 Insufficient Storage; operator must resize the named volume |
| `make data-up` before `arc_platform_net` exists | Docker compose fails at network lookup; run `make dev` or `docker network create arc_platform_net` first |
| Oracle password changed post-init | Postgres ignores `POSTGRES_PASSWORD` after data dir is initialized; operator must `make oracle-nuke` to reset |
| Tardis credentials mismatch (env vs stored config) | MinIO refuses to start; `make tardis-nuke` resets the volume and stored config |

## Success Criteria

* \[ ] SC-1: `make data-up && make data-health` exits 0 with all three services healthy
* \[ ] SC-2: `curl -s http://localhost:8081/health/deep | jq .oracle.status` returns `"ok"` after `make cortex-docker-up`
* \[ ] SC-3: `curl -s http://localhost:6333/readyz` returns HTTP 200 (Qdrant ready)
* \[ ] SC-4: MinIO console accessible at `http://localhost:9001` with `arc` / `arc-minio-dev` credentials
* \[ ] SC-5: `data-images.yml` CI completes in under 3 minutes for an amd64-only change
* \[ ] SC-6: `git tag data/v0.1.0 && git push --tags` triggers `data-release.yml`; all three multi-platform images published to GHCR
* \[ ] SC-7: All Dockerfiles pass `trivy image` scan with zero CRITICAL CVEs
* \[ ] SC-8: `services/profiles.yaml` `think` profile includes `oracle` and `cerebro`; `reason` profile includes `tardis`

## Docs & Links Update

* \[ ] Update `services/profiles.yaml` — add oracle + cerebro to `think`; tardis to `reason`
* \[ ] Verify `services/cortex/service.yaml` `depends_on` includes `oracle` codename
* \[ ] Update `CLAUDE.md` monorepo layout section to reference `persistence/`, `vector/`, `storage/` directories

## Constitution Compliance

| Principle | Applies | Compliant | Notes |
|-----------|---------|-----------|-------|
| I. Zero-Dep CLI | \[ ] | n/a | Services only — no CLI changes |
| II. Platform-in-a-Box | \[x] | \[x] | `make data-up` boots all three; oracle + cerebro join `think` profile |
| III. Modular Services | \[x] | \[x] | Each service self-contained in own directory; flat under `services/` |
| IV. Two-Brain | \[x] | \[x] | Config-only upstream images — no language concern |
| V. Polyglot Standards | \[x] | \[x] | Dockerfiles, compose, health checks follow established 003 pattern |
| VI. Local-First | \[ ] | n/a | CLI-only principle |
| VII. Observability | \[x] | \[x] | Qdrant exposes Prometheus on :6333; MinIO health on :9000; Oracle via pg\_isready |
| VIII. Security | \[x] | \[x] | Non-root containers; 127.0.0.1-only port binding; credentials via env, not git |
| IX. Declarative | \[ ] | n/a | CLI-only principle |
| X. Stateful Ops | \[ ] | n/a | CLI-only principle |
| XI. Resilience | \[x] | \[x] | Health checks with start\_periods; named volumes survive container restart |
| XII. Interactive | \[ ] | n/a | CLI-only principle |

---

---
url: /arc-platform/specs-site/004-dev-setup/spec.md
---
# Feature: Dev Setup Orchestration

> **Spec**: 004-dev-setup
> **Author**: arc-framework
> **Date**: 2026-02-28
> **Status**: Draft

## Target Modules

| Module | Path | Impact |
|--------|------|--------|
| Services | `Makefile` | Update — add `.make/` generation rules, `dev-*` orchestration targets |
| Services | `services/profiles.yaml` | Update — add `friday-collector` to `think` profile |
| Services | `services/otel/otel.mk` | Update — add `friday-collector-up/down/health` alias targets |
| Services | `services/cortex/cortex.mk` | Update — add `cortex-up/down` alias targets |
| Services | `services/cortex/service.yaml` | Update — remove `oracle` from `depends_on` (unregistered) |
| Services | `services/otel/service.yaml` | New — metadata for full OTEL stack |
| Services | `services/otel/telemetry/service.yaml` | New — metadata for collector-only |
| Scripts | `scripts/lib/` | New — 5 POSIX shell scripts for parsing, dep resolution, health, prereqs |
| Config | `.gitignore` | Update — add `.make/` |

## Overview

Build a Makefile-based orchestration framework so that `make dev` starts the entire platform in proper dependency order. Adding a new service requires only a `service.yaml`, `.mk` file, `Dockerfile`, and `docker-compose.yml` — the framework reads `profiles.yaml` and `service.yaml` files, resolves startup order via topological sort, gates each layer on health checks, and fails fast on missing or circular dependencies.

## Architecture

```mermaid
graph TD
    subgraph "Developer Interface"
        dev["make dev\nmake dev PROFILE=reason"]
    end

    subgraph "Generated Layer (.make/ — gitignored)"
        profiles_mk[".make/profiles.mk\nPROFILE_THINK_SERVICES := ..."]
        registry_mk[".make/registry.mk\nSERVICE_flash_HEALTH := ..."]
    end

    subgraph "Source of Truth"
        profiles_yaml["services/profiles.yaml"]
        service_yamls["services/*/service.yaml\nservices/*/*/service.yaml"]
    end

    subgraph "Orchestration Scripts (scripts/lib/)"
        parse_profiles["parse-profiles.sh"]
        parse_registry["parse-registry.sh"]
        resolve_deps["resolve-deps.sh\n(topological sort — DFS)"]
        wait_health["wait-for-health.sh"]
        check_prereqs["check-dev-prereqs.sh"]
    end

    subgraph "Service Targets (existing .mk files)"
        flash_up["flash-up / flash-down / flash-health"]
        sonic_up["sonic-up / sonic-down / sonic-health"]
        strange_up["strange-up / strange-down / strange-health"]
        collector_up["friday-collector-up (alias → otel-up-telemetry)"]
        cortex_up["cortex-up (alias → cortex-docker-up)"]
    end

    profiles_yaml -->|parsed by| parse_profiles --> profiles_mk
    service_yamls -->|parsed by| parse_registry --> registry_mk

    dev -->|1. prereqs| check_prereqs
    dev -->|2. include generated| profiles_mk & registry_mk
    dev -->|3. resolve order| resolve_deps
    dev -->|4. start each layer| flash_up & sonic_up & strange_up & collector_up & cortex_up
    dev -->|5. gate on health| wait_health
```

## Startup Sequence (think profile)

```mermaid
sequenceDiagram
    participant Dev as make dev
    participant Pre as check-dev-prereqs.sh
    participant Res as resolve-deps.sh
    participant Net as Docker Networks
    participant F as flash (NATS)
    participant So as sonic (Redis)
    participant St as strange (Pulsar)
    participant FC as friday-collector
    participant C as cortex

    Dev->>Pre: Check Docker, Compose v2, ports (4222, 6379, 6650, 13133, 8081)
    Pre-->>Dev: ✓ All prerequisites met

    Dev->>Res: resolve flash sonic strange friday-collector cortex
    Res-->>Dev: Layer 0: flash, sonic, strange, friday-collector → Layer 1: cortex

    Dev->>Net: docker network create arc_platform_net (idempotent)
    Dev->>Net: docker network create arc_otel_net (idempotent)

    Note over F,So: Layer 0 — no inter-dependencies, start in parallel
    Dev->>F: flash-up
    Dev->>So: sonic-up
    Dev->>St: strange-up
    Dev->>FC: friday-collector-up

    Dev->>F: wait-for-health http://localhost:8222/healthz 60s
    Dev->>So: wait-for-health redis-cli ping 30s
    Dev->>St: wait-for-health http://localhost:8082/admin/v2/brokers/health 120s
    Dev->>FC: wait-for-health http://localhost:13133/ 30s

    Note over C: Layer 1 — all Layer 0 services healthy
    Dev->>C: cortex-up (→ cortex-docker-up)
    Dev->>C: wait-for-health http://localhost:8081/health 60s
    C-->>Dev: ✓ healthy

    Dev-->>Dev: ✓ Profile 'think' is ready
```

## Service Type Contract

Every service must provide:

| Artifact | Purpose | Required |
|----------|---------|---------|
| `service.yaml` | Codename, health endpoint, `depends_on` | Yes |
| `<codename>.mk` | Targets: `<name>-up`, `<name>-down`, `<name>-health` | Yes |
| `Dockerfile` | Container image definition | Yes |
| `docker-compose.yml` | Container orchestration | Yes |
| Go extras | `<name>-test`, `<name>-lint`, `<name>-bin`, `<name>-run` | Go services only |
| Python extras | `<name>-test`, `<name>-lint`, `<name>-venv`, `<name>-run` | Python services only |

## User Scenarios & Testing

### P1 — Must Have

**US-1**: As a platform developer, I want to run `make dev` and have the entire `think` profile start in dependency order so that my local environment is ready without memorizing startup commands.

* **Given**: Docker is running, no arc-\* containers are active
* **When**: `make dev` is executed from repo root
* **Then**: flash, sonic, strange, friday-collector start in parallel (Layer 0); cortex starts after all Layer 0 services are healthy
* **Test**: `make dev && make dev-health` exits 0

**US-2**: As a platform developer, I want to run `make dev PROFILE=reason` to start the full observability stack so that I can debug with SigNoz dashboards.

* **Given**: Docker is running
* **When**: `make dev PROFILE=reason` is executed
* **Then**: All `think` services plus the full OTEL stack (SigNoz + ClickHouse + ZooKeeper + collector) start; SigNoz available at http://localhost:3301
* **Test**: `make dev PROFILE=reason && make dev-health` exits 0; `curl -sf http://localhost:3301/api/v1/health` exits 0

**US-3**: As a platform developer, I want `make dev` to fail immediately with a clear message if a dependency is unregistered so that I know exactly what to add.

* **Given**: A `service.yaml` lists a `depends_on` entry that has no corresponding `service.yaml`
* **When**: `make dev` is executed
* **Then**: Prints `✗ Service 'cortex' depends on 'oracle' which is not registered.` and exits 1
* **Test**: Add `fake-service` to cortex's `depends_on`, run `make dev`, verify exit 1 with correct message; then revert

**US-4**: As a platform developer, I want `make dev-down` to stop all services in my active profile so that I can free resources cleanly.

* **Given**: Services are running from `make dev`
* **When**: `make dev-down` is executed
* **Then**: All profile services stop; no arc-\* containers remain
* **Test**: `make dev && make dev-down && docker ps --filter name=arc- --format '{{.Names}}'` returns empty

**US-5**: As a platform developer, I want `make dev-prereqs` to verify Docker and port availability before starting so that I get a clear error instead of a cryptic failure mid-startup.

* **Given**: Developer machine with Docker stopped
* **When**: `make dev-prereqs` is executed
* **Then**: Prints colored checklist with ✓/✗ per check; exits 1 if any check fails
* **Test**: Stop Docker, run `make dev-prereqs`, verify `✗ Docker daemon not running` in output and exit 1

### P2 — Should Have

**US-6**: As a platform developer, I want `make dev-health` to report per-service health so I can diagnose issues at a glance.

* **Test**: `make dev-health` prints `✓ flash`, `✓ sonic`, etc. for all running profile services; exits 1 if any fail

**US-7**: As a platform developer, I want `make dev-logs` to tail logs from all profile services simultaneously so I can debug cross-service issues in one terminal.

* **Test**: `make dev-logs` shows interleaved, service-prefixed log output from all running containers

**US-8**: As a platform developer, I want `.make/` generated files to auto-regenerate when `profiles.yaml` or any `service.yaml` changes so I never use stale metadata.

* **Test**: `touch services/profiles.yaml && make dev` — verify `.make/profiles.mk` timestamp is newer than before

### P3 — Nice to Have

**US-9**: As a platform developer, I want `make dev-status` to show a formatted table of running services and their health status.

* **Test**: `make dev-status` shows a table with service name, container status, and ✓/✗ health symbol

## Requirements

### Functional

* \[ ] FR-1: Create `scripts/lib/parse-profiles.sh` — converts `services/profiles.yaml` to Make variable assignments using `awk` (no external deps); expands `services: '*'` to `ALL_SERVICES`
* \[ ] FR-2: Create `scripts/lib/parse-registry.sh` — scans `services/*/service.yaml` and `services/*/*/service.yaml`; emits `SERVICE_<codename>_HEALTH`, `SERVICE_<codename>_DEPENDS`, `SERVICE_<codename>_DIR` Make variables
* \[ ] FR-3: Create `scripts/lib/resolve-deps.sh` — DFS topological sort on the service dependency graph; exits 1 with actionable message on unregistered or circular dependency
* \[ ] FR-4: Create `scripts/lib/wait-for-health.sh` — polls a health endpoint with configurable timeout (default 120s); supports HTTP (`curl -sf`) and command (`docker exec`) endpoint types
* \[ ] FR-5: Create `scripts/lib/check-dev-prereqs.sh` — validates: Docker daemon running, `docker compose` v2 available, required ports free; prints ✓/✗ checklist; exits 1 on any failure
* \[ ] FR-6: Create `services/otel/service.yaml` — full OTEL stack metadata: `codename: otel`, `health: http://localhost:3301/api/v1/health`, `depends_on: []`
* \[ ] FR-7: Create `services/otel/telemetry/service.yaml` — collector-only metadata: `codename: friday-collector`, `health: http://localhost:13133/`, `depends_on: []`
* \[ ] FR-8: Update `Makefile` — add `-include .make/profiles.mk`, `-include .make/registry.mk`, generation rules with `profiles.yaml`/`service.yaml` as prerequisites, and targets: `dev`, `dev-up`, `dev-down`, `dev-health`, `dev-logs`, `dev-status`, `dev-clean`, `dev-prereqs`
* \[ ] FR-9: Update `services/profiles.yaml` — add `friday-collector` to the `think` profile service list
* \[ ] FR-10: Update `services/otel/otel.mk` — add `friday-collector-up` (→ `otel-up-telemetry`), `friday-collector-down`, `friday-collector-health` alias targets to `.PHONY`
* \[ ] FR-11: Update `services/cortex/cortex.mk` — add `cortex-up` (→ `cortex-docker-up`), `cortex-down` (→ `cortex-docker-down`) alias targets to `.PHONY`
* \[ ] FR-12: Update `services/cortex/service.yaml` — remove `oracle` from `depends_on`; add inline comment `# TODO: re-add oracle when persistence service lands`
* \[ ] FR-13: Update `.gitignore` — add `.make/` entry
* \[ ] FR-14: `make dev` must create `arc_platform_net` and `arc_otel_net` Docker networks idempotently before starting any service containers
* \[ ] FR-15: All 5 shell scripts must source `scripts/lib/common.sh` and use its `log_info`, `log_success`, `log_error`, `die` helpers; no bashisms beyond what `common.sh` already uses

### Non-Functional

* \[ ] NFR-1: No new external tool dependencies — scripts use only `awk`, `curl`, `docker` (already required by existing Makefiles)
* \[ ] NFR-2: `.make/` files must auto-regenerate when source YAML files change (Make prerequisite rules, not timestamp-based polling)
* \[ ] NFR-3: `make dev` for the `think` profile must complete in under 3 minutes on a warm Docker cache (Pulsar cold-start ~90s is the expected bottleneck)
* \[ ] NFR-4: All scripts must match existing `scripts/lib/` conventions: ANSI colors via `common.sh` vars, `→` for steps, `✓` for success, `✗` for errors
* \[ ] NFR-5: Existing per-service targets (`flash-up`, `otel-up`, `cortex-docker-up`, etc.) must continue to work unchanged — aliases are additive only

### Key Entities

| Entity | Path | Description |
|--------|------|-------------|
| `.make/profiles.mk` | Generated | `PROFILE_THINK_SERVICES`, `ALL_PROFILES`, `PROFILE_<X>_SERVICES` Make variables |
| `.make/registry.mk` | Generated | Per-service `SERVICE_<codename>_HEALTH`, `_DEPENDS`, `_DIR` Make variables |
| `service.yaml` | `services/*/` | Service metadata: `codename`, `health`, `depends_on` list |
| `profiles.yaml` | `services/` | Profile → service list mapping; source of truth for composition |
| `parse-profiles.sh` | `scripts/lib/` | YAML → Make converter for profiles |
| `parse-registry.sh` | `scripts/lib/` | YAML → Make converter for service registry |
| `resolve-deps.sh` | `scripts/lib/` | Topological sort for startup layer ordering |
| `wait-for-health.sh` | `scripts/lib/` | Universal health-check poller with timeout |
| `check-dev-prereqs.sh` | `scripts/lib/` | Environment validation before startup |

## Edge Cases

| Scenario | Expected Behavior |
|----------|-------------------|
| Dependency not registered (e.g., `oracle` in `cortex/service.yaml`) | `resolve-deps.sh` exits 1: `✗ Service 'cortex' depends on 'oracle' which is not registered.` |
| Circular dependency (A → B → A) | `resolve-deps.sh` exits 1: `✗ Circular dependency detected: A → B → A` |
| Service health check times out | `wait-for-health.sh` exits 1: `✗ flash did not become healthy after 60s` |
| Docker not running | `check-dev-prereqs.sh` exits 1: `✗ Docker daemon not running` |
| Port already in use (e.g., 4222) | `check-dev-prereqs.sh` exits 1: `✗ Port 4222 already in use (required by flash)` |
| `ultra-instinct` profile with `services: '*'` | `parse-profiles.sh` expands `*` to the full `ALL_SERVICES` list from `registry.mk` |
| No `service.yaml` files found | `parse-registry.sh` emits `ALL_SERVICES :=`; `make dev` warns `No services registered` |
| `.make/` directory doesn't exist | Make auto-creates via `mkdir -p .make` in generation rules |
| Unknown profile name | `make dev PROFILE=foo` exits 1: `✗ Unknown profile 'foo'. Available: think, reason, ultra-instinct` |
| `make dev` run twice (services already running) | `docker compose up -d` is idempotent — no error, services remain running |
| `friday-collector-down` while full OTEL is running | Stops only `arc-friday-collector` container; SigNoz stack remains up |

## Success Criteria

* \[ ] SC-1: `make dev` starts all 5 `think`-profile services in dependency order and exits 0
* \[ ] SC-2: `make dev-health` reports all 5 services healthy after `make dev`
* \[ ] SC-3: `make dev-down` stops all profile services; `docker ps --filter name=arc-` returns empty
* \[ ] SC-4: `make dev PROFILE=reason` starts the full OTEL stack plus `think` services; SigNoz accessible at http://localhost:3301
* \[ ] SC-5: Introducing an unregistered `depends_on` entry causes `make dev` to exit 1 with an actionable error message
* \[ ] SC-6: All existing per-service targets (`flash-up`, `otel-up`, `cortex-docker-up`, etc.) still work independently
* \[ ] SC-7: `.make/profiles.mk` regenerates automatically when `services/profiles.yaml` is modified
* \[ ] SC-8: `.make/registry.mk` regenerates automatically when any `services/*/service.yaml` is modified
* \[ ] SC-9: `make dev-prereqs` detects stopped Docker daemon and exits 1 with a clear message

## Docs & Links Update

* \[ ] Update `CLAUDE.md` — replace `make up PROFILE=think` example in Commands section with `make dev` and `make dev PROFILE=reason`
* \[ ] Update `services/profiles.yaml` — profile descriptions to reflect `friday-collector` inclusion in `think`
* \[ ] Verify `services/cortex/service.yaml` `depends_on` no longer lists `oracle` after FR-12

## Constitution Compliance

| Principle | Applies | Compliant | Notes |
|-----------|---------|-----------|-------|
| I. Zero-Dep CLI | \[ ] | n/a | No CLI changes — pure Makefile/shell orchestration |
| II. Platform-in-a-Box | \[x] | \[x] | `make dev` = working platform from a single command |
| III. Modular Services | \[x] | \[x] | Framework reads per-service `service.yaml`; add service = add its directory |
| IV. Two-Brain | \[ ] | n/a | Shell scripts only — no Go/Python concerns introduced |
| V. Polyglot Standards | \[x] | \[x] | Scripts source `common.sh`; follow existing `scripts/lib/` conventions |
| VI. Local-First | \[ ] | n/a | CLI-only principle |
| VII. Observability | \[x] | \[x] | `friday-collector` in `think` profile ensures traces/metrics collected by default |
| VIII. Security | \[x] | \[x] | No secrets in scripts; port pre-checks prevent accidental exposure conflicts |
| IX. Declarative | \[x] | \[x] | `profiles.yaml` + `service.yaml` = declarative source of truth; Make reconciles |
| X. Stateful Ops | \[ ] | n/a | CLI-only principle |
| XI. Resilience | \[x] | \[x] | Health-check gating between layers; fail-fast on unregistered or circular deps |
| XII. Interactive | \[ ] | n/a | CLI-only principle |

---

---
url: /arc-platform/specs-site/003-messaging-setup/spec.md
---
# Feature: Messaging & Cache Services Setup

> **Spec**: 003-messaging-setup
> **Author**: arc-framework
> **Date**: 2026-02-23
> **Status**: Draft

## Target Modules

| Module | Path | Impact |
|--------|------|--------|
| Services | `services/messaging/` | New — NATS (Flash), port 4222 |
| Services | `services/streaming/` | New — Pulsar (Strange), port 6650 |
| Services | `services/cache/` | New — Redis (Sonic), port 6379 |
| Services | `services/otel/` | Update — collector scrapes new services |
| Services | `services/profiles.yaml` | Update — add messaging to `think` profile |
| CI/CD | `.github/workflows/` | New — messaging-images.yml + messaging-release.yml |
| Makefile | `Makefile`, `*.mk` | New — flash.mk, strange.mk, sonic.mk |

## Overview

Provision NATS (Flash), Apache Pulsar (Strange), and Redis (Sonic) as production-grade platform services in the arc-platform monorepo. These services form the messaging and caching backbone that Cortex and all future AI agents depend on. Each service follows the same pattern as the OTEL stack: thin Dockerfile, `service.yaml`, `docker-compose.yml`, dedicated Makefile include, and CI/release workflows. The OTEL collector is updated to scrape Prometheus metrics from all three services, making them visible in arc-friday (SigNoz).

## Architecture

```mermaid
graph TD
    subgraph arc_platform_net
        flash["arc-messaging\n(NATS)\n:4222 client\n:8222 monitor"]
        strange["arc-streaming\n(Pulsar)\n:6650 broker\n:8082 admin"]
        sonic["arc-cache\n(Redis)\n:6379"]
        collector["arc-friday-collector\n(OTEL Collector)\n:4317 gRPC"]
    end

    cortex["arc-cortex\n(:8081)"] -->|pub/sub\ncommands| flash
    cortex -->|durable\nevents| strange
    cortex -->|cache\nsessions| sonic

    flash -->|:8222/metrics\nPrometheus| collector
    strange -->|:8080/metrics\nPrometheus| collector
    redis_exporter["redis-exporter\nsidecar"] -->|:9121/metrics| collector
    sonic --- redis_exporter

    collector --> friday["arc-friday\n(SigNoz)"]
```

### Service Roles

| Codename | Service | Role | When to Use |
|----------|---------|------|-------------|
| Flash | NATS + JetStream | Ephemeral pub/sub, request/reply, commands | Sub-ms agent coordination |
| Strange | Apache Pulsar | Durable event streams, replay, multi-tenant | Event sourcing, audit logs |
| Sonic | Redis | Cache, session store, rate limiting | Fast k/v access, TTL-based state |

### Directory Layout

```
services/
├── messaging/        ← Flash (NATS)
│   ├── Dockerfile
│   ├── service.yaml
│   ├── docker-compose.yml
│   └── flash.mk
├── streaming/        ← Strange (Pulsar)
│   ├── Dockerfile
│   ├── service.yaml
│   ├── docker-compose.yml
│   └── strange.mk
└── cache/            ← Sonic (Redis)
    ├── Dockerfile
    ├── service.yaml
    ├── docker-compose.yml
    └── sonic.mk
```

## User Scenarios & Testing

### P1 — Must Have

**US-1**: As a platform developer, I want to run NATS, Pulsar, and Redis with a single make command so that my local dev environment is ready for agent workloads.

* **Given**: Docker is running, `make messaging-up` is executed
* **When**: All three services start
* **Then**: NATS is reachable on `:4222`, Pulsar on `:6650`, Redis on `:6379`; all health checks pass
* **Test**: `make flash-health && make strange-health && make sonic-health`

**US-2**: As a platform developer, I want arc-cortex to successfully bootstrap against live NATS, Pulsar, and Redis so that the full `think` profile works end-to-end.

* **Given**: Messaging services are running
* **When**: `make cortex-run-dev` is executed
* **Then**: Cortex starts without "connection refused" errors; `/health/deep` reports all dependencies healthy
* **Test**: `curl http://localhost:8081/health/deep`

**US-3**: As a platform operator, I want all three services visible in arc-friday (SigNoz) so that I can monitor messaging health from a single dashboard.

* **Given**: OTEL stack and messaging services are running
* **When**: Metrics are scraped by the updated collector
* **Then**: NATS, Pulsar, and Redis metrics appear in SigNoz under the respective service names
* **Test**: Navigate to arc-friday metrics explorer; filter by `service.name = arc-messaging`, `arc-streaming`, `arc-cache`

**US-4**: As a CI consumer, I want Docker images for all three services built and pushed on main branch merges so that the team always has fresh images.

* **Given**: A commit is merged to main touching `services/messaging/**`, `services/streaming/**`, or `services/cache/**`
* **When**: `messaging-images.yml` workflow runs
* **Then**: Images `ghcr.io/arc-framework/arc-messaging`, `arc-streaming`, `arc-cache` are updated with the new `sha-*` tag
* **Test**: Inspect GHCR after CI completes

### P2 — Should Have

**US-5**: As a platform developer, I want each messaging service to have its own `service.yaml` so that cortex and the arc CLI can discover and orchestrate them declaratively.

* **Test**: `cat services/messaging/service.yaml` contains name, port, health, depends\_on

**US-6**: As a release engineer, I want to publish versioned images for all three services via a single `messaging/vX.Y.Z` git tag so that releases are atomic.

* **Test**: Push tag `messaging/v0.1.0`; `messaging-release.yml` builds + pushes all three images with `messaging-v0.1.0` tag

**US-7**: As a platform developer, I want `make messaging-up` to start all three services at once, and `make messaging-down` to stop them, mirroring the `otel-up` / `otel-down` UX.

* **Test**: Both commands work; subsequent `make messaging-up` is idempotent

### P3 — Nice to Have

**US-8**: As a platform developer, I want `make messaging-logs` to tail logs from all three containers simultaneously so I can debug messaging issues without opening multiple terminals.

* **Test**: `make messaging-logs` fans out to NATS, Pulsar, and Redis log streams

**US-9**: As a developer, I want NATS JetStream streams pre-created on startup (AGENT\_COMMANDS, AGENT\_EVENTS, SYSTEM\_METRICS) so that agents can publish without manual setup.

* **Test**: After `make flash-up`, `nats stream ls` shows the three streams

## Requirements

### Functional

* \[ ] FR-1: Create `services/messaging/` with a NATS Dockerfile using `nats:alpine`, `service.yaml`, `docker-compose.yml`, and `flash.mk`
* \[ ] FR-2: Create `services/streaming/` with a Pulsar Dockerfile using `apachepulsar/pulsar:latest`, `service.yaml`, `docker-compose.yml`, and `strange.mk`
* \[ ] FR-3: Create `services/cache/` with a Redis Dockerfile using `redis:alpine`, `service.yaml`, `docker-compose.yml`, and `sonic.mk`
* \[ ] FR-4: NATS must run with JetStream enabled; three default streams created (AGENT\_COMMANDS, AGENT\_EVENTS, SYSTEM\_METRICS)
* \[ ] FR-5: Pulsar must run in standalone mode; admin API available on `:8082` (host-mapped from internal `:8080`)
* \[ ] FR-6: Redis must run with AOF persistence (`appendonly yes`, `appendfsync everysec`), `maxmemory 512mb`, `maxmemory-policy noeviction`
* \[ ] FR-7: Update `arc-friday-collector` config to scrape Prometheus metrics from NATS (`:8222/metrics`) and Pulsar (`:8080/metrics`); Redis metrics deferred — see Tech Debt
* \[ ] FR-8: Update `services/profiles.yaml` — add messaging services to `think` profile (cortex needs them at bootstrap)
* \[ ] FR-9: Create `messaging-images.yml` CI workflow — change detection per service, parallel build for all three, `linux/amd64` in CI
* \[ ] FR-10: Create `messaging-release.yml` release workflow — tag format `messaging/vX.Y.Z`, builds all three multi-platform (`linux/amd64,linux/arm64`), creates GitHub release
* \[ ] FR-11: Add convenience aggregates: `make messaging-up` / `make messaging-down` / `make messaging-health` / `make messaging-logs`
* \[ ] FR-12: Include `flash.mk`, `strange.mk`, `sonic.mk` in root Makefile

### Non-Functional

* \[ ] NFR-1: NATS and Redis containers must run as non-root (UID 1000); Pulsar may run as its default UID (upstream constraint, documented)
* \[ ] NFR-2: All services must expose health endpoints consumable by Docker health checks
* \[ ] NFR-3: Data volumes must use named Docker volumes (not bind mounts) to match the otel pattern and keep host filesystem clean
* \[ ] NFR-4: NATS monitoring port (8222) and Pulsar admin port (8082) must bind to `127.0.0.1` only (no `0.0.0.0` exposure)
* \[ ] NFR-5: Docker images must include standard OCI and `arc.service.*` labels
* \[ ] NFR-6: CI build must complete in under 3 minutes (amd64 only; no QEMU)

### Key Entities

| Entity | Module | Description |
|--------|--------|-------------|
| `arc-messaging` | `services/messaging/` | NATS 2.10 + JetStream; ephemeral pub/sub broker |
| `arc-streaming` | `services/streaming/` | Pulsar 3.3 standalone; durable event store |
| `arc-cache` | `services/cache/` | Redis 7.4; cache + session store |
| `flash.mk` | `services/messaging/` | Make targets: flash-up, flash-down, flash-health, flash-logs, flash-build |
| `strange.mk` | `services/streaming/` | Make targets: strange-up, strange-down, strange-health, strange-logs, strange-build |
| `sonic.mk` | `services/cache/` | Make targets: sonic-up, sonic-down, sonic-health, sonic-logs, sonic-build |

## Port Reference

| Service | Container Port | Host Binding | Purpose |
|---------|---------------|--------------|---------|
| NATS | 4222 | `127.0.0.1:4222` | Client connections |
| NATS | 8222 | `127.0.0.1:8222` | HTTP monitoring + Prometheus |
| NATS | 6222 | not exposed | Cluster routing (single-node) |
| Pulsar | 6650 | `127.0.0.1:6650` | Binary protocol (broker) |
| Pulsar | 8080 | `127.0.0.1:8082` | HTTP admin API (mapped to 8082) |
| Redis | 6379 | `127.0.0.1:6379` | Redis protocol |
| redis-exporter | 9121 | internal only | Prometheus metrics for Redis |

## OTEL Collector Update

The `arc-friday-collector` config (baked into `services/otel/telemetry/`) needs a new `prometheus` receiver block and updated `service.pipelines.metrics`:

```yaml
receivers:
  # ... existing receivers ...
  prometheus:
    config:
      scrape_configs:
        - job_name: arc-messaging
          static_configs:
            - targets: ['arc-messaging:8222']
        - job_name: arc-streaming
          static_configs:
            - targets: ['arc-streaming:8080']
        # arc-cache (Redis) — deferred, see Tech Debt TD-001
```

The collector must join `arc_platform_net` (see Network Strategy) to resolve `arc-messaging` and `arc-streaming` by hostname.

## Network Strategy

Two networks, not three. One shared platform network for all externally-visible services; one isolated internal network for the otel storage backend.

```mermaid
graph LR
    subgraph arc_platform_net [arc_platform_net — external, shared]
        flash[arc-messaging]
        strange[arc-streaming]
        sonic[arc-cache]
        cortex[arc-cortex]
        collector[arc-friday-collector]
    end
    subgraph arc_otel_net [arc_otel_net — internal to otel stack]
        friday[arc-friday / SigNoz]
        clickhouse[arc-friday-clickhouse]
        zookeeper[arc-friday-zookeeper]
    end
    collector -. bridges .-> arc_otel_net
```

**Rules:**

* `arc_platform_net` is declared `external: true` in each compose file — created once (`docker network create arc_platform_net`)
* `arc_otel_net` stays internal to the otel stack; only the collector joins both
* Messaging services join only `arc_platform_net`
* The collector scrapes NATS/Pulsar metrics by container hostname over `arc_platform_net`
* `arc_otel_net` is **removed** from this feature — the old single-network `arc_otel_net` in the otel compose becomes the internal net; the collector gains `arc_platform_net`

## Tech Debt

| ID | Item | Rationale |
|----|------|-----------|
| TD-001 | Redis Prometheus metrics via `redis_exporter` sidecar | `redis_exporter` adds a sidecar container per redis instance. Deferred until we have a concrete dashboard requirement. When ready: add `oliver006/redis_exporter:latest` alongside `arc-cache` in `services/cache/docker-compose.yml`, scrape `:9121/metrics` from the collector. |

## Edge Cases

| Scenario | Expected Behavior |
|----------|-------------------|
| Pulsar cold start > 60s | Health check allows 60s start\_period; compose waits before marking dependent services healthy |
| NATS data volume missing | JetStream creates `/data/jetstream` automatically; no manual init needed |
| Redis OOM with noeviction | Redis returns `OOM command not allowed` error to client; does not evict keys; operator must increase `maxmemory` |
| Collector cannot reach NATS `:8222` | Scrape fails silently; traces/logs still flow; no cascading failure |
| Pulsar admin port (8082) conflict | If `:8082` is already bound on host, docker compose fails at port bind; operator must free the port |
| `make messaging-up` before otel-up | Services start but OTEL metrics are lost until collector is running; not a blocking failure |
| `messaging-release.yml` — CRITICAL CVE | Security gate blocks release; creates GitHub issue per finding |

## Success Criteria

* \[ ] SC-1: `make messaging-up && make messaging-health` exits 0 with all three services healthy
* \[ ] SC-2: `make cortex-run-dev` succeeds with messaging services running; `/health/deep` reports healthy
* \[ ] SC-3: NATS, Pulsar, Redis metrics visible in arc-friday within 60s of `make otel-up && make messaging-up`
* \[ ] SC-4: `messaging-images.yml` CI completes in under 3 minutes for an amd64-only change
* \[ ] SC-5: `git tag messaging/v0.1.0 && git push --tags` triggers `messaging-release.yml` and publishes all three multi-platform images
* \[ ] SC-6: All Dockerfiles pass `trivy image` scan with zero CRITICAL CVEs
* \[ ] SC-7: `services/profiles.yaml` `think` profile includes flash, strange, sonic
* \[ ] SC-8: `docker inspect arc-messaging` shows it runs as non-root UID

## Docs & Links Update

* \[ ] Update `services/profiles.yaml` — add flash, strange, sonic to `think` profile
* \[ ] Update `CLAUDE.md` service codename table if changed
* \[ ] Verify `services/cortex/service.yaml` `depends_on` codenames match the new service.yaml `name` fields

## Constitution Compliance

| Principle | Applies | Compliant | Notes |
|-----------|---------|-----------|-------|
| I. Zero-Dep CLI | \[ ] | n/a | Services only — no CLI changes |
| II. Platform-in-a-Box | \[x] | \[x] | `make messaging-up` boots all three; joins `think` profile |
| III. Modular Services | \[x] | \[x] | Each service self-contained in own directory; flat under `services/` |
| IV. Two-Brain | \[x] | \[x] | Config-only services — no language concern |
| V. Polyglot Standards | \[x] | \[x] | Dockerfiles, compose, health checks follow otel pattern |
| VI. Local-First | \[ ] | n/a | CLI-only principle |
| VII. Observability | \[x] | \[x] | All three services scraped by collector → visible in arc-friday |
| VIII. Security | \[x] | \[x] | Non-root containers (NATS, Redis); host-only port binding; no secrets in git |
| IX. Declarative | \[ ] | n/a | CLI-only principle |
| X. Stateful Ops | \[ ] | n/a | CLI-only principle |
| XI. Resilience | \[x] | \[x] | Health checks with appropriate start\_periods; data volumes survive restart |
| XII. Interactive | \[ ] | n/a | CLI-only principle |

---

---
url: /arc-platform/specs-site/006-platform-control/spec.md
---
# Feature: Platform Control Plane Services Setup

> **Spec**: 006-platform-control
> **Author**: arc-framework
> **Date**: 2026-02-28
> **Status**: Draft

## Target Modules

| Module | Path | Impact |
|--------|------|--------|
| Services | `services/gateway/` | New — Traefik v3 (Heimdall), ports 80/8090 |
| Services | `services/secrets/` | New — OpenBao (Nick Fury), port 8200 |
| Services | `services/flags/` | New — Unleash (Mystique), port 4242 |
| Services | `services/profiles.yaml` | Update — heimdall → `think`; nick-fury + mystique → `reason` |
| CI/CD | `.github/workflows/` | New — control-images.yml + control-release.yml |
| Makefile | `Makefile`, `*.mk` | New — heimdall.mk, nick-fury.mk, mystique.mk, control.mk |

## Overview

Provision Traefik v3 (Heimdall), OpenBao (Nick Fury), and Unleash (Mystique) as the platform control plane. Heimdall is the HTTP ingress gateway with Docker label-based routing; Nick Fury provides secrets management in auto-unsealed dev mode; Mystique manages feature flags with Postgres (Oracle) and Redis (Sonic) as its backend. All three follow the canonical thin-wrapper pattern from 003-messaging-setup and 005-data-layer: Dockerfile with OCI labels, `service.yaml`, `docker-compose.yml`, `.mk` include, and CI/release workflows.

## Architecture

```mermaid
graph TD
    subgraph arc_platform_net
        heimdall["arc-gateway\n(Traefik v3)\n:80 HTTP · :8090 dashboard"]
        fury["arc-vault\n(OpenBao)\n:8200 API + UI"]
        mystique["arc-flags\n(Unleash)\n:4242 UI + REST"]
        oracle["arc-sql-db\n(Postgres 17)\n:5432"]
        sonic["arc-cache\n(Redis)\n:6379"]
    end

    internet["External Traffic"] -->|HTTP :80| heimdall
    heimdall -->|label-based routing| services["Platform Services"]
    apps["Client Apps"] -->|secrets API| fury
    apps -->|feature flag eval| mystique
    mystique -->|feature flag state| oracle
    mystique -->|session cache| sonic
    dockersock["/var/run/docker.sock (ro)"] -.->|provider.docker| heimdall
```

### Service Roles

| Codename | Image | Port(s) | Role |
|----------|-------|---------|------|
| Heimdall | `traefik:v3` | 80 · 8090 | HTTP ingress; Docker label routing; no TLS in dev |
| Nick Fury | `openbao/openbao` | 8200 | Secrets API; dev mode auto-unsealed, in-memory |
| Mystique | `unleashorg/unleash-server` | 4242 | Feature flags; Postgres state + Redis cache |

### Directory Layout

```
services/
├── gateway/            ← Heimdall (Traefik v3)
│   ├── Dockerfile
│   ├── service.yaml
│   ├── docker-compose.yml
│   └── heimdall.mk
├── secrets/            ← Nick Fury (OpenBao)
│   ├── Dockerfile
│   ├── service.yaml
│   ├── docker-compose.yml
│   └── nick-fury.mk
└── flags/              ← Mystique (Unleash)
    ├── Dockerfile
    ├── service.yaml
    ├── docker-compose.yml
    └── mystique.mk
```

## User Scenarios & Testing

### P1 — Must Have

**US-1**: As a platform developer, I want to start all three control plane services with a single make command so the control plane is ready immediately.

* **Given**: Docker is running and `arc_platform_net` exists
* **When**: `make control-up` is executed
* **Then**: Heimdall, Nick Fury, and Mystique start; all Docker health checks pass
* **Test**: `make control-health` exits 0

**US-2**: As a platform developer, I want Heimdall to auto-discover running services via Docker labels so I can route HTTP traffic without editing config files.

* **Given**: Another service runs with `traefik.enable=true` label on `arc_platform_net`
* **When**: Heimdall is running
* **Then**: Route is available via Heimdall on port 80; dashboard at `:8090` shows the route
* **Test**: `curl -s http://localhost:8090/api/http/routers` lists the route

**US-3**: As a platform developer, I want Nick Fury running in dev mode so I can read/write secrets without unsealing.

* **Given**: `make nick-fury-up` has succeeded
* **When**: `curl -H "X-Vault-Token: arc-dev-token" http://localhost:8200/v1/sys/health`
* **Then**: HTTP 200; `"initialized": true, "sealed": false`
* **Test**: `make nick-fury-health` exits 0

**US-4**: As a platform developer, I want Mystique to start and serve the feature flag UI on `:4242` backed by Oracle and Sonic.

* **Given**: `arc-sql-db` and `arc-cache` are healthy; `make mystique-up` called
* **When**: `curl http://localhost:4242/health`
* **Then**: HTTP 200; Unleash UI accessible at `http://localhost:4242`
* **Test**: `make mystique-health` exits 0

### P2 — Should Have

**US-5**: As a CI consumer, I want Docker images for all three control services built and pushed on main merges.

* **Given**: A commit touches `services/gateway/**`, `services/secrets/**`, or `services/flags/**`
* **When**: `control-images.yml` workflow runs
* **Then**: `arc-gateway`, `arc-vault`, `arc-flags` updated on GHCR with `sha-*` tag
* **Test**: GHCR package shows `sha-*` tag after CI

**US-6**: As a release engineer, I want versioned images via `control/vX.Y.Z` tag so control plane releases are atomic.

* **Test**: `git tag control/v0.1.0 && git push --tags` triggers `control-release.yml`; multi-platform images published

**US-7**: As a platform developer, I want each service to have `service.yaml` for declarative CLI discovery.

* **Test**: All three `service.yaml` files contain name, codename, image, ports, health, depends\_on

### P3 — Nice to Have

**US-8**: As a developer, I want `make control-logs` to tail all three services simultaneously.

* **Test**: `make control-logs` fans out with `[heimdall]`, `[nick-fury]`, `[mystique]` prefixes

## Requirements

### Functional

* \[ ] FR-1: Create `services/gateway/` with Traefik v3 Dockerfile (`traefik:v3`), `service.yaml`, `docker-compose.yml`, `heimdall.mk`
* \[ ] FR-2: Create `services/secrets/` with OpenBao Dockerfile (`openbao/openbao`), `service.yaml`, `docker-compose.yml`, `nick-fury.mk`
* \[ ] FR-3: Create `services/flags/` with Unleash Dockerfile (`unleashorg/unleash-server`), `service.yaml`, `docker-compose.yml`, `mystique.mk`
* \[ ] FR-4: Heimdall must bind port 80 (HTTP) and 8090 (dashboard); configured via CLI args only (`--api.dashboard=true --providers.docker=true`); Docker socket mounted read-only; no TLS in dev
* \[ ] FR-5: Nick Fury must run in OpenBao dev mode (`-dev` flag); root token set via `VAULT_DEV_ROOT_TOKEN_ID=arc-dev-token`; no persistent volume (stateless in dev)
* \[ ] FR-6: Mystique must connect to Oracle via `DATABASE_URL=postgresql://arc:arc@arc-sql-db:5432/unleash` and to Sonic via `REDIS_HOST=arc-cache REDIS_PORT=6379`; DB migrations run automatically on startup
* \[ ] FR-7: Update `services/profiles.yaml` — add `heimdall` to `think`; add `nick-fury` + `mystique` to `reason`
* \[ ] FR-8: Create `control-images.yml` CI — path-filtered per service dir, builds all three, `linux/amd64` only in CI
* \[ ] FR-9: Create `control-release.yml` — tag format `control/vX.Y.Z`, multi-platform (`linux/amd64,linux/arm64`), creates GitHub release
* \[ ] FR-10: Create `services/control.mk` with `control-up`, `control-down`, `control-health`, `control-logs`
* \[ ] FR-11: Include `heimdall.mk`, `nick-fury.mk`, `mystique.mk`, `control.mk` in root Makefile; add all three to `publish-all`

### Non-Functional

* \[ ] NFR-1: Docker socket mounted read-only (`/var/run/docker.sock:/var/run/docker.sock:ro`) — Heimdall only
* \[ ] NFR-2: Nick Fury is explicitly stateless in dev (no volume); documented in `nick-fury-help` output
* \[ ] NFR-3: All ports bind to `127.0.0.1` only — no `0.0.0.0` exposure
* \[ ] NFR-4: Verify non-root user for each upstream image; add `USER` + `RUN chown` in Dockerfile if root by default
* \[ ] NFR-5: All Dockerfiles include OCI (`org.opencontainers.*`) and `arc.service.*` labels
* \[ ] NFR-6: Mystique `service.yaml` `depends_on` references `oracle` and `sonic`
* \[ ] NFR-7: CI build completes in under 3 minutes (amd64 only; no QEMU)

### Key Entities

| Entity | Module | Description |
|--------|--------|-------------|
| `arc-gateway` | `services/gateway/` | Traefik v3; HTTP gateway, Docker label routing, dashboard at :8090 |
| `arc-vault` | `services/secrets/` | OpenBao; dev-mode secrets API, auto-unsealed, token `arc-dev-token` |
| `arc-flags` | `services/flags/` | Unleash; feature flags backed by Oracle (Postgres) + Sonic (Redis) |
| `heimdall.mk` | `services/gateway/` | Make targets: heimdall-up/down/health/logs/build/push/publish/tag/clean/nuke |
| `nick-fury.mk` | `services/secrets/` | Make targets: nick-fury-up/down/health/logs/build/push/publish/tag/clean/nuke |
| `mystique.mk` | `services/flags/` | Make targets: mystique-up/down/health/logs/build/push/publish/tag/clean/nuke |

## Port Reference

| Service | Container Port | Host Binding | Purpose |
|---------|---------------|--------------|---------|
| Heimdall | 80 | `127.0.0.1:80` | HTTP entrypoint — routes to platform services |
| Heimdall | 8090 | `127.0.0.1:8090` | Traefik dashboard (avoids :8081 Cortex, :8082 Pulsar) |
| Nick Fury | 8200 | `127.0.0.1:8200` | OpenBao API + Web UI |
| Mystique | 4242 | `127.0.0.1:4242` | Unleash REST API + Web UI |

## Network Strategy

All three services join `arc_platform_net` only. Heimdall additionally mounts the Docker socket (bind mount, not a network) for service discovery.

```mermaid
graph LR
    subgraph arc_platform_net [arc_platform_net — external, shared]
        heimdall[arc-gateway]
        fury[arc-vault]
        mystique[arc-flags]
        oracle[arc-sql-db]
        sonic[arc-cache]
    end
    dockersock["/var/run/docker.sock"] -.->|read-only bind| heimdall
```

**Rules:**

* `arc_platform_net` declared `external: true` in all three compose files
* Mystique must be on the same network as `arc-sql-db` and `arc-cache`
* Container hostnames are DNS-resolvable across the network (used by Mystique's `DATABASE_URL`)

## Edge Cases

| Scenario | Expected Behavior |
|----------|-------------------|
| `make mystique-up` before Oracle ready | Unleash retries DB connection; health fails until Oracle is healthy |
| `make mystique-up` before Sonic ready | Unleash starts in Postgres-only mode; logs Redis warning; feature flags still work |
| Nick Fury restarts | All secrets lost (in-memory dev mode); clients must re-fetch — expected, documented |
| Heimdall dashboard port :8090 conflicts | Startup fails; check `make dev-status` for port occupants |
| Service starts without `traefik.enable=true` | Not routed by Heimdall — opt-in model by design |
| `make control-up` before `arc_platform_net` exists | Docker compose fails at network lookup; run `make dev-networks` or `docker network create arc_platform_net` |
| Mystique DB migration fails on startup | Container exits non-zero; check `make mystique-logs` — usually a DB connection issue |
| Unleash default admin credentials | Unleash auto-creates admin user on first boot; credentials printed in logs |

## Success Criteria

* \[ ] SC-1: `make control-up && make control-health` exits 0; all three containers healthy
* \[ ] SC-2: Traefik dashboard accessible at `http://localhost:8090`
* \[ ] SC-3: `curl -H "X-Vault-Token: arc-dev-token" http://localhost:8200/v1/sys/health` returns HTTP 200
* \[ ] SC-4: Unleash UI accessible at `http://localhost:4242`; `/health` returns 200
* \[ ] SC-5: `control-images.yml` CI completes in under 3 minutes (amd64 only)
* \[ ] SC-6: `git tag control/v0.1.0` triggers `control-release.yml`; multi-platform images on GHCR
* \[ ] SC-7: All Dockerfiles pass `trivy image` scan with zero CRITICAL CVEs
* \[ ] SC-8: `services/profiles.yaml` `think` profile includes `heimdall`; `reason` profile includes `nick-fury` and `mystique`

## Docs & Links Update

* \[ ] Update `services/profiles.yaml` — heimdall → `think`; nick-fury + mystique → `reason`
* \[ ] Update `CLAUDE.md` monorepo layout to reference `gateway/`, `secrets/`, `flags/` directories
* \[ ] Update `CLAUDE.md` Service Codenames table — Nick Fury: Infisical → OpenBao
* \[ ] Update `.specify/config.yaml` `secrets` entry — change tech from `infisical` to `openbao`
* \[ ] Verify `services/flags/service.yaml` `depends_on` lists `oracle` and `sonic`

## Constitution Compliance

| Principle | Applies | Compliant | Notes |
|-----------|---------|-----------|-------|
| I. Zero-Dep CLI | \[ ] | n/a | Services only — no CLI changes |
| II. Platform-in-a-Box | \[x] | \[x] | `make control-up` boots all three; heimdall joins `think` profile |
| III. Modular Services | \[x] | \[x] | Each self-contained in own directory; flat under `services/` |
| IV. Two-Brain | \[x] | \[x] | Config-only upstream images — no language concern |
| V. Polyglot Standards | \[x] | \[x] | Follows 003/005 patterns: Dockerfile, compose, .mk, CI |
| VI. Local-First | \[ ] | n/a | CLI-only principle |
| VII. Observability | \[x] | \[x] | Traefik dashboard + metrics; OpenBao `/v1/sys/health`; Unleash `/health` |
| VIII. Security | \[x] | \[x] | Non-root containers; 127.0.0.1-only; Docker socket ro; no secrets in git |
| IX. Declarative | \[ ] | n/a | CLI-only principle |
| X. Stateful Ops | \[ ] | n/a | CLI-only principle |
| XI. Resilience | \[x] | \[x] | Health checks with start\_periods; Mystique compose `depends_on` oracle + sonic |
| XII. Interactive | \[ ] | n/a | CLI-only principle |

---

---
url: /arc-platform/specs-site/007-voice-stack/spec.md
---
# Feature: Realtime Voice Infrastructure Setup

> **Spec**: 007-voice-stack
> **Author**: arc-framework
> **Date**: 2026-03-01
> **Status**: Draft

## Target Modules

| Module | Path | Impact |
|--------|------|--------|
| Services | `services/realtime/` | New — LiveKit Server (Daredevil), LK Ingress (Sentry), LK Egress (Scribe) |
| Services | `services/profiles.yaml` | Update — add `realtime` to all three profiles |
| CI/CD | `.github/workflows/` | New — realtime-images.yml + realtime-release.yml |
| Makefile | `Makefile`, `services/realtime/realtime.mk` | New — individual + aggregate realtime targets |

## Overview

Provision LiveKit Server (Daredevil), LiveKit Ingress (Sentry), and LiveKit Egress (Scribe) as the platform realtime voice/media infrastructure. All three are thin-wrapper Dockerfiles over official LiveKit images, co-located in `services/realtime/` with a shared `docker-compose.yml`. Daredevil provides WebRTC media rooms; Sentry pulls external media (RTMP/WHIP) into rooms; Scribe records and exports room content. Follows the established thin-wrapper pattern from 003-messaging-setup, 005-data-layer, and 006-platform-control.

## Architecture

```mermaid
graph TD
    subgraph arc_platform_net
        realtime["arc-realtime\n(LiveKit Server / Daredevil)\n:7880 HTTP · :7881 gRPC · :7882 TURN"]
        ingress["arc-realtime-ingress\n(LK Ingress / Sentry)\n:7888 HTTP · :1935 RTMP"]
        egress["arc-realtime-egress\n(LK Egress / Scribe)\n:7889 HTTP"]
        cache["arc-cache\n(Redis)\n:6379"]
        storage["arc-storage\n(MinIO)\n:9000"]
    end

    webrtc["WebRTC Clients"] -->|7880/7881 signal + media| realtime
    rtmp["RTMP Sources"] -->|1935 RTMP| ingress
    ingress -->|ws://arc-realtime:7880| realtime
    egress -->|realtime API :7880| realtime
    egress -->|S3 recordings| storage
    realtime -->|multi-node pub/sub| cache
    realtime -.->|50100-50200/UDP RTP| webrtc
```

### Service Roles

| Codename | Role | Image | Ports | Depends on |
|----------|------|-------|-------|------------|
| Daredevil | `realtime` | `livekit/livekit-server` | 7880/7881/7882/50100-50200udp | cache |
| Sentry | `realtime-ingress` | `livekit/ingress` | 7888/1935 | realtime |
| Scribe | `realtime-egress` | `livekit/egress` | 7889 | realtime, storage |

### Directory Layout

```
services/realtime/
├── Dockerfile              # arc-realtime (FROM livekit/livekit-server)
├── Dockerfile.ingress      # arc-realtime-ingress (FROM livekit/ingress)
├── Dockerfile.egress       # arc-realtime-egress (FROM livekit/egress)
├── service.yaml            # role: realtime, codename: daredevil
├── docker-compose.yml      # All 3 services; shared arc_platform_net
├── livekit.yaml            # LiveKit server config (mounted at /etc/livekit.yaml)
├── ingress.yaml            # Ingress config (LIVEKIT_URL: ws://arc-realtime:7880)
├── egress.yaml             # Egress config (LIVEKIT_URL, API credentials, storage endpoint)
└── realtime.mk             # Targets: realtime-* / realtime-ingress-* / realtime-egress-*
```

### Co-location Rationale

Sentry and Scribe are tightly coupled to Daredevil — they can never run without it. Placing all three in one directory with a shared compose file:

* Prevents accidental partial-stack deploys
* Shares config files (`livekit.yaml` is mounted by ingress too for API keys)
* Single Make include vs three fragments
* `profiles.yaml` registers just `realtime`; `make realtime-up` brings up all three

## User Scenarios & Testing

### P1 — Must Have

**US-1**: As a platform developer, I want all three realtime services to start with `make realtime-up` so the voice stack is ready as a unit.

* **Given**: Docker is running, `arc_platform_net` exists, and `arc-cache` is healthy
* **When**: `make realtime-up`
* **Then**: arc-realtime, arc-realtime-ingress, arc-realtime-egress all start and Docker health checks pass
* **Test**: `make realtime-health` exits 0

**US-2**: As a platform developer, I want the LiveKit API accessible at `:7880` so WebRTC clients can connect.

* **Given**: `make realtime-up` succeeded
* **When**: `curl -s http://localhost:7880`
* **Then**: HTTP 200 or redirect (LiveKit API responds)
* **Test**: `make realtime-health` exits 0

**US-3**: As a platform developer, I want Sentry's RTMP port `:1935` and controller `:7888` available so ingest streams work.

* **Given**: Daredevil is healthy; Sentry is running
* **When**: `curl -s http://localhost:7888`
* **Then**: Sentry controller responds with 200
* **Test**: `make realtime-ingress-health` exits 0

**US-4**: As a platform developer, I want Scribe's controller at `:7889` available so recording requests work.

* **Given**: Daredevil is healthy; Scribe is running
* **When**: `curl -s http://localhost:7889`
* **Then**: Scribe controller responds with 200
* **Test**: `make realtime-egress-health` exits 0

**US-5**: As a developer running `make dev`, I want realtime to start after cache in the dependency-ordered boot so the stack is coherent.

* **Given**: `make dev` is executed (think profile)
* **When**: Dependency layers are resolved
* **Then**: `cache` layer starts before `realtime` layer; all health checks pass
* **Test**: `make dev && make dev-health` exits 0

### P2 — Should Have

**US-6**: As a CI consumer, I want all three images built and pushed on main merges.

* **Given**: A commit touches `services/realtime/**`
* **When**: `realtime-images.yml` workflow runs
* **Then**: `arc-realtime`, `arc-realtime-ingress`, `arc-realtime-egress` updated on GHCR with `sha-*` tag
* **Test**: GHCR packages show `sha-*` tag after CI

**US-7**: As a release engineer, I want versioned images via `realtime/vX.Y.Z` tag.

* **Test**: `git tag realtime/v0.1.0 && git push --tags` triggers `realtime-release.yml`; multi-platform images on GHCR

**US-8**: As a platform developer, I want `service.yaml` for declarative CLI discovery.

* **Test**: `services/realtime/service.yaml` contains role, codename, image, ports, health, depends\_on

### P3 — Nice to Have

**US-9**: As a developer, I want `make realtime-logs` to tail all three services with service-prefixed output.

* **Test**: Log lines from each service are distinguishable by container name

## Requirements

### Functional

* \[ ] FR-1: Create `services/realtime/Dockerfile` (arc-realtime), `Dockerfile.ingress` (arc-realtime-ingress), `Dockerfile.egress` (arc-realtime-egress) — thin wrappers with OCI + arc.service.\* labels
* \[ ] FR-2: Create `services/realtime/docker-compose.yml` — defines all 3 services on `arc_platform_net`; mounts config files; `arc-realtime-ingress` and `arc-realtime-egress` depend on `arc-realtime`
* \[ ] FR-3: Create `services/realtime/livekit.yaml` — LiveKit server config; `keys: devkey: devsecret`; Redis URL `redis://arc-cache:6379`; node IP from `LIVEKIT_NODE_IP` env (default `127.0.0.1`)
* \[ ] FR-4: Create `services/realtime/ingress.yaml` — Ingress config; `api_url: ws://arc-realtime:7880`; API key/secret match livekit.yaml dev values
* \[ ] FR-5: Create `services/realtime/egress.yaml` — Egress config; `api_url: ws://arc-realtime:7880`; API key/secret match livekit.yaml dev values; S3 output to `arc-storage:9000`
* \[ ] FR-6: Create `services/realtime/service.yaml` — `role: realtime`, `codename: daredevil`, `image: ghcr.io/arc-framework/arc-realtime:latest`, health endpoint `:7880`, `depends_on: [cache]`
* \[ ] FR-7: Create `services/realtime/realtime.mk` — targets: `realtime-up/down/health/logs/build/push/publish/tag/clean/nuke`, `realtime-ingress-up/down/health/logs`, `realtime-egress-up/down/health/logs`
* \[ ] FR-8: Update `services/profiles.yaml` — add `realtime` to `think`, `reason`, and `ultra-instinct`
* \[ ] FR-9: Create `.github/workflows/realtime-images.yml` — path-filtered to `services/realtime/**`, builds all 3 images separately, `linux/amd64` only in CI
* \[ ] FR-10: Create `.github/workflows/realtime-release.yml` — tag format `realtime/vX.Y.Z`, multi-platform (`linux/amd64,linux/arm64`), creates GitHub release
* \[ ] FR-11: Include `services/realtime/realtime.mk` in root Makefile; add realtime build/publish targets to `publish-all`

### Non-Functional

* \[ ] NFR-1: All TCP ports bind to `127.0.0.1` only; WebRTC RTP UDP range `50100-50200` binds to `0.0.0.0` (required for WebRTC NAT traversal)
* \[ ] NFR-2: LiveKit API key/secret for dev is static (`devkey`/`devsecret`) defined directly in config files and compose; vault integration is a tracked future enhancement (see Docs section)
* \[ ] NFR-3: Non-root containers — verify upstream image user; add `USER` + `RUN chown` in Dockerfile if default is root
* \[ ] NFR-4: All three Dockerfiles include OCI (`org.opencontainers.*`) and `arc.service.*` labels including `arc.service.codename`
* \[ ] NFR-5: `arc-realtime` compose service mounts `livekit.yaml` as read-only at `/etc/livekit.yaml`
* \[ ] NFR-6: `LIVEKIT_NODE_IP` must be documented in `realtime-help` output; defaults to `127.0.0.1` for local dev
* \[ ] NFR-7: CI build of 3 images must complete in under 5 minutes (amd64 only, no QEMU)

### Key Entities

| Entity | Module | Description |
|--------|--------|-------------|
| `arc-realtime` | `services/realtime/` | LiveKit Server; WebRTC rooms; Redis-backed multi-node state |
| `arc-realtime-ingress` | `services/realtime/` | LiveKit Ingress; RTMP→WebRTC ingest pipeline |
| `arc-realtime-egress` | `services/realtime/` | LiveKit Egress; recording + export to arc-storage (MinIO) |
| `livekit.yaml` | `services/realtime/` | LiveKit server config; API keys, ICE config, Redis URL |
| `ingress.yaml` | `services/realtime/` | LK Ingress config; LiveKit API URL + credentials |
| `egress.yaml` | `services/realtime/` | LK Egress config; LiveKit API URL + credentials + storage |
| `realtime.mk` | `services/realtime/` | Make targets for all 3 realtime services |

## Port Reference

| Service | Container Port | Host Binding | Protocol | Purpose |
|---------|----------------|--------------|----------|---------|
| arc-realtime | 7880 | `127.0.0.1:7880` | HTTP | LiveKit API + WebRTC signalling |
| arc-realtime | 7881 | `127.0.0.1:7881` | gRPC | gRPC API |
| arc-realtime | 7882 | `127.0.0.1:7882` | TCP | TURN |
| arc-realtime | 50100-50200 | `0.0.0.0:50100-50200` | UDP | WebRTC RTP media (NAT traversal required) |
| arc-realtime-ingress | 7888 | `127.0.0.1:7888` | HTTP | Ingress controller API |
| arc-realtime-ingress | 1935 | `127.0.0.1:1935` | TCP | RTMP ingest |
| arc-realtime-egress | 7889 | `127.0.0.1:7889` | HTTP | Egress controller API |

## Network Strategy

All three services join `arc_platform_net` only. The UDP range `50100-50200` must bind `0.0.0.0` — WebRTC clients outside the Docker network need direct reachability to this range for RTP media.

```mermaid
graph LR
    subgraph arc_platform_net [arc_platform_net — external, shared]
        realtime[arc-realtime]
        ingress[arc-realtime-ingress]
        egress[arc-realtime-egress]
        cache[arc-cache]
        storage[arc-storage]
    end
    ingress -->|internal DNS| realtime
    egress -->|internal DNS| realtime
    egress -->|S3 API| storage
    realtime -->|redis| cache
```

## Edge Cases

| Scenario | Expected Behavior |
|----------|-------------------|
| `make realtime-up` before cache ready | LiveKit starts in single-node mode; logs Redis warning; multi-node state unavailable but rooms function |
| `make realtime-ingress-up` before Daredevil healthy | Sentry retries; health check fails until Daredevil responds on :7880 |
| `make realtime-egress-up` before storage ready | Scribe starts; recording requests return error until arc-storage is healthy |
| UDP 50100-50200 blocked by host firewall | WebRTC ICE fails; clients connect but no media — must open UDP range on dev machine |
| `LIVEKIT_NODE_IP` not set | Defaults to `127.0.0.1`; works for local dev; fails for remote clients (set to machine IP) |
| API key mismatch between server and ingress/egress | Sentry/Scribe return 401; all three must use `devkey`/`devsecret` from same source |
| `make realtime-clean` with active WebRTC rooms | Rooms destroyed immediately; all connected clients disconnected — expected, documented |
| Port 1935 (RTMP) in use by OBS or other media tool | Sentry startup fails; check `make dev-status` and stop conflicting process |

## Success Criteria

* \[ ] SC-1: `make realtime-up && make realtime-health` exits 0; all three containers healthy
* \[ ] SC-2: `curl -s http://localhost:7880` returns a LiveKit response (2xx or 3xx)
* \[ ] SC-3: `curl -s http://localhost:7888` returns 200 (Sentry controller)
* \[ ] SC-4: `curl -s http://localhost:7889` returns 200 (Scribe controller)
* \[ ] SC-5: `make dev` (think profile) includes `realtime`; `make dev-health` exits 0
* \[ ] SC-6: `realtime-images.yml` CI completes in under 5 minutes (3 images, amd64 only)
* \[ ] SC-7: `git tag realtime/v0.1.0` triggers `realtime-release.yml`; multi-platform images on GHCR
* \[ ] SC-8: All three Dockerfiles pass `trivy image` scan with zero CRITICAL CVEs

## Docs & Links Update

* \[ ] Update `services/profiles.yaml` — add `realtime` to `think`, `reason`, `ultra-instinct`
* \[ ] Update `CLAUDE.md` monorepo layout to reference `services/realtime/` and note Daredevil/Sentry/Scribe
* \[ ] Update `CLAUDE.md` Service Codenames table — add Daredevil (LiveKit), Sentry (LK Ingress), Scribe (LK Egress)
* \[ ] Update `.specify/config.yaml` — update `realtime` entry (`role: realtime, codename: daredevil`); add `realtime-ingress` (Sentry) and `realtime-egress` (Scribe) entries
* \[ ] Update `scripts/lib/check-dev-prereqs.sh` — add ports 7880/7881/1935 to required port checks
* \[ ] Track vault integration for LiveKit API keys as a follow-on task (production hardening)
* \[ ] Verify `services/realtime/service.yaml` `depends_on` lists `cache`

## Constitution Compliance

| Principle | Applies | Compliant | Notes |
|-----------|---------|-----------|-------|
| I. Zero-Dep CLI | \[ ] | n/a | Services only — no CLI changes |
| II. Platform-in-a-Box | \[x] | \[x] | `make realtime-up` boots all three; joined to all profiles including `think` |
| III. Modular Services | \[x] | \[x] | Self-contained in `services/realtime/`; three images, one service.yaml, one compose |
| IV. Two-Brain | \[x] | \[x] | Config-only upstream images — no Python or Go custom code |
| V. Polyglot Standards | \[x] | \[x] | Follows 003/005/006 thin-wrapper pattern exactly |
| VI. Local-First | \[ ] | n/a | CLI-only principle |
| VII. Observability | \[x] | \[x] | LiveKit exposes HTTP health on :7880; ingress :7888; egress :7889; all have Docker health checks |
| VIII. Security | \[x] | \[x] | Non-root containers; TCP ports on 127.0.0.1; no secrets in git; UDP RTP exception documented |
| IX. Declarative | \[ ] | n/a | CLI-only principle |
| X. Stateful Ops | \[ ] | n/a | CLI-only principle |
| XI. Resilience | \[x] | \[x] | Docker healthchecks with start\_periods; depends\_on ordering in compose |
| XII. Interactive | \[ ] | n/a | CLI-only principle |

***

## Open Questions (Resolved)

| Question | Decision |
|----------|----------|
| Directory | `services/realtime/` (matches config.yaml `dir: realtime`) |
| Image names | `arc-realtime`, `arc-realtime-ingress`, `arc-realtime-egress` |
| API keys dev strategy | Static `devkey`/`devsecret` in config files; vault integration is future production work |
| Profiles | All three — `think`, `reason`, `ultra-instinct` |
| Co-location | Single directory, shared compose, one profiles.yaml entry (`realtime`) |

---

---
url: /arc-platform/specs-site/008-specs-site/spec.md
---
# Feature: Specs Documentation Site

> **Spec**: 008-specs-site
> **Author**: arc-framework
> **Date**: 2026-03-01
> **Status**: Draft

## Target Modules

| Module | Path | Impact |
|--------|------|--------|
| Docs | `docs/specs/` | New — MkDocs Material config, landing page, `requirements.txt` |
| CI/CD | `.github/workflows/` | New — `deploy-specs-site.yml` for GitHub Pages deployment |
| Specs | `specs/` | Additive-only — `.pages` files per folder for nav titles; no existing files modified |
| Root | `.gitignore` | Update — add `site_build/` |
| Root | `CLAUDE.md` | Update — add local preview command |

No CLI, SDK, or service changes in this spec.

## Overview

Publish the `specs/` folder as a searchable, mermaid-rendered static site using MkDocs Material,
deployed to GitHub Pages via CI. The site reads spec, plan, and task markdown directly from
`specs/` via `docs_dir` — zero content duplication. Navigation is driven by
`mkdocs-awesome-pages-plugin` with per-folder `.pages` files; adding a new spec folder only
requires dropping in a `.pages` file.

## Architecture

```mermaid
graph LR
    subgraph repo ["arc-platform repo"]
        specs["specs/\n(source of truth)\n+ .pages per folder"]
        config["docs/specs/\nmkdocs.yml\nindex.md\nrequirements.txt"]
    end

    subgraph ci ["GitHub Actions"]
        action["deploy-specs-site.yml\non push to main\nspecs/** or docs/specs/**"]
    end

    subgraph gh ["GitHub Pages"]
        site["arc-framework.github.io\n/arc-platform/specs-site/"]
    end

    specs -->|"docs_dir: ../../specs/"| config
    config -->|pip install + mkdocs build| action
    action -->|mkdocs gh-deploy --force| gh
```

### Directory Layout

```
docs/
└── specs/                       # MkDocs site config (new)
    ├── mkdocs.yml               # docs_dir: ../../specs/; awesome-pages plugin
    ├── index.md                 # Landing page — project overview + spec index
    └── requirements.txt         # Pinned: mkdocs-material>=9.5, awesome-pages plugin

specs/                           # Content source (minimal additions only)
├── .pages                       # Root nav order (index.md first)
├── 001-otel-setup/
│   └── .pages                   # Section title: "001 — OTEL Setup"; file title map
├── 002-cortex-setup/
│   └── .pages
├── 003-messaging-setup/
│   └── .pages
├── 004-dev-setup/
│   └── .pages
├── 005-data-layer/
│   └── .pages
├── 006-platform-control/
│   └── .pages
└── 007-voice-stack/
    └── .pages

site_build/                      # Build output (gitignored)
```

### Navigation Strategy — `mkdocs-awesome-pages-plugin`

Each spec folder gets a `.pages` file that defines section title and file order:

```yaml
# specs/003-messaging-setup/.pages
title: "003 — Messaging Setup"
nav:
  - spec.md: Specification
  - plan.md: Implementation Plan
  - tasks.md: Task Breakdown
  - analysis-report.md: Analysis Report
```

Root `.pages` controls top-level ordering:

```yaml
# specs/.pages
nav:
  - index.md
  - 001-otel-setup
  - 002-cortex-setup
  - 003-messaging-setup
  - 004-dev-setup
  - 005-data-layer
  - 006-platform-control
  - 007-voice-stack
```

**Why `awesome-pages` over alternatives:**

| Option | Verdict |
|--------|---------|
| Hardcode `nav:` in `mkdocs.yml` | Brittle — breaks every time a spec is added |
| Script-generated `nav:` in CI | Fragile — YAML generation is error-prone |
| `mkdocs-awesome-pages-plugin` | Standard pattern; `.pages` files are declarative and additive |
| Restructure `docs/` to avoid `docs_dir` | Adds duplication; contradicts zero-copy goal |

### `.work-docs/` Exclusion

`.work-docs/` directories are excluded from source control via `.gitignore`:

```
specs/**/.work-docs/
```

MkDocs only builds files on disk from `docs_dir`. Since `.work-docs/` is never committed, no
build-time filter is needed. The `.pages` nav files also omit `.work-docs/` entries as a second
exclusion layer.

`pr-description.md` and `analysis-report.md` are **intentionally included** — they are useful
contributor references.

## User Scenarios & Testing

### P1 — Must Have

**US-1**: As a contributor, I want to browse all feature specs in a searchable website so that I
can find architecture decisions without digging through raw markdown on GitHub.

* **Given**: The site is deployed at `arc-framework.github.io/arc-platform/specs-site/`
* **When**: I visit the URL and search for "NATS"
* **Then**: Search results show entries from `003-messaging-setup/spec.md` with highlighted matches
* **Test**: Navigate to deployed site; search "NATS"; verify results include 003 spec link

**US-2**: As a contributor, I want mermaid architecture diagrams to render visually so that I can
understand service topology at a glance.

* **Given**: A spec contains a ` ```mermaid ` fenced block
* **When**: The page loads in a browser
* **Then**: The diagram renders as an SVG — not raw text
* **Test**: Open `001-otel-setup/spec.md` on the site; verify the OTEL architecture graph renders

**US-3**: As a contributor, I want the site to auto-deploy when specs are merged to main so that
the site is always current without manual steps.

* **Given**: A PR touching `specs/**` or `docs/specs/**` is merged to main
* **When**: The GitHub Action triggers
* **Then**: Site rebuilds and the change is live within 2 minutes
* **Test**: Merge a one-line spec change; watch CI; verify updated content on site

**US-4**: As a contributor, I want to preview the site locally so that I can verify mermaid and
nav before pushing.

* **Given**: Python 3.10+ is installed; `pip install -r docs/specs/requirements.txt` has been run
* **When**: `mkdocs serve -f docs/specs/mkdocs.yml` is executed from repo root
* **Then**: Dev server starts at `localhost:8000` with live reload; mermaid diagrams render
* **Test**: Run command; open browser; verify diagrams and nav structure

### P2 — Should Have

**US-5**: As a contributor, I want dark mode so that I can read specs comfortably in any
environment.

* **Given**: The site is loaded in a browser
* **When**: I click the theme toggle in the header
* **Then**: Theme switches between light and dark; preference persists on reload
* **Test**: Toggle; reload; verify preference preserved (`localStorage` key `__palette`)

**US-6**: As a contributor, I want an "Edit on GitHub" link on every page so that I can jump
directly to the source file to propose changes.

* **Given**: Any spec page is open
* **When**: I click "Edit this page"
* **Then**: GitHub opens the source `.md` file in `specs/` in the editor
* **Test**: Click edit link on `002-cortex-setup/spec.md` page; verify URL points to correct file

## Requirements

### Functional

* \[ ] FR-1: Create `docs/specs/mkdocs.yml` — `docs_dir: ../../specs/`, Material theme,
  `awesome-pages` plugin, mermaid via `pymdownx.superfences`, `search` plugin, `edit_uri` pointing
  to `specs/` on GitHub
* \[ ] FR-2: Create `docs/specs/index.md` — landing page with project overview, spec index table
  (feature number, name, status, link), and local dev instructions
* \[ ] FR-3: Create `docs/specs/requirements.txt` — pinned `mkdocs-material>=9.5`,
  `mkdocs-awesome-pages-plugin>=2.9`, `pymdown-extensions>=10.0`; document Python 3.10+ requirement
* \[ ] FR-4: Add `.pages` files to each existing spec folder (`001`–`007`) with section title and
  file-to-title mapping; add root `specs/.pages` controlling top-level nav order
* \[ ] FR-5: Enable mermaid rendering via `pymdownx.superfences` custom fence — mermaid.js loaded
  from CDN via `extra_javascript`
* \[ ] FR-6: Enable instant search with built-in `search` plugin; `lang: en`
* \[ ] FR-7: Enable dark/light mode toggle via Material `palette` — two schemes (`default` →
  `slate`)
* \[ ] FR-8: Configure `edit_uri` to
  `https://github.com/arc-framework/arc-platform/edit/main/specs/` so edit links point at source
  files in `specs/`
* \[ ] FR-9: Create `.github/workflows/deploy-specs-site.yml` — triggers on push to `main` when
  `specs/**` or `docs/specs/**` change; uses `actions/setup-python@v5` with Python 3.12; installs
  `docs/specs/requirements.txt`; runs `mkdocs gh-deploy --force --config-file docs/specs/mkdocs.yml`
* \[ ] FR-10: Set `site_url: https://arc-framework.github.io/arc-platform/specs-site/` and
  `repo_url: https://github.com/arc-framework/arc-platform` in `mkdocs.yml` —
  **prerequisite**: GitHub Pages must be enabled with source = "Deploy from a branch" → `gh-pages`
* \[ ] FR-11: Update `.gitignore` — add `site_build/`
* \[ ] FR-12: Update `CLAUDE.md` Commands section — add
  `mkdocs serve -f docs/specs/mkdocs.yml  # preview specs site locally`

### Non-Functional

* \[ ] NFR-1: `mkdocs build` must complete in under 30 seconds for the current 7 specs
* \[ ] NFR-2: CI workflow (install + build + deploy) must complete in under 2 minutes
* \[ ] NFR-3: All existing mermaid diagrams in `specs/` must render without modifying any source
  `.md` file
* \[ ] NFR-4: Site must be readable on mobile (320px viewport minimum); mermaid diagrams scrollable
  on narrow screens — Material theme handles this by default
* \[ ] NFR-5: No secrets or tokens required — CI uses automatic `GITHUB_TOKEN` with
  `contents: write` permission

### Key Entities

| Entity | Module | Description |
|--------|--------|-------------|
| `mkdocs.yml` | `docs/specs/` | MkDocs Material config — theme, plugins, mermaid, nav strategy |
| `index.md` | `docs/specs/` | Site landing page — project overview and spec directory table |
| `requirements.txt` | `docs/specs/` | Pinned Python deps |
| `.pages` (per folder) | `specs/NNN-*/` | Section title + file-to-nav-title mapping |
| `specs/.pages` | `specs/` | Root nav order |
| `deploy-specs-site.yml` | `.github/workflows/` | CI — build and deploy to GitHub Pages on main push |

## MkDocs Configuration Reference

Full `docs/specs/mkdocs.yml`:

```yaml
site_name: "A.R.C. Platform — Specs"
site_url: "https://arc-framework.github.io/arc-platform/specs-site/"
repo_url: "https://github.com/arc-framework/arc-platform"
repo_name: "arc-framework/arc-platform"
edit_uri: "edit/main/specs/"

docs_dir: "../../specs/"
site_dir: "../../site_build/"

theme:
  name: material
  palette:
    - scheme: default
      primary: indigo
      accent: indigo
      toggle:
        icon: material/brightness-7
        name: Switch to dark mode
    - scheme: slate
      primary: indigo
      accent: indigo
      toggle:
        icon: material/brightness-4
        name: Switch to light mode
  features:
    - navigation.instant
    - navigation.sections
    - search.highlight
    - search.suggest
    - content.action.edit

markdown_extensions:
  - pymdownx.superfences:
      custom_fences:
        - name: mermaid
          class: mermaid
          format: !!python/name:pymdownx.superfences.fence_code_format
  - pymdownx.highlight:
      anchor_linenums: true
  - pymdownx.tabbed:
      alternate_style: true
  - tables
  - admonition
  - toc:
      permalink: true

plugins:
  - search:
      lang: en
  - awesome-pages

extra_javascript:
  - https://unpkg.com/mermaid@10/dist/mermaid.min.js
```

## CI Workflow Reference

`.github/workflows/deploy-specs-site.yml`:

```yaml
name: Deploy Specs Site
on:
  push:
    branches: [main]
    paths:
      - "specs/**"
      - "docs/specs/**"

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - run: pip install -r docs/specs/requirements.txt
      - run: mkdocs gh-deploy --force --config-file docs/specs/mkdocs.yml
```

> **Prerequisite**: GitHub Pages must be enabled before first deploy:
> Repo Settings → Pages → Source = **"Deploy from a branch"** → branch = `gh-pages`.
> `mkdocs gh-deploy` pushes the built site to the `gh-pages` branch automatically.

## Edge Cases

| Scenario | Expected Behavior |
|----------|-------------------|
| Spec folder has no `.pages` file | `awesome-pages` falls back to alphabetical; section title is raw folder name — not ideal but not a build failure |
| New spec added without `.pages` file | Site builds; spec appears with raw folder name; add `.pages` in same PR as spec |
| Mermaid diagram has syntax error | Mermaid.js renders an inline error; page still loads; other diagrams unaffected |
| `docs_dir: ../../specs/` path wrong | `mkdocs build` fails immediately with clear error; CI fails; no partial deploy |
| `.work-docs/` accidentally committed | Would appear in site nav; prevented by `.gitignore`; remove and re-push if it happens |
| Cross-spec internal links | MkDocs rewrites relative links; verify with `mkdocs build --strict` |
| Concurrent CI runs | `mkdocs gh-deploy --force` is atomic; last deploy wins; no corruption |
| `site_build/` accidentally committed | Prevented by `.gitignore` entry added in FR-11 |
| GitHub Pages not enabled | CI runs but deploy fails with 404; prerequisite in FR-10 must be verified first |

## Success Criteria

* \[ ] SC-1: `mkdocs build -f docs/specs/mkdocs.yml` exits 0 from repo root; produces `site_build/`
* \[ ] SC-2: All 7 existing specs appear in site navigation with human-readable titles (e.g., "001 — OTEL Setup", not "001-otel-setup")
* \[ ] SC-3: Mermaid diagrams in `001-otel-setup/spec.md` and `003-messaging-setup/spec.md` render as SVGs (verified in browser)
* \[ ] SC-4: Searching "NATS" returns results from `003-messaging-setup/spec.md` as top result
* \[ ] SC-5: Dark mode toggle works; preference persists on page reload
* \[ ] SC-6: "Edit this page" link opens the correct source file on GitHub
* \[ ] SC-7: Push to `specs/` on main → CI completes and site is updated within 2 minutes
* \[ ] SC-8: `mkdocs serve` starts without errors; mermaid renders in browser
* \[ ] SC-9: `mkdocs build --strict` exits 0 (no broken links, no warnings-as-errors)

## Docs & Links Update

* \[ ] Update `CLAUDE.md` Commands section — add: `mkdocs serve -f docs/specs/mkdocs.yml`
* \[ ] Add `site_build/` to `.gitignore`
* \[ ] Verify GitHub Pages is enabled in repo settings before first CI deploy
* \[ ] Add note to SpecKit workflow doc: "When creating a new spec folder, add a `.pages` file with the section title"

## Constitution Compliance

| Principle | Applies | Compliant | Notes |
|-----------|---------|-----------|-------|
| I. Zero-Dep CLI | \[ ] | n/a | No CLI changes |
| II. Platform-in-a-Box | \[ ] | n/a | Documentation only |
| III. Modular Services | \[ ] | n/a | No service changes |
| IV. Two-Brain | \[ ] | n/a | Config and markdown only — no custom code |
| V. Polyglot Standards | \[x] | \[x] | Python tooling (MkDocs) consistent with SDK/services; commenting conventions followed |
| VI. Local-First | \[x] | \[x] | `mkdocs serve` works fully offline after `pip install`; no external services for local preview |
| VII. Observability | \[ ] | n/a | Documentation only |
| VIII. Security | \[x] | \[x] | No secrets; CI uses automatic `GITHUB_TOKEN`; no sensitive data in built site |
| IX. Declarative | \[ ] | n/a | No CLI changes |
| X. Stateful Ops | \[ ] | n/a | No CLI changes |
| XI. Resilience | \[ ] | n/a | Static site — inherently resilient; GitHub Pages 99.9% SLA |
| XII. Interactive | \[ ] | n/a | No CLI changes |

---

---
url: /arc-platform/specs-site/001-otel-setup/plan.md
---
# Implementation Plan: 001-otel-setup — Stand Up SigNoz as the Observability Backend

> **Spec**: 001-otel-setup
> **Date**: 2026-02-21

## Summary

Set up SigNoz as the A.R.C. observability backend with two service directories under `services/otel/`: `telemetry/` (OTEL collector - Black Widow) and `observability/` (SigNoz + ClickHouse + ZooKeeper - Friday). All operations are orchestrated via a new root `Makefile` with `otel-*` targets. No direct `docker compose` usage. SigNoz UI at `localhost:3301`, OTLP endpoints at `:4317` (gRPC) and `:4318` (HTTP). Infrastructure only — SDK instrumentation is future work.

## Target Modules

| Module | Language | Changes |
|--------|----------|---------|
| services/otel/telemetry/ | config (YAML) | New directory — OTEL collector service with `service.yaml`, `docker-compose.yml`, `config/otel-collector-config.yaml` |
| services/otel/observability/ | config (YAML) | New directory — SigNoz + storage stack with `service.yaml`, `docker-compose.yml`, `config/` for ClickHouse/ZK configs |
| root (Makefile) | Makefile | New file — orchestration targets: `otel-up`, `otel-down`, `otel-health`, `otel-logs`, `otel-ps` |

## Technical Context

| Aspect | Value |
|--------|-------|
| Language(s) | YAML (config) + Makefile |
| Framework(s) | Docker Compose, SigNoz, OpenTelemetry Collector |
| Storage | ClickHouse (signals), ZooKeeper (coordination) |
| Testing | Health endpoint verification, telemetrygen integration test |
| Key Dependencies | `signoz/signoz-otel-collector`, `signoz/signoz`, `clickhouse/clickhouse-server`, `zookeeper` |
| Ports | 3301 (SigNoz UI), 4317 (OTLP gRPC), 4318 (OTLP HTTP), 13133 (collector health) |

## Architecture

### Component Interaction

```mermaid
graph TD
    subgraph External
        SDK[Future OTEL SDK clients]
        User[Developer]
        Test[telemetrygen test client]
    end

    subgraph services/otel/telemetry
        BW[signoz-otel-collector<br/>Black Widow<br/>:4317 :4318 :13133]
    end

    subgraph services/otel/observability
        ZK[ZooKeeper<br/>coordination]
        CH[ClickHouse<br/>signal store]
        SN[SigNoz<br/>query + UI<br/>:3301]
    end

    subgraph Root
        MK[Makefile<br/>otel-* targets]
    end

    User -->|make otel-up| MK
    MK -->|orchestrate| BW
    MK -->|orchestrate| SN

    SDK -.->|OTLP gRPC/HTTP| BW
    Test -->|test span| BW
    BW -->|export OTLP| CH
    ZK -->|coordinate| CH
    CH -->|signals| SN
    User -->|browse| SN

    BW -.->|health check| BW
    SN -.->|health check| SN

    style BW fill:#e1f5ff
    style SN fill:#ffe1f5
    style MK fill:#f5ffe1
```

### Startup Sequence

```mermaid
sequenceDiagram
    participant User
    participant Make as Makefile
    participant ZK as ZooKeeper
    participant CH as ClickHouse
    participant BW as OTEL Collector
    participant SN as SigNoz

    User->>Make: make otel-up
    Make->>ZK: docker compose up (observability)
    ZK-->>Make: healthy
    Make->>CH: start (depends_on ZK)
    CH-->>Make: healthy
    Make->>BW: docker compose up (telemetry)
    BW-->>Make: healthy (:13133)
    Make->>SN: start (depends_on CH)
    SN-->>Make: healthy (:3301/api/v1/health)
    Make->>User: Stack ready

    Note over User,Make: make otel-health runs next
    Make->>BW: curl :13133/
    BW-->>Make: 200 OK
    Make->>SN: curl :3301/api/v1/health
    SN-->>Make: 200 OK
    Make->>User: Health checks passed
```

### Data Flow

```mermaid
flowchart LR
    SDK[OTEL SDK] -->|OTLP/gRPC :4317| BW[Collector]
    SDK -->|OTLP/HTTP :4318| BW

    BW -->|OTLP export| CH[ClickHouse]
    CH -->|query| SN[SigNoz Query Service]
    SN -->|serve| UI[SigNoz UI :3301]

    ZK[ZooKeeper] -.->|coordinate| CH

    style BW fill:#e1f5ff
    style CH fill:#ffe1e1
    style SN fill:#ffe1f5
    style UI fill:#e1ffe1
```

## Constitution Check

| # | Principle | Status | Evidence |
|---|-----------|--------|----------|
| I | Zero-Dep CLI | N/A | CLI scope only; this is platform infrastructure |
| II | Platform-in-a-Box | **PASS** | Single `make otel-up` starts entire stack; no manual steps; health checks validate readiness |
| III | Modular Services | **PASS** | Each service dir (`telemetry/`, `observability/`) is self-contained with `service.yaml`, `docker-compose.yml`, and `config/`; root Makefile orchestrates without tight coupling |
| IV | Two-Brain | N/A | No Go/Python code; pure infrastructure config |
| V | Polyglot Standards | **PASS** | YAML config follows 12-Factor (env vars for secrets); consistent `service.yaml` metadata |
| VI | Local-First | N/A | CLI scope only |
| VII | Observability | **PASS** | This spec **IS** the observability foundation; health endpoints required (`:13133`, `:3301/api/v1/health`) |
| VIII | Security | **PASS** | Non-root containers enforced; ClickHouse/ZK NOT exposed externally; SigNoz UI bound to `127.0.0.1` only; no secrets in git |
| IX | Declarative | N/A | CLI scope only |
| X | Stateful Ops | N/A | CLI scope only |
| XI | Resilience | N/A | Chaos testing is follow-on work; basic health checks included |
| XII | Interactive | N/A | CLI scope only |

## Project Structure

```
arc-platform/
├── Makefile                                    # NEW — root orchestration with otel-* targets
├── services/
│   ├── profiles.yaml                           # UNCHANGED — future profiles may reference otel
│   └── otel/                                   # NEW — observability parent
│       ├── telemetry/                          # NEW — Black Widow (OTEL collector)
│       │   ├── service.yaml                    # Codename: widow, tech: signoz-otel-collector
│       │   ├── docker-compose.yml              # Single service: signoz-otel-collector
│       │   ├── config/
│       │   │   └── otel-collector-config.yaml  # OTLP receivers, SigNoz exporter, health ext
│       │   └── README.md                       # Ports, health endpoint, config reference
│       └── observability/                      # NEW — Friday (SigNoz + storage)
│           ├── service.yaml                    # Codename: friday, tech: signoz
│           ├── docker-compose.yml              # Services: signoz, clickhouse, zookeeper
│           ├── config/
│           │   ├── clickhouse-config.xml       # ClickHouse settings (4GB RAM minimum)
│           │   └── zookeeper.properties        # ZK coordination config
│           └── README.md                       # Prerequisites (4GB RAM), quickstart, UI URL
└── specs/001-otel-setup/
    ├── spec.md                                 # Input
    ├── plan.md                                 # This file
    └── tasks.md                                # Generated next
```

## Parallel Execution Strategy

```mermaid
gantt
    title OTEL Setup Implementation Phases
    dateFormat YYYY-MM-DD
    section Phase 1 - Scaffolding (Sequential)
    Create directory structure       :s1, 2026-02-21, 1h
    Write service.yaml files          :s2, after s1, 30m
    section Phase 2 - Service Config (Parallel)
    Telemetry compose + config        :p1, after s2, 2h
    Observability compose + config    :p2, after s2, 2h
    section Phase 3 - Orchestration (Sequential)
    Root Makefile targets             :s3, after p1, 1h
    section Phase 4 - Validation (Parallel)
    Health check implementation       :p3, after s3, 1h
    Integration test setup            :p4, after s3, 1h
    section Phase 5 - Documentation (Parallel)
    Telemetry README                  :p5, after p3, 30m
    Observability README              :p6, after p3, 30m
```

**Parallelizable Task Groups**:

* **Group A** (after scaffolding): Telemetry service config (`TASK-2`) + Observability service config (`TASK-3`)
* **Group B** (after Makefile): Health check targets (`TASK-5`) + Integration test script (`TASK-6`)
* **Group C** (after validation): READMEs (`TASK-7`, `TASK-8`)

**Sequential Dependencies**:

1. Scaffolding (`TASK-1`) must complete before config tasks
2. Config tasks must complete before Makefile (`TASK-4`)
3. Makefile must complete before validation tasks

## Reviewer Checklist

**Post-Implementation Verification**:

* \[ ] All tasks in tasks.md marked completed
* \[ ] Directory structure matches plan (services/otel/telemetry/, services/otel/observability/)
* \[ ] `make otel-up` from repo root exits 0 and starts all 4 containers
* \[ ] `make otel-health` exits 0 after stack is running
* \[ ] `curl -sf http://localhost:3301/api/v1/health` returns 200
* \[ ] `curl -sf http://localhost:13133/` returns 200
* \[ ] `nc -z localhost 9000` fails (ClickHouse NOT externally exposed)
* \[ ] `nc -z localhost 2181` fails (ZooKeeper NOT externally exposed)
* \[ ] Test span via telemetrygen appears in SigNoz UI within 30 seconds
* \[ ] All containers run as non-root (verify with `docker compose exec <service> whoami`)
* \[ ] No secrets or credentials committed to git
* \[ ] SigNoz UI bound to 127.0.0.1 (verify in docker-compose.yml)
* \[ ] `make otel-down` cleanly removes all containers
* \[ ] READMEs exist with quickstart instructions
* \[ ] Constitution compliance verified for Principles II, III, V, VII, VIII
* \[ ] No TODO/FIXME comments without tracking issue

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| ClickHouse OOMs on dev machines with < 4GB available | **HIGH** | Document 4GB minimum RAM in observability/README.md; health check will fail fast with clear error in logs |
| Port conflicts (3301, 4317, 4318) on developer machines | **MEDIUM** | Make bind errors explicit; document required ports in README; future: make ports configurable via env vars |
| SigNoz starts before ClickHouse is fully ready | **MEDIUM** | Use `depends_on` with `condition: service_healthy` in docker-compose.yml; ClickHouse health check verifies readiness |
| Collector buffer overflow during ClickHouse startup | **LOW** | Collector queue settings allow buffering ~60s of data; startup time < 30s per NFR-2 |
| No existing Makefile → new patterns to establish | **LOW** | Follow platform-spike conventions: compose variable references, no `cd` commands, explicit target naming |
| Developers bypass Makefile and use raw `docker compose` | **LOW** | Document Makefile as single entrypoint; make targets simpler than raw compose commands |

## Implementation Notes

### Makefile Pattern

Follow platform-spike convention:

* Define compose variables at top: `COMPOSE_OTEL_TELEMETRY := $(COMPOSE) -f services/otel/telemetry/docker-compose.yml`
* Combine for full stack: `COMPOSE_OTEL := $(COMPOSE_OTEL_TELEMETRY) -f services/otel/observability/docker-compose.yml`
* No `cd` commands — all paths relative to repo root
* Explicit target names: `otel-up`, not just `up`

### Health Check Strategy

Two-tier validation:

1. **Container-level**: `healthcheck` blocks in docker-compose.yml prevent premature dependent starts
2. **Makefile-level**: `otel-health` target curls both endpoints and exits non-zero on failure

### Security Defaults

* SigNoz UI: `ports: ["127.0.0.1:3301:3301"]` (NOT `"3301:3301"`)
* ClickHouse: No `ports` key (internal network only)
* ZooKeeper: No `ports` key (internal network only)
* OTEL collector: `ports: ["127.0.0.1:4317:4317", "127.0.0.1:4318:4318"]`

### OTEL Collector Config

Single config file at `services/otel/telemetry/config/otel-collector-config.yaml`:

* **Receivers**: `otlp` (gRPC `:4317`, HTTP `:4318`)
* **Exporters**: `otlp` (ClickHouse endpoint)
* **Extensions**: `health_check` (`:13133`)
* **Service pipelines**: traces → otlp exporter

### Testing Approach

Manual verification for this spec (automated testing is follow-on work):

1. `make otel-up` → verify all containers healthy
2. `make otel-health` → verify health endpoints respond
3. Send test span: `docker run --rm --network host ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:latest traces --otlp-insecure --otlp-endpoint localhost:4317`
4. Verify span appears in SigNoz UI → Traces within 30s
5. `make otel-down` → verify clean shutdown

***

**Next Step**: Generate `tasks.md` via `/speckit.tasks`

---

---
url: /arc-platform/specs-site/002-cortex-setup/plan.md
---
# Implementation Plan: Cortex Bootstrap Service

> **Spec**: 002-cortex-setup
> **Date**: 2026-02-22

## Summary

Cortex is a new Go service (`services/cortex/`) that provisions all A.R.C. platform infrastructure at startup. It exposes a Gin HTTP API (`POST /api/v1/bootstrap`, `/health`, `/health/deep`, `/ready`) and a Cobra CLI (`cortex server` / `cortex bootstrap`). All four bootstrap phases (Postgres schema validation, NATS JetStream stream creation, Pulsar tenant/topic provisioning, Redis ping) run in parallel via `errgroup`, each wrapped with a circuit breaker and exponential-backoff retry. OTEL traces and metrics are exported to `arc-friday-collector` at `arc-widow:4317`.

***

## Target Modules

| Module | Language | Changes |
|--------|----------|---------|
| `services/cortex/` | Go | New service — full implementation |
| `services/profiles.yaml` | YAML | Add `cortex` to `think` and `reason` profiles |
| `.specify/config.yaml` | YAML | Update bootstrap entry: `dir: cortex`, `codename: cortex` |
| `.specify/meta/service-codename-map.md` | Markdown | Add Cortex row |
| `.specify/docs/architecture/cortex.md` | Markdown | Architecture doc linked to implementation |

***

## Technical Context

| Aspect | Value |
|--------|-------|
| Language | Go |
| HTTP framework | Gin (`github.com/gin-gonic/gin`) |
| CLI framework | Cobra (`github.com/spf13/cobra`) |
| Config | Viper (`github.com/spf13/viper`) — precedence: flags → env → YAML → defaults |
| NATS client | `github.com/nats-io/nats.go` + JetStream API |
| Pulsar provisioning | REST admin API via `net/http` to `arc-streaming:8080` (not binary protocol) |
| Postgres client | `github.com/jackc/pgx/v5` pool |
| Redis client | `github.com/go-redis/redis/v9` |
| OTEL | `go.opentelemetry.io/otel` — gRPC export to `arc-widow:4317` |
| Concurrency | `golang.org/x/sync/errgroup` for parallel phases |
| Circuit breaker | `github.com/sony/gobreaker` — 3 failures → open, 30s reset |
| Testing | `go test ./...` with table-driven tests; `testify` for assertions |
| Linting | `golangci-lint run` |

### Dependency Notes

* **Pulsar admin**: Provisioning (tenant, namespace, topic creation) uses the REST admin API at `:8080`, not the binary protocol at `:6650`. The binary service URL is stored in config for future producer/consumer use but unused in this feature.
* **Circuit breaker per client**: Each of the four infra clients gets its own `gobreaker.CircuitBreaker` instance — failure state is isolated so one flapping dependency doesn't open the breaker for others.
* **Bootstrap idempotency**: NATS stream `CreateStream` updates config if the stream already exists (no error). Pulsar topic creation is skipped silently if topic exists — implemented via a 409-check on the admin API response.

***

## Architecture

### Component Overview

```mermaid
graph TD
    subgraph Triggers
        CLI[cortex bootstrap\nCobra CLI]
        HTTP[POST /api/v1/bootstrap\nGin API]
    end

    subgraph Cortex["services/cortex/"]
        Cobra[cmd/cortex/bootstrap.go]
        Gin[cmd/cortex/server.go]
        Handler[internal/api/handlers.go]
        Orchestrator[internal/orchestrator/service.go\nRunBootstrap · RunDeepHealth]
        Config[internal/config/config.go\nViper]
        Telemetry[internal/telemetry/friday.go\nOTEL Provider]

        subgraph Clients["internal/clients/"]
            NATS[nats.go\n+ circuit breaker]
            Pulsar[pulsar.go\n+ circuit breaker]
            PG[postgres.go\n+ circuit breaker]
            Redis[redis.go\n+ circuit breaker]
        end
    end

    subgraph Infra["Infrastructure"]
        Flash[(arc-messaging\nNATS :4222)]
        Strange[(arc-streaming\nPulsar :8080 / :6650)]
        Oracle[(arc-sql-db\nPG :5432)]
        Sonic[(arc-cache\nRedis :6379)]
    end

    subgraph Observability["Observability"]
        Widow[arc-friday-collector\n:4317 gRPC]
        Friday[arc-friday\nSigNoz]
    end

    CLI --> Cobra --> Orchestrator
    HTTP --> Gin --> Handler --> Orchestrator
    Orchestrator --> Config
    Orchestrator --> Telemetry
    Orchestrator --> NATS --> Flash
    Orchestrator --> Pulsar --> Strange
    Orchestrator --> PG --> Oracle
    Orchestrator --> Redis --> Sonic
    Telemetry -.->|OTLP gRPC| Widow -.-> Friday

    classDef core fill:#2b2d42,stroke:#8d99ae,color:#fff
    classDef infra fill:#ef233c,stroke:#2b2d42,color:#fff
    classDef obs fill:#6d6875,stroke:#2b2d42,color:#fff
    class Orchestrator,Config,Telemetry core
    class Flash,Strange,Oracle,Sonic infra
    class Widow,Friday obs
```

### Bootstrap Phase Sequence (Parallel Execution)

```mermaid
sequenceDiagram
    participant T as Trigger (HTTP/CLI)
    participant O as Orchestrator
    participant PG as arc-sql-db
    participant N as arc-messaging (NATS)
    participant P as arc-streaming (Pulsar)
    participant R as arc-cache (Redis)
    participant W as arc-widow (OTEL)

    T->>O: RunBootstrap(ctx)
    O->>W: Start span "cortex.bootstrap"

    par Phase 1 — Postgres
        O->>PG: ping + schema validate
        PG-->>O: ok
    and Phase 2 — NATS
        O->>N: connect + create 3 streams
        N-->>O: streams ready
    and Phase 3 — Pulsar
        O->>P: create tenant + 3 namespaces + 3 topics
        P-->>O: provisioned
    and Phase 4 — Redis
        O->>R: PING
        R-->>O: PONG
    end

    O->>W: End span (success/error + phase durations)
    O-->>T: BootstrapResult{phases: {pg:ok, nats:ok, pulsar:ok, redis:ok}}
```

### Retry + Circuit Breaker Flow (per client)

```mermaid
stateDiagram-v2
    [*] --> Closed
    Closed --> Open: 3 consecutive failures
    Open --> HalfOpen: 30s elapsed
    HalfOpen --> Closed: probe succeeds
    HalfOpen --> Open: probe fails

    state Closed {
        [*] --> Attempt
        Attempt --> Success: ok
        Attempt --> Backoff: error
        Backoff --> Attempt: 2s → 4s → 8s … max 30s\n(timeout 5m per phase)
    }
```

### Health Endpoint Behaviour

```mermaid
stateDiagram-v2
    [*] --> Starting
    Starting --> Ready: RunBootstrap() completes
    Starting --> Ready: bootstrap fails (non-fatal)

    state Starting {
        GET_health: /health → 200 healthy (shallow)
        GET_ready: /ready → 503 not-ready
        POST_bootstrap: POST /api/v1/bootstrap → 409 if already running
    }
    state Ready {
        GET_health2: /health → 200 healthy
        GET_ready2: /ready → 200 ok
        GET_deep: /health/deep → 200/503 per dep probe
        Monitor: background goroutine probes every 30s
    }
```

***

## Constitution Check

| # | Principle | Status | Evidence |
|---|-----------|--------|----------|
| I | Zero-Dep CLI | N/A | Service, not CLI binary |
| II | Platform-in-a-Box | **PASS** | `docker compose up cortex` bootstraps all infra automatically |
| III | Modular Services | **PASS** | Self-contained in `services/cortex/`; `service.yaml` declares all deps |
| IV | Two-Brain | **PASS** | Pure Go — infrastructure orchestration only; no Python |
| V | Polyglot Standards | **PASS** | golangci-lint, `slog`, table-driven tests, 12-factor config via Viper |
| VI | Local-First | N/A | Service, not CLI |
| VII | Observability | **PASS** | OTEL traces + metrics from day one; `/health` + `/health/deep` |
| VIII | Security | **PASS** | Non-root `USER cortex` in Dockerfile; no secrets in logs |
| IX | Declarative | N/A | Not a reconciler |
| X | Stateful Ops | N/A | Not a CLI |
| XI | Resilience | **PASS** | Circuit breakers + exponential backoff on all four infra clients |
| XII | Interactive | N/A | Not a CLI TUI |

***

## Project Structure

```
arc-platform/
├── services/
│   ├── cortex/
│   │   ├── cmd/
│   │   │   └── cortex/
│   │   │       ├── main.go             # Wire AppContext, start actor group
│   │   │       ├── root.go             # Cobra root + global flags (--config, --log-level)
│   │   │       ├── server.go           # `cortex server` — start Gin API on :8081
│   │   │       └── bootstrap.go        # `cortex bootstrap` — one-shot CLI; exit 0/1
│   │   ├── internal/
│   │   │   ├── api/
│   │   │   │   ├── server.go           # Gin router wiring + graceful shutdown
│   │   │   │   ├── handlers.go         # /bootstrap, /health, /health/deep, /ready handlers
│   │   │   │   └── middleware.go       # OTEL trace propagation + panic recovery
│   │   │   ├── orchestrator/
│   │   │   │   ├── service.go          # RunBootstrap(), RunDeepHealth() — errgroup parallel phases
│   │   │   │   └── types.go            # BootstrapResult, PhaseResult, ProbeResult
│   │   │   ├── clients/
│   │   │   │   ├── nats.go             # JetStream stream creation + gobreaker
│   │   │   │   ├── pulsar.go           # Pulsar admin REST + gobreaker
│   │   │   │   ├── postgres.go         # pgx pool ping + schema check + gobreaker
│   │   │   │   └── redis.go            # go-redis PING + gobreaker
│   │   │   ├── config/
│   │   │   │   └── config.go           # Root Viper config; typed structs per section
│   │   │   └── telemetry/
│   │   │       └── friday.go           # OTEL TracerProvider + MeterProvider → arc-widow:4317
│   │   ├── service.yaml                # codename: cortex; deps; health endpoint
│   │   ├── Dockerfile                  # Non-root USER cortex; multi-stage build
│   │   ├── go.mod
│   │   └── go.sum
│   └── profiles.yaml                   # Add cortex to think + reason profiles
├── .specify/
│   ├── config.yaml                     # Update bootstrap → cortex entry
│   ├── meta/
│   │   └── service-codename-map.md     # Add Cortex row
│   └── docs/
│       └── architecture/
│           └── cortex.md               # Architecture doc
```

***

## Parallel Execution Strategy

```mermaid
gantt
    title Cortex Implementation Phases
    dateFormat  YYYY-MM-DD
    axisFormat  Day %d

    section Phase 1 — Foundation (sequential, one agent)
    go.mod + module scaffold           :p1a, 2026-02-22, 1d
    Config (Viper structs + env map)   :p1b, after p1a, 1d
    Telemetry (OTEL provider friday.go):p1c, after p1b, 1d
    Cobra root + server/bootstrap cmds :p1d, after p1c, 1d

    section Phase 2 — Parallel (4 agents)
    Client: NATS (nats.go + tests)     :p2a, after p1d, 2d
    Client: Pulsar (pulsar.go + tests) :p2b, after p1d, 2d
    Client: Postgres (postgres.go + tests) :p2c, after p1d, 2d
    Client: Redis (redis.go + tests)   :p2d, after p1d, 1d

    section Phase 3 — Integration (sequential)
    Orchestrator (service.go + types.go):p3a, after p2a, 2d
    API layer (server/handlers/middleware):p3b, after p3a, 1d
    Dockerfile + service.yaml          :p3c, after p3b, 1d

    section Phase 4 — Wiring & Validation
    main.go DI wiring                  :p4a, after p3c, 1d
    Integration + health endpoint tests :p4b, after p4a, 1d
    Docs + config.yaml + profiles.yaml :p4c, after p3c, 1d
```

### Parallelisation Notes

| Group | Tasks | Parallel? | Dependency |
|-------|-------|-----------|------------|
| A | 4 infra clients (nats, pulsar, postgres, redis) | **YES** — independent packages | Phase 1 complete |
| B | Orchestrator + API layer | **NO** — orchestrator uses all 4 clients | Group A complete |
| C | Docs/config updates | **YES** — no code dependency | Phase 3 started |
| D | main.go wiring | **NO** — needs orchestrator + API | Group B complete |

***

## Key Design Decisions

### 1. Phase parallelism via `errgroup`

```go
// internal/orchestrator/service.go
g, ctx := errgroup.WithContext(ctx)
g.Go(func() error { return s.runPostgresPhase(ctx, result) })
g.Go(func() error { return s.runNATSPhase(ctx, result) })
g.Go(func() error { return s.runPulsarPhase(ctx, result) })
g.Go(func() error { return s.runRedisPhase(ctx, result) })
err = g.Wait()
```

Each phase writes to `result` under a `sync.Mutex`. Phase-level error does not cancel other phases — `errgroup` collects all errors.

### 2. Circuit breaker per client

Each client embeds a `gobreaker.CircuitBreaker` configured as:

* `MaxRequests`: 1 (probe in half-open)
* `Interval`: 0 (count window = open duration)
* `Timeout`: 30s (open → half-open)
* `ReadyToTrip`: trips after 3 consecutive failures

### 3. `POST /api/v1/bootstrap` concurrency guard

An `atomic.Bool` (`bootstrapInProgress`) gates concurrent bootstrap requests — returns `409` immediately if a bootstrap is already running.

### 4. Pulsar admin via REST, not binary client

Tenant/namespace/topic creation uses `http.Post` to `PULSAR_ADMIN_URL`. A 409 response on topic creation is treated as success (idempotent). The binary `PULSAR_SERVICE_URL` is stored in config for future use.

### 5. `/ready` lifecycle

`/ready` returns `503` until `bootstrapInProgress` flips false AND `bootstrapCompleted` is true. This enables Traefik to withhold traffic until infra is fully provisioned.

***

## Reviewer Checklist

* \[ ] All tasks in `tasks.md` marked completed
* \[ ] `go test ./...` passes with ≥ 75% coverage on `internal/orchestrator` and `internal/clients`
* \[ ] `go test ./...` passes with ≥ 60% coverage on `internal/api`
* \[ ] `golangci-lint run` produces zero errors
* \[ ] `GET /health` responds in < 50ms (manual curl check)
* \[ ] `docker inspect` confirms non-root user (`cortex`)
* \[ ] Constitution check: all REQUIRED principles verified (II, III, IV, V, VII, VIII, XI)
* \[ ] All four NATS streams created with correct config (subjects, retention, max-age)
* \[ ] All Pulsar resources provisioned (tenant `arc-system`, 3 namespaces, 3 partitioned topics)
* \[ ] `POST /api/v1/bootstrap` returns `409` when called while bootstrap in progress
* \[ ] `cortex bootstrap` CLI exits 0 on success, non-zero on failure
* \[ ] OTEL traces visible in arc-friday under service name `arc-cortex`
* \[ ] `services/profiles.yaml` includes `cortex` in `think` and `reason`
* \[ ] `service.yaml` declares correct `depends_on: [flash, strange, oracle, sonic, widow]`
* \[ ] `.specify/config.yaml` updated from `bootstrap/raymond` → `cortex/cortex`
* \[ ] No secrets, passwords, or credentials appear in logs or error responses

***

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Pulsar REST admin API has different error shapes across versions | M | Defensive JSON parsing; log raw response on unexpected status |
| `errgroup` phase error masks partial success | M | `BootstrapResult` stores per-phase status independently; errgroup collects all, doesn't cancel on first error |
| OTEL gRPC dial failure blocks startup | H | Use `WithFailFast(false)` + async export; bootstrap proceeds regardless of OTEL availability |
| pgx pool exhausted under concurrent `/health/deep` calls | L | Pool max connections capped; `/health/deep` uses single probe connection with short timeout |
| `service.yaml` `depends_on` not yet enforced by arc CLI | L | Document in README; CLI enforcement is out-of-scope for this feature |

---

---
url: /arc-platform/specs-site/005-data-layer/plan.md
---
# Implementation Plan: Data Layer Services Setup

> **Spec**: 005-data-layer
> **Date**: 2026-02-28

## Summary

Add three infrastructure services — Postgres 17 (Oracle), Qdrant (Cerebro), MinIO (Tardis) — each following the exact same structural pattern as 003-messaging-setup: thin Dockerfile label wrapper, `service.yaml`, `docker-compose.yml`, dedicated `.mk` include, and CI/release workflows. All three join `arc_platform_net` (existing external bridge). No OTEL collector changes in this spec — Prometheus scraping for Qdrant and MinIO is deferred as tech debt. Profiles updated so `think` includes oracle + cerebro; `reason` adds tardis.

## Target Modules

| Module | Language | Changes |
|--------|----------|---------|
| `services/persistence/` | Config/Dockerfile | New — Postgres 17 (Oracle) |
| `services/vector/` | Config/Dockerfile | New — Qdrant (Cerebro) |
| `services/storage/` | Config/Dockerfile | New — MinIO (Tardis) |
| `services/profiles.yaml` | YAML | Add oracle + cerebro to `think`; tardis to `reason` |
| `.github/workflows/` | YAML | New — data-images.yml + data-release.yml |
| `Makefile` | Make | Include oracle.mk, cerebro.mk, tardis.mk, data.mk |
| `services/data.mk` | Make | New — aggregate up/down/health/logs for all three |

## Technical Context

| Aspect | Value |
|--------|-------|
| Language | Config-only (YAML, Dockerfile) — no application code |
| Base Images | `postgres:17-alpine`, `qdrant/qdrant`, `minio/minio` |
| Testing | `docker compose ps`, `pg_isready`, `curl /readyz`, `curl /minio/health/live` |
| Network | `arc_platform_net` (external bridge, pre-existing) — no new networks |
| Volumes | Named volumes: `arc-sql-db-data`, `arc-vector-db-data`, `arc-storage-data` |
| CI Pattern | Mirror `messaging-images.yml` — amd64-only CI, dorny/paths-filter per service |
| Health Checks | Oracle: `pg_isready`; Cerebro: HTTP `/readyz`; Tardis: HTTP `/minio/health/live` |

## Architecture

### Service file pattern (same for all three)

```mermaid
graph LR
    A[Dockerfile\npins upstream image\nadds OCI + arc labels\nnon-root user] --> B[docker-compose.yml\nports 127.0.0.1 only\nnamed volumes\narc_platform_net external]
    B --> C[service.yaml\nname, codename, image\nports, health, depends_on]
    C --> D[oracle.mk / cerebro.mk / tardis.mk\nup · down · health · logs\nbuild · push · publish · tag\nclean · nuke · help]
```

### Runtime topology

```mermaid
graph TB
    subgraph Host
        subgraph arc_platform_net [arc_platform_net — external bridge, pre-existing]
            oracle["arc-sql-db\npostgres:17-alpine\n:5432"]
            cerebro["arc-vector-db\nqdrant/qdrant\n:6333 REST · :6334 gRPC"]
            tardis["arc-storage\nminio/minio\n:9000 S3 · :9001 console"]
            cortex["arc-cortex\n:8081"]
            flash["arc-messaging\n:4222"]
            sonic["arc-cache\n:6379"]
        end
    end

    cortex -- "sql bootstrap\npg wire :5432" --> oracle
    mystique_future["arc-flags (future)"] -. "pg :5432" .-> oracle
    sherlock_future["arc-sherlock (future)"] -. "vector :6333" .-> cerebro
    scarlett_future["arc-scarlett (future)"] -. "s3 :9000" .-> tardis
```

### Volume layout

```mermaid
graph LR
    oracle_c["arc-sql-db\ncontainer"] --- oracle_v["arc-sql-db-data\n/var/lib/postgresql/data"]
    cerebro_c["arc-vector-db\ncontainer"] --- cerebro_v["arc-vector-db-data\n/qdrant/storage"]
    tardis_c["arc-storage\ncontainer"] --- tardis_v["arc-storage-data\n/data"]
```

## Constitution Check

| # | Principle | Status | Evidence |
|---|-----------|--------|----------|
| I | Zero-Dep CLI | N/A | No CLI changes |
| II | Platform-in-a-Box | PASS | `make data-up` boots all three; oracle + cerebro join `think` profile |
| III | Modular Services | PASS | Each in own dir under `services/`; self-contained with own service.yaml |
| IV | Two-Brain | PASS | Config-only upstream images — no language concern |
| V | Polyglot Standards | PASS | Same Dockerfile/compose/healthcheck structure as 003-messaging-setup |
| VI | Local-First | N/A | CLI only |
| VII | Observability | PASS | pg\_isready, Qdrant /readyz, MinIO /health/live; Prometheus scraping deferred (TD-001) |
| VIII | Security | PASS | postgres uid 70, qdrant uid 1000; MinIO uid verified (see Decision 3); 127.0.0.1 ports; no secrets in git |
| IX | Declarative | N/A | CLI only |
| X | Stateful Ops | N/A | CLI only |
| XI | Resilience | PASS | Health checks with appropriate start\_periods; named volumes survive restart |
| XII | Interactive | N/A | CLI only |

## Project Structure

```
arc-platform/
├── services/
│   ├── persistence/                          ← NEW (Oracle / Postgres 17)
│   │   ├── Dockerfile                        # FROM postgres:17-alpine; OCI + arc labels; uid 70 (postgres)
│   │   ├── service.yaml                      # name, codename, image, ports, health
│   │   ├── docker-compose.yml                # arc-sql-db; POSTGRES_* env; named volume; arc_platform_net
│   │   └── oracle.mk                         # oracle-up/down/health/logs/build/push/publish/tag/clean/nuke
│   ├── vector/                               ← NEW (Cerebro / Qdrant)
│   │   ├── Dockerfile                        # FROM qdrant/qdrant; OCI + arc labels; uid 1000
│   │   ├── service.yaml
│   │   ├── docker-compose.yml                # arc-vector-db; :6333/:6334; named volume; arc_platform_net
│   │   └── cerebro.mk
│   ├── storage/                              ← NEW (Tardis / MinIO)
│   │   ├── Dockerfile                        # FROM minio/minio; OCI + arc labels; uid verified
│   │   ├── service.yaml
│   │   ├── docker-compose.yml                # arc-storage; :9000/:9001; MINIO_ROOT_*; named volume; arc_platform_net
│   │   └── tardis.mk
│   ├── profiles.yaml                         # MODIFY — think += oracle, cerebro; reason += tardis
│   └── data.mk                               ← NEW (aggregate)
├── .github/workflows/
│   ├── data-images.yml                       ← NEW
│   └── data-release.yml                      ← NEW
└── Makefile                                  # MODIFY — include oracle.mk, cerebro.mk, tardis.mk, data.mk
```

## Key Implementation Decisions

### 1. Dockerfiles are thin label wrappers (same as 003)

No config baked in. All service configuration is in `docker-compose.yml` via command-line flags or environment variables. The Dockerfile only pins the upstream image and adds labels.

```dockerfile
# services/persistence/Dockerfile
FROM postgres:17-alpine
LABEL org.opencontainers.image.title="ARC Oracle — Persistence"
LABEL org.opencontainers.image.description="Postgres 17 relational database for the A.R.C. Platform"
LABEL org.opencontainers.image.source="https://github.com/arc-framework/arc-platform"
LABEL arc.service.name="arc-sql-db"
LABEL arc.service.codename="oracle"
LABEL arc.service.tech="postgres"
# postgres:17-alpine already runs as postgres user (uid 70) — no USER directive needed
```

```dockerfile
# services/vector/Dockerfile
FROM qdrant/qdrant
LABEL org.opencontainers.image.title="ARC Cerebro — Vector Store"
LABEL arc.service.codename="cerebro"
LABEL arc.service.tech="qdrant"
# qdrant/qdrant runs as uid 1000 — non-root by default
```

```dockerfile
# services/storage/Dockerfile
FROM minio/minio
LABEL org.opencontainers.image.title="ARC Tardis — Object Storage"
LABEL arc.service.codename="tardis"
LABEL arc.service.tech="minio"
# minio/minio: verify uid at build time — document if root deviation applies
```

### 2. arc\_platform\_net as external network (same as 003)

Each `docker-compose.yml` declares the network as external. The network is created once by `make dev` or `make flash-up`. `data.mk`'s `data-up` target ensures it exists with `|| true`.

```yaml
# Bottom of every docker-compose.yml in this spec
networks:
  arc_platform_net:
    external: true
    name: arc_platform_net
```

### 3. MinIO non-root handling

`minio/minio` standard image runs as root by default. Options in order of preference:

* **Option A**: Use `user: "1000:1000"` in docker-compose and ensure `/data` is writable — verify MinIO supports this without a custom Dockerfile
* **Option B**: Add `RUN mkdir -p /data && chown 1000:1000 /data && adduser -u 1000 minio` in Dockerfile and `USER 1000`

If neither works with the upstream image, document the root deviation in docker-compose.yml comments (same as Pulsar in 003).

### 4. Postgres docker-compose pattern

```yaml
services:
  arc-sql-db:
    build: {context: ., dockerfile: Dockerfile}
    image: ghcr.io/arc-framework/arc-sql-db:latest
    container_name: arc-sql-db
    environment:
      POSTGRES_USER: arc
      POSTGRES_PASSWORD: arc
      POSTGRES_DB: arc
    ports:
      - "127.0.0.1:5432:5432"
    volumes:
      - oracle-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U arc || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s
    networks:
      - arc_platform_net
    restart: unless-stopped

volumes:
  oracle-data:
    name: arc-sql-db-data

networks:
  arc_platform_net:
    external: true
    name: arc_platform_net
```

### 5. Qdrant docker-compose pattern

```yaml
services:
  arc-vector-db:
    image: ghcr.io/arc-framework/arc-vector-db:latest
    container_name: arc-vector-db
    ports:
      - "127.0.0.1:6333:6333"   # REST + Prometheus metrics
      - "127.0.0.1:6334:6334"   # gRPC
    volumes:
      - cerebro-data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:6333/readyz || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s
    networks:
      - arc_platform_net
    restart: unless-stopped
```

### 6. MinIO docker-compose pattern

```yaml
services:
  arc-storage:
    image: ghcr.io/arc-framework/arc-storage:latest
    container_name: arc-storage
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: arc
      MINIO_ROOT_PASSWORD: arc-minio-dev
    ports:
      - "127.0.0.1:9000:9000"   # S3 API
      - "127.0.0.1:9001:9001"   # Web console
    volumes:
      - tardis-data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/minio/health/live || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - arc_platform_net
    restart: unless-stopped
```

### 7. data.mk aggregate (mirrors messaging.mk)

```makefile
PHONY: data-help data-up data-down data-health data-logs

## data-up: Start all data services (Oracle + Cerebro + Tardis)
data-up:
	@docker network create arc_platform_net 2>/dev/null || true
	$(MAKE) oracle-up --no-print-directory
	$(MAKE) cerebro-up --no-print-directory
	$(MAKE) tardis-up --no-print-directory

## data-down: Stop all data services
data-down:
	$(MAKE) oracle-down --no-print-directory
	$(MAKE) cerebro-down --no-print-directory
	$(MAKE) tardis-down --no-print-directory

## data-health: Check health of all three data services
data-health:
	@$(MAKE) oracle-health --no-print-directory && \
	 $(MAKE) cerebro-health --no-print-directory && \
	 $(MAKE) tardis-health --no-print-directory
```

### 8. CI workflow — mirrors messaging-images.yml exactly

`data-images.yml` uses `_reusable-build.yml` and `_reusable-security.yml` with path filters:

* `services/persistence/**` → build-oracle
* `services/vector/**` → build-cerebro
* `services/storage/**` → build-tardis

`data-release.yml` tag format: `data/vX.Y.Z` → Docker tag `data-vX.Y.Z`

## Parallel Execution Strategy

```mermaid
gantt
    title 005-data-layer Implementation Phases
    dateFormat X
    axisFormat %s

    section Phase 1 — Foundation
    Update profiles.yaml (oracle+cerebro→think, tardis→reason)  :p1, 0, 1

    section Phase 2 — Services (fully parallel)
    persistence/: Dockerfile + service.yaml + compose            :p2a, after p1, 2
    vector/: Dockerfile + service.yaml + compose                 :p2b, after p1, 2
    storage/: Dockerfile + service.yaml + compose                :p2c, after p1, 2

    section Phase 3 — Make targets (parallel per service)
    oracle.mk                                                     :p3a, after p2a, 1
    cerebro.mk                                                    :p3b, after p2b, 1
    tardis.mk                                                     :p3c, after p2c, 1
    data.mk aggregate + Makefile includes                         :p3d, after p2c, 1

    section Phase 4 — CI/CD (parallel)
    data-images.yml workflow                                      :p4a, after p3d, 2
    data-release.yml workflow                                     :p4b, after p3d, 2

    section Phase 5 — Integration
    make data-up + make data-health                               :p5, after p4b, 1
    Cortex /health/deep shows oracle ok                           :p5b, after p5, 1
```

**Parallelizable task groups:**

* Phase 2: All three service directories are fully independent
* Phase 3: oracle.mk + cerebro.mk + tardis.mk — independent after their Phase 2 deps
* Phase 4: data-images.yml + data-release.yml — independent of each other

## Tech Debt

| ID | Item | Rationale |
|----|------|-----------|
| TD-001 | Qdrant Prometheus scraping via otel collector | Qdrant exposes `/metrics` on :6333. When a concrete dashboard requirement exists, add a `prometheus` scrape job for `arc-vector-db:6333` to the collector config (see 003 pattern). |
| TD-002 | MinIO Prometheus scraping | MinIO exports metrics at `:9000/minio/v2/metrics/cluster`. Same deferral as TD-001. |
| TD-003 | Default bucket creation | MinIO starts empty. Cortex bootstrap (or a future init task) should create standard buckets (`arc-artifacts`, `arc-models`, etc.) via `mc` client. |

## Reviewer Checklist

* \[ ] `make data-up` exits 0; all three containers in `healthy` state (`docker compose ps`)
* \[ ] `make data-health` exits 0
* \[ ] `make data-down` stops all three; no orphaned containers
* \[ ] `make oracle-up && make oracle-health` works independently
* \[ ] `make cerebro-up && make cerebro-health` works independently
* \[ ] `make tardis-up && make tardis-health` works independently
* \[ ] `curl -s http://localhost:8081/health/deep | jq .oracle.status` returns `"ok"` after cortex restart
* \[ ] `curl -s http://localhost:6333/readyz` returns HTTP 200
* \[ ] `curl -s http://localhost:9000/minio/health/live` returns HTTP 200
* \[ ] `docker inspect arc-sql-db | jq '.[0].Config.User'` confirms non-root (empty = postgres default uid 70)
* \[ ] `docker inspect arc-vector-db | jq '.[0].Config.User'` confirms non-root (uid 1000)
* \[ ] MinIO uid documented (non-root or deviation noted in compose comments)
* \[ ] All ports bind `127.0.0.1` — verify with `docker compose ps`
* \[ ] All volumes are named (not bind mounts) — `docker volume ls | grep arc`
* \[ ] `services/profiles.yaml` `think` includes `oracle` + `cerebro`; `reason` includes `tardis`
* \[ ] `Makefile` includes oracle.mk, cerebro.mk, tardis.mk, data.mk
* \[ ] `data-images.yml` runs only on relevant path changes; path filters cover all three service dirs
* \[ ] `data-release.yml` tag format `data/v*` builds multi-platform images (amd64 + arm64)
* \[ ] All Dockerfiles have OCI + `arc.service.*` labels
* \[ ] No credentials or secrets in any compose file (use env vars with dev defaults)
* \[ ] `make oracle-clean` and `make oracle-nuke` prompt for confirmation before destructive action

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| MinIO runs as root in upstream image | M | Verify at implementation time; either `user: "1000:1000"` in compose or document deviation in comments (like Pulsar) |
| `arc_platform_net` not created before `make data-up` | H | `data-up` calls `docker network create arc_platform_net 2>/dev/null \|\| true` first |
| Postgres ignores env vars after data dir is initialized | M | Document in `make oracle-clean` help text; `oracle-nuke` removes the volume |
| Cortex fails to connect to Oracle if password mismatch with Cortex config | M | Cortex default Postgres password in `cortex.mk` env vars must match `POSTGRES_PASSWORD=arc` |
| Port 5432 conflict with a local Postgres installation | L | Document in oracle.mk help; user can override host port via compose override file |

---

---
url: /arc-platform/specs-site/004-dev-setup/plan.md
---
# Implementation Plan: Dev Setup Orchestration

> **Spec**: 004-dev-setup
> **Date**: 2026-02-28

## Summary

Add a `make dev` entrypoint that starts the full `think` profile in dependency order by generating `.make/profiles.mk` and `.make/registry.mk` from YAML source files, then resolving startup layers via topological sort before gating each layer on health checks. The implementation is pure Makefile + POSIX shell — no new runtimes, no new tool dependencies.

## Target Modules

| Module | Language | Changes |
|--------|----------|---------|
| `scripts/lib/` | bash (POSIX) | 5 new orchestration scripts |
| `services/otel/` | YAML | 2 new `service.yaml` metadata files |
| `services/cortex/` | YAML + Make | `service.yaml` patch + `cortex.mk` alias targets |
| `services/otel/` | Make | `otel.mk` alias targets |
| `services/` | YAML | `profiles.yaml` patch |
| Root | Make | `Makefile` — generated includes + `dev-*` targets |
| Root | gitignore | `.gitignore` patch |

## Technical Context

| Aspect | Value |
|--------|-------|
| Language(s) | bash (POSIX-compatible), GNU Make |
| Framework(s) | Make — `-include`, prerequisite rules, `.PHONY` |
| Storage | `.make/` generated files (gitignored) |
| Testing | Manual `make dev && make dev-health`; unit tests via `bats` if available; otherwise `bash -n` syntax check |
| Key Dependencies | `awk`, `curl`, `docker`, `docker compose` v2 — all pre-existing |

## Architecture

### Script Layer

```mermaid
graph LR
    subgraph "Source YAML"
        PY["services/profiles.yaml"]
        SY["services/*/service.yaml\nservices/*/*/service.yaml"]
    end

    subgraph "scripts/lib/"
        PP["parse-profiles.sh\nawk: YAML → Make vars"]
        PR["parse-registry.sh\nawk: YAML → Make vars"]
        RD["resolve-deps.sh\nDFS topological sort"]
        WH["wait-for-health.sh\ncurl / shell cmd poller"]
        CP["check-dev-prereqs.sh\nDocker + port checks"]
    end

    subgraph "Generated (.make/ — gitignored)"
        PMK[".make/profiles.mk"]
        RMK[".make/registry.mk"]
    end

    PY -->|input| PP --> PMK
    SY -->|input| PR --> RMK
    PMK & RMK -->|consumed by| RD
    RD -->|ordered list| WH
    CP -->|gate| RD
```

### Make Execution Flow

```mermaid
sequenceDiagram
    participant D as make dev
    participant G as .make/ generation
    participant P as check-dev-prereqs.sh
    participant R as resolve-deps.sh
    participant N as docker network create
    participant S as service targets
    participant W as wait-for-health.sh

    D->>G: .make/profiles.mk (.PHONY prereq on profiles.yaml)
    D->>G: .make/registry.mk (.PHONY prereq on service.yaml glob)
    D->>P: check-dev-prereqs — exits 1 on failure
    D->>R: resolve-deps PROFILE_$(PROFILE)_SERVICES → ordered layers
    D->>N: arc_platform_net + arc_otel_net (idempotent)
    loop Each layer (parallel within layer)
        D->>S: <codename>-up
    end
    loop Each service
        D->>W: wait-for-health <codename> <endpoint> <timeout>
        W-->>D: ✓ healthy OR ✗ timeout → exit 1
    end
    D-->>D: ✓ Profile ready
```

## Technical Decisions

### 1. YAML Parsing — `awk` only

No `yq`, no Python, no `jq`. The YAML in `service.yaml` files is simple enough that `awk` handles it:

* Scalar fields: `awk '/^codename:/ { print $2 }'`
* List fields (`depends_on`): capture lines between `depends_on:` and next dedented key
* `profiles.yaml` profile sections: capture `services:` list per profile block

**Risk**: Multi-level indentation in `ultra-instinct`'s `services: '*'` — handle as a special case in `parse-profiles.sh`.

### 2. Generated Files — `-include` pattern

```makefile
# Silently skip if .make/ doesn't exist yet — generation targets create it
-include .make/profiles.mk
-include .make/registry.mk

.make/profiles.mk: services/profiles.yaml | .make
	scripts/lib/parse-profiles.sh > $@

.make/registry.mk: $(shell find services -name service.yaml) | .make
	scripts/lib/parse-registry.sh > $@

.make:
	mkdir -p $@
```

The `$(shell find ...)` in the prerequisite list means Make re-evaluates staleness on every `make` invocation — no manual `make regen` step needed.

### 3. Health Endpoint Type Discrimination

`wait-for-health.sh` inspects the endpoint argument:

```
http:// or https://  →  curl -sf <endpoint> > /dev/null
otherwise            →  eval <endpoint>   (e.g. redis-cli ping)
```

**sonic edge case**: `service.yaml` has `health: redis-cli ping` which requires `redis-cli` on host. The implementation will prefer `docker exec arc-cache redis-cli ping` as the runtime endpoint passed to `wait-for-health.sh`, overriding the YAML value in the generated registry. Alternatively, update sonic's `service.yaml` health field to `docker exec arc-cache redis-cli ping` — **preferred** to keep single source of truth.

> **Decision**: Update `services/cache/service.yaml` health to `docker exec arc-cache redis-cli ping` as part of this feature (add as FR-16 in tasks).

### 4. Topological Sort — DFS in shell

`resolve-deps.sh` implements Kahn's algorithm (simpler than recursive DFS in POSIX shell):

1. Build adjacency list from `SERVICE_<n>_DEPENDS` registry variables
2. Compute in-degree for each service in the profile
3. Emit zero-in-degree services as Layer 0; reduce in-degrees; repeat
4. If any service remains unprocessed → cycle detected → exit 1
5. If any depends\_on name not in `ALL_SERVICES` → unregistered → exit 1

Output format: space-separated service names per line, one line per layer.

### 5. Network Creation — idempotent

```makefile
dev-networks:
	@docker network create arc_platform_net 2>/dev/null || true
	@docker network create arc_otel_net 2>/dev/null || true
```

### 6. Alias Target Pattern

Aliases use Make prerequisite chaining (not shell `$(MAKE)` recursion) to avoid subprocess overhead:

```makefile
.PHONY: cortex-up cortex-down
cortex-up: cortex-docker-up   # Make runs cortex-docker-up recipe
cortex-down: cortex-docker-down
```

For `friday-collector-up` (delegates to `otel-up-telemetry`):

```makefile
.PHONY: friday-collector-up friday-collector-down friday-collector-health
friday-collector-up: otel-up-telemetry
friday-collector-down:
	$(COMPOSE_OTEL) stop arc-friday-collector
friday-collector-health:
	@curl -sf http://localhost:13133/ > /dev/null \
	  && printf "$(COLOR_OK)✓$(COLOR_OFF) friday-collector healthy\n" \
	  || { printf "$(COLOR_ERR)✗ friday-collector unreachable$(COLOR_OFF)\n"; exit 1; }
```

## Constitution Check

| # | Principle | Status | Evidence |
|---|-----------|--------|----------|
| I | Zero-Dep CLI | N/A | No CLI changes |
| II | Platform-in-a-Box | **PASS** | `make dev` = single command → working platform |
| III | Modular Services | **PASS** | Framework reads per-service `service.yaml`; adding a service is self-contained |
| IV | Two-Brain | N/A | Shell only — no Go/Python introduced |
| V | Polyglot Standards | **PASS** | Scripts source `common.sh`; follow `→ ✓ ✗` conventions |
| VI | Local-First | N/A | CLI principle only |
| VII | Observability | **PASS** | `friday-collector` in `think` profile ensures OTEL collection by default |
| VIII | Security | **PASS** | No secrets in scripts; port pre-checks prevent conflicts |
| IX | Declarative | **PASS** | `profiles.yaml` + `service.yaml` = source of truth; Make reconciles |
| X | Stateful Ops | N/A | CLI principle only |
| XI | Resilience | **PASS** | Health gating between layers; fail-fast on unregistered/circular deps |
| XII | Interactive | N/A | CLI principle only |

## Project Structure

```
arc-platform/
├── .make/                                  # Generated — gitignored
│   ├── profiles.mk                         # PROFILE_THINK_SERVICES := ...
│   └── registry.mk                         # SERVICE_flash_HEALTH := ...
├── scripts/lib/
│   ├── common.sh                           # Existing — sourced by all scripts
│   ├── parse-profiles.sh                   # NEW: profiles.yaml → profiles.mk
│   ├── parse-registry.sh                   # NEW: service.yaml glob → registry.mk
│   ├── resolve-deps.sh                     # NEW: topological sort (Kahn's algo)
│   ├── wait-for-health.sh                  # NEW: HTTP + command health poller
│   └── check-dev-prereqs.sh               # NEW: Docker + port prereq checks
├── services/
│   ├── profiles.yaml                       # PATCH: add friday-collector to think
│   ├── cache/service.yaml                  # PATCH: health → docker exec arc-cache
│   ├── cortex/
│   │   ├── service.yaml                    # PATCH: remove oracle from depends_on
│   │   └── cortex.mk                       # PATCH: add cortex-up/down aliases
│   └── otel/
│       ├── otel.mk                         # PATCH: add friday-collector-* aliases
│       ├── service.yaml                    # NEW: full OTEL stack metadata
│       └── telemetry/
│           └── service.yaml               # NEW: collector-only metadata
├── Makefile                                # PATCH: -include, .make/ rules, dev-* targets
└── .gitignore                              # PATCH: add .make/
```

## Parallel Execution Strategy

The implementation has two natural phases. Within Phase 1, all groups run fully in parallel.

```mermaid
gantt
    title 004-dev-setup Implementation Phases
    dateFormat  HH:mm
    axisFormat  %H:%M

    section Phase 1 — Parallel
    Group A: YAML patches       :a1, 00:00, 20m
    Group B: parse-*.sh scripts :b1, 00:00, 30m
    Group C: resolve + wait + prereqs :c1, 00:00, 45m
    Group D: .mk alias targets  :d1, 00:00, 15m

    section Phase 2 — Sequential
    Makefile update (FR-8)      :e1, after b1 c1, 30m

    section Phase 3 — Verify
    Integration test + review   :f1, after e1, 20m
```

### Group A — YAML Patches (no dependencies)

* FR-9: `services/profiles.yaml` — add `friday-collector` to `think`
* FR-12: `services/cortex/service.yaml` — remove `oracle` from `depends_on`
* FR-13: `.gitignore` — add `.make/`
* FR-6: `services/otel/service.yaml` — new file
* FR-7: `services/otel/telemetry/service.yaml` — new file
* FR-16 (new): `services/cache/service.yaml` — update health endpoint

### Group B — Parsing Scripts (no dependencies)

* FR-1: `scripts/lib/parse-profiles.sh`
* FR-2: `scripts/lib/parse-registry.sh`

### Group C — Orchestration Scripts (no dependencies)

* FR-3: `scripts/lib/resolve-deps.sh`
* FR-4: `scripts/lib/wait-for-health.sh`
* FR-5: `scripts/lib/check-dev-prereqs.sh`

### Group D — .mk Alias Targets (no dependencies)

* FR-10: `services/otel/otel.mk` — `friday-collector-*` aliases
* FR-11: `services/cortex/cortex.mk` — `cortex-up/down` aliases

### Phase 2 — Makefile (after Phase 1 complete)

* FR-8: Root `Makefile` — `-include` directives, `.make/` generation rules, `dev-*` targets

## Script Interface Contracts

These contracts are the boundary between Phase 1 and Phase 2.

```bash
# parse-profiles.sh — reads services/profiles.yaml, writes to stdout
# Output: Make variable assignments
#   PROFILE_THINK_SERVICES := flash sonic strange friday-collector cortex
#   PROFILE_REASON_SERVICES := flash sonic strange friday-collector cortex otel
#   ALL_PROFILES := think reason ultra-instinct
scripts/lib/parse-profiles.sh

# parse-registry.sh — scans service.yaml files, writes to stdout
# Output:
#   ALL_SERVICES := flash sonic strange friday-collector cortex otel
#   SERVICE_flash_HEALTH := http://localhost:8222/healthz
#   SERVICE_flash_DEPENDS :=
#   SERVICE_flash_TIMEOUT := 60
#   SERVICE_cortex_DEPENDS := flash sonic strange friday-collector
scripts/lib/parse-registry.sh

# resolve-deps.sh — reads .make/registry.mk vars, takes service list, outputs layers
# Usage: resolve-deps.sh <svc1> <svc2> ...
# Output: one line per layer, services space-separated
#   flash sonic strange friday-collector
#   cortex
# Exit 1 on unregistered dep or cycle
scripts/lib/resolve-deps.sh flash sonic strange friday-collector cortex

# wait-for-health.sh — polls until healthy or timeout
# Usage: wait-for-health.sh <codename> <endpoint> [timeout_seconds]
# HTTP endpoint: curl -sf <endpoint>
# Command endpoint: eval <endpoint>
scripts/lib/wait-for-health.sh flash "http://localhost:8222/healthz" 60
scripts/lib/wait-for-health.sh sonic "docker exec arc-cache redis-cli ping" 30

# check-dev-prereqs.sh — validates environment
# Checks: docker daemon, docker compose v2, ports 4222 6379 6650 8082 13133 8081
# Output: colored checklist; exit 1 if any fail
scripts/lib/check-dev-prereqs.sh
```

## Makefile Target Specification

```makefile
PROFILE ?= think

# dev-* targets
dev:          dev-prereqs dev-networks .make/profiles.mk .make/registry.mk dev-up dev-wait
dev-up:       # start all profile services by layer (from resolve-deps output)
dev-down:     # stop all profile services (reverse layer order)
dev-health:   # call <codename>-health for each profile service
dev-logs:     # docker compose logs -f for all profile services
dev-status:   # formatted table: container name, status, health
dev-clean:    dev-down  # remove containers + volumes for profile services
dev-prereqs:  # scripts/lib/check-dev-prereqs.sh
dev-networks: # docker network create arc_platform_net arc_otel_net (idempotent)
dev-regen:    # force-rebuild .make/ files
```

## Reviewer Checklist

* \[ ] `make dev` exits 0 for `think` profile (SC-1)
* \[ ] `make dev-health` exits 0 and shows 5 ✓ services (SC-2)
* \[ ] `make dev-down` leaves no `arc-*` containers running (SC-3)
* \[ ] `make dev PROFILE=reason` starts SigNoz at `:3301` (SC-4)
* \[ ] Unregistered `depends_on` causes `make dev` to exit 1 with correct message (SC-5)
* \[ ] All existing targets (`flash-up`, `otel-up`, `cortex-docker-up`) still work (SC-6)
* \[ ] `.make/profiles.mk` timestamp updates when `profiles.yaml` is touched (SC-7)
* \[ ] `.make/registry.mk` timestamp updates when any `service.yaml` is touched (SC-8)
* \[ ] `make dev-prereqs` exits 1 with `✗ Docker daemon not running` when Docker stopped (SC-9)
* \[ ] All 5 scripts pass `bash -n` (syntax check)
* \[ ] All 5 scripts are executable (`chmod +x`)
* \[ ] `.make/` is present in `.gitignore`
* \[ ] `services/cortex/service.yaml` no longer lists `oracle` in `depends_on`
* \[ ] `services/profiles.yaml` `think` profile includes `friday-collector`

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| `awk` YAML parsing brittle for edge-case formatting | M | Validate against all existing `service.yaml` files; document expected format |
| Pulsar cold-start exceeds 120s timeout on slow machines | M | Default `strange` timeout 120s; document `STRANGE_TIMEOUT=180` override |
| `$(shell find ...)` in Makefile runs on every `make` invocation | L | Acceptable for dev workflow; document that CI should use `make dev-regen` explicitly |
| `ultra-instinct` `services: '*'` expansion hits unregistered services | M | `parse-profiles.sh` expands `*` only after `ALL_SERVICES` is built; validates each |
| `friday-collector-down` stops only collector but OTEL comment confuses users | L | Document clearly in `otel.mk` comment and `make friday-collector-help` output |
| cortex-docker-up builds the image (slow) before starting | M | `cortex-up` alias chains to `cortex-docker-up` which calls `cortex-build`; acceptable for now |

---

---
url: /arc-platform/specs-site/003-messaging-setup/plan.md
---
# Implementation Plan: Messaging & Cache Services Setup

> **Spec**: 003-messaging-setup
> **Date**: 2026-02-23

## Summary

Add three infrastructure services — NATS (Flash), Pulsar (Strange), Redis (Sonic) — each following the exact same structural pattern as the OTEL stack: thin version-pinned Dockerfile, `service.yaml`, `docker-compose.yml`, dedicated `.mk` include, and CI/release workflows. Network topology is simplified to two networks: `arc_platform_net` (shared, external) and `arc_otel_net` (internal to otel storage). The otel collector gains a Prometheus receiver and joins `arc_platform_net` so it can scrape NATS and Pulsar metrics. Profiles are updated so the `think` profile bootstraps messaging alongside cortex.

## Target Modules

| Module | Language | Changes |
|--------|----------|---------|
| `services/messaging/` | Config/Dockerfile | New — NATS (Flash) |
| `services/streaming/` | Config/Dockerfile | New — Pulsar (Strange) |
| `services/cache/` | Config/Dockerfile | New — Redis (Sonic) |
| `services/otel/telemetry/` | YAML | Update collector config — add prometheus receiver + arc\_platform\_net |
| `services/otel/docker-compose.yml` | YAML | Add arc\_platform\_net to collector service |
| `services/profiles.yaml` | YAML | Add flash, strange, sonic to `think` profile |
| `.github/workflows/` | YAML | New — messaging-images.yml + messaging-release.yml |
| `Makefile` | Make | Include flash.mk, strange.mk, sonic.mk |

## Technical Context

| Aspect | Value |
|--------|-------|
| Language | Config-only (YAML, Dockerfile) — no application code |
| Base Images | `nats:alpine`, `apachepulsar/pulsar:latest`, `redis:alpine` — floating tags, consistent with platform-spike |
| Testing | `docker compose ps`, `curl`, `redis-cli ping`, `nats stream ls` |
| OTEL Collector | `signoz/signoz-otel-collector:v0.142.0` — add `prometheus` receiver |
| Network | `arc_platform_net` (external bridge) + `arc_otel_net` (internal) |
| Volumes | Named Docker volumes: `arc-messaging-jetstream`, `arc-streaming-data`, `arc-cache-data` |
| CI Pattern | Mirror `cortex-images.yml` — amd64-only CI, dorny/paths-filter per service |

## Architecture

### Network topology

```mermaid
graph TB
    subgraph Host
        subgraph arc_platform_net [arc_platform_net — external bridge]
            flash["arc-messaging\nnats:2.10-alpine\n4222 • 8222"]
            strange["arc-streaming\npulsar:3.3.0\n6650 • 8082→8080"]
            sonic["arc-cache\nredis:7.4-alpine\n6379"]
            collector["arc-friday-collector\nsignoz otel v0.142\n4317 • 4318"]
            cortex["arc-cortex\n:8081"]
        end
        subgraph arc_otel_net [arc_otel_net — internal]
            zk["arc-friday-zookeeper"]
            ch["arc-friday-clickhouse"]
            friday["arc-friday / SigNoz\n:3301"]
        end
    end

    collector -- "prometheus scrape\narc-messaging:8222/metrics" --> flash
    collector -- "prometheus scrape\narc-streaming:8080/metrics" --> strange
    collector -- "OTLP export\nclickhousetraces/metrics/logs" --> ch
    cortex -- "nats://4222" --> flash
    cortex -- "pulsar://6650" --> strange
    cortex -- "redis://6379" --> sonic
```

### Collector config delta

```mermaid
flowchart LR
    subgraph Before
        B_otlp[otlp receiver] --> B_batch[batch] --> B_exp[clickhouse exporters]
    end
    subgraph After
        A_otlp[otlp receiver] --> A_batch[batch] --> A_exp[clickhouse exporters]
        A_prom[prometheus receiver\narc-messaging:8222\narc-streaming:8080] --> A_batch
    end
```

### Service file pattern (same for all three)

```mermaid
graph LR
    A[Dockerfile\npins upstream image\nadds OCI + arc labels] --> B[docker-compose.yml\nports, volumes, healthcheck\nexternal arc_platform_net]
    B --> C[service.yaml\nname, codename, port, health]
    C --> D[flash.mk / strange.mk / sonic.mk\nmake targets: up, down, health, logs, build, clean, nuke]
```

## Constitution Check

| # | Principle | Status | Evidence |
|---|-----------|--------|----------|
| I | Zero-Dep CLI | N/A | No CLI changes |
| II | Platform-in-a-Box | PASS | `make messaging-up` starts all three; `think` profile includes them |
| III | Modular Services | PASS | Each in own dir under `services/`; self-contained; added to profile |
| IV | Two-Brain | PASS | Config-only — no language concern |
| V | Polyglot Standards | PASS | Same Dockerfile/compose/healthcheck structure as otel |
| VI | Local-First | N/A | CLI only |
| VII | Observability | PASS | NATS + Pulsar scraped by collector → SigNoz; Redis deferred (TD-001) |
| VIII | Security | PASS | NATS+Redis run non-root; ports bind `127.0.0.1`; no secrets in compose |
| IX | Declarative | N/A | CLI only |
| X | Stateful Ops | N/A | CLI only |
| XI | Resilience | PASS | Health checks with correct start\_periods; named volumes survive restarts |
| XII | Interactive | N/A | CLI only |

> **Known deviation — Pulsar non-root (NFR-1)**: The `apachepulsar/pulsar:3.3.0` image requires root for `/pulsar` directory initialization. Documented in service.yaml and docker-compose.yml comments. No upstream non-root alternative available.

## Project Structure

```
arc-platform/
├── services/
│   ├── messaging/                        ← NEW (Flash / NATS)
│   │   ├── Dockerfile                    # FROM nats:2.10-alpine; adds labels; non-root (nats uid 1000)
│   │   ├── service.yaml                  # name, codename, port, health
│   │   ├── docker-compose.yml            # arc-messaging container; arc_platform_net external
│   │   └── flash.mk                      # flash-up/down/health/logs/build/clean/nuke
│   ├── streaming/                        ← NEW (Strange / Pulsar)
│   │   ├── Dockerfile                    # FROM apachepulsar/pulsar:3.3.0; adds labels
│   │   ├── service.yaml
│   │   ├── docker-compose.yml            # arc-streaming; PULSAR_MEM; arc_platform_net external
│   │   └── strange.mk
│   ├── cache/                            ← NEW (Sonic / Redis)
│   │   ├── Dockerfile                    # FROM redis:7.4-alpine; adds labels; non-root
│   │   ├── service.yaml
│   │   ├── docker-compose.yml            # arc-cache; AOF; arc_platform_net external
│   │   └── sonic.mk
│   ├── otel/
│   │   ├── docker-compose.yml            # MODIFY — collector gains arc_platform_net
│   │   └── telemetry/
│   │       └── config/
│   │           └── otel-collector-config.yaml  # MODIFY — add prometheus receiver
│   └── profiles.yaml                     # MODIFY — think profile += flash, strange, sonic
├── .github/workflows/
│   ├── messaging-images.yml              ← NEW
│   └── messaging-release.yml            ← NEW
└── Makefile                              # MODIFY — include 3 new .mk files + messaging-* aggregates
```

## Key Implementation Decisions

### 1. Dockerfiles are thin label wrappers

No config baked in — NATS/Redis/Pulsar are configured entirely via command-line flags in `docker-compose.yml`. The Dockerfile exists only to:

* Pin the upstream version (single source of truth for the image tag)
* Add OCI labels (`org.opencontainers.image.*`) and `arc.service.*` labels
* Set non-root user for NATS and Redis

```dockerfile
# flash/Dockerfile example
FROM nats:2.10-alpine
LABEL org.opencontainers.image.title="ARC Flash — Messaging"
LABEL arc.service.codename="flash"
LABEL arc.service.tech="nats"
USER 1000  # nats user already exists in upstream image
```

### 2. arc\_platform\_net as external network

Each `docker-compose.yml` declares the network as external so services from different compose files can resolve each other by hostname:

```yaml
networks:
  arc_platform_net:
    external: true
    name: arc_platform_net
```

The network is created once: `docker network create arc_platform_net`.
Makefile aggregate `messaging-up` ensures the network exists before starting containers.

### 3. Collector prometheus receiver — scrape over arc\_platform\_net

The collector's `otel-collector-config.yaml` gains a `prometheus` receiver. The collector's `docker-compose.yml` entry gains `arc_platform_net` as a second network (it already has `arc_otel_net`).

```yaml
# otel-collector-config.yaml additions
receivers:
  prometheus:
    config:
      scrape_configs:
        - job_name: arc-messaging
          scrape_interval: 15s
          static_configs:
            - targets: ['arc-messaging:8222']
        - job_name: arc-streaming
          scrape_interval: 30s
          static_configs:
            - targets: ['arc-streaming:8080']

service:
  pipelines:
    metrics:
      receivers: [otlp, prometheus]   # was: [otlp]
      processors: [batch]
      exporters: [signozclickhousemetrics]
```

### 4. Makefile aggregate targets

A top-level `messaging.mk` (included in `Makefile`) provides the three-service orchestration. The per-service `.mk` files stay in their own directories.

```makefile
# messaging.mk — aggregate: start all three
messaging-up:
    @docker network create arc_platform_net 2>/dev/null || true
    $(MAKE) flash-up --no-print-directory
    $(MAKE) strange-up --no-print-directory
    $(MAKE) sonic-up --no-print-directory
```

### 5. CI workflow pattern (mirrors cortex-images.yml)

Single `messaging-images.yml` with per-service path filters and parallel build jobs — amd64 only:

```
changes ──┬──> test-flash (if flash changed)   ──> build-flash
          ├──> test-strange (if strange changed) ──> build-strange  ──> security
          └──> test-sonic (if sonic changed)    ──> build-sonic
```

No Go/Python code → no test/lint jobs. Build jobs fire on Dockerfile or compose changes only.

## Parallel Execution Strategy

```mermaid
gantt
    title Implementation Phases
    dateFormat X
    axisFormat %s

    section Phase 1 — Foundation (sequential)
    Create arc_platform_net network infra          :p1a, 0, 1
    Update profiles.yaml think profile             :p1b, after p1a, 1

    section Phase 2 — Services (fully parallel)
    Flash: Dockerfile + service.yaml + compose     :p2a, after p1b, 2
    Strange: Dockerfile + service.yaml + compose   :p2b, after p1b, 2
    Sonic: Dockerfile + service.yaml + compose     :p2c, after p1b, 2

    section Phase 3 — Make targets (parallel per service)
    flash.mk                                       :p3a, after p2a, 1
    strange.mk                                     :p3b, after p2b, 1
    sonic.mk                                       :p3c, after p2c, 1
    messaging.mk aggregates + Makefile includes    :p3d, after p2c, 1

    section Phase 4 — OTEL integration (sequential, needs Phase 2)
    otel-collector-config.yaml — prometheus recv   :p4a, after p2a, 1
    otel docker-compose.yml — add arc_platform_net :p4b, after p4a, 1
    Rebuild + test collector image                 :p4c, after p4b, 1

    section Phase 5 — CI/CD (parallel)
    messaging-images.yml workflow                  :p5a, after p3d, 2
    messaging-release.yml workflow                 :p5b, after p3d, 2

    section Phase 6 — Integration test
    make messaging-up + health checks              :p6, after p5b, 1
```

**Parallelizable task groups:**

* Phase 2: Flash + Strange + Sonic service directories — fully independent
* Phase 3: flash.mk + strange.mk + sonic.mk — independent after their Phase 2 deps
* Phase 5: messaging-images.yml + messaging-release.yml — independent of each other

## Reviewer Checklist

* \[ ] `make messaging-up` exits 0; all three containers in `healthy` state
* \[ ] `make messaging-health` exits 0
* \[ ] `make messaging-down` stops all three; no orphaned containers
* \[ ] `make flash-up && make flash-health` works independently
* \[ ] `make strange-up && make strange-health` works independently
* \[ ] `make sonic-up && make sonic-health` works independently
* \[ ] `docker network inspect arc_platform_net` shows arc-messaging, arc-streaming, arc-cache, arc-friday-collector connected
* \[ ] `docker inspect arc-messaging | jq '.[0].Config.User'` returns non-root
* \[ ] `docker inspect arc-cache | jq '.[0].Config.User'` returns non-root
* \[ ] Pulsar non-root deviation is documented in `services/streaming/docker-compose.yml`
* \[ ] All ports bind `127.0.0.1` — verify with `docker compose ps`
* \[ ] All volumes are named (not bind mounts) — verify with `docker volume ls | grep arc`
* \[ ] `otel-collector-config.yaml` has `prometheus` receiver; metrics pipeline includes it
* \[ ] `services/otel/docker-compose.yml` collector service has `arc_platform_net` network
* \[ ] `services/profiles.yaml` `think` profile includes flash, strange, sonic
* \[ ] `Makefile` includes flash.mk, strange.mk, sonic.mk, messaging.mk
* \[ ] `messaging-images.yml` runs only on relevant path changes; completes under 3 min
* \[ ] `messaging-release.yml` tag format `messaging/v*` builds multi-platform images
* \[ ] All Dockerfiles have OCI + `arc.service.*` labels
* \[ ] No secrets or credentials in any compose file

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Pulsar standalone cold start > 60s on slow machines | M | `start_period: 90s` with 15 retries; `strange-health` probes with explicit timeout message |
| `arc_platform_net` not created before compose up | H | `messaging-up` calls `docker network create arc_platform_net 2>/dev/null \|\| true` first |
| Collector prometheus receiver syntax differs between SigNoz collector versions | M | Test with `docker compose config` after edit; collector version is pinned at v0.142.0 |
| Port 8082 (Pulsar admin) conflict with other local services | L | Document in service.yaml; `strange-health` gives a clear error if bind fails |
| NATS JetStream stream auto-creation | L | NATS JetStream does not auto-create named streams — a post-start `nats stream add` step is needed if FR-4 is in scope; otherwise defer US-9 to a future task |

---

---
url: /arc-platform/specs-site/006-platform-control/plan.md
---
# Implementation Plan: Platform Control Plane Services Setup

> **Spec**: 006-platform-control
> **Date**: 2026-02-28

## Summary

Add three control plane services — Traefik v3 (Heimdall), OpenBao (Nick Fury), Unleash (Mystique) — each following the same structural pattern as 003-messaging-setup and 005-data-layer: thin Dockerfile label wrapper, `service.yaml`, `docker-compose.yml`, dedicated `.mk` include, CI/release workflows. All three join `arc_platform_net`. Non-root handling requires per-service decisions: Heimdall uses internal port remapping (non-privileged :8080 proxy); Nick Fury documents root deviation (dev mode is inherently insecure by design); Mystique attempts `user: "1000:1000"` at runtime. Profiles updated so `think` includes heimdall; `reason` adds nick-fury + mystique.

## Target Modules

| Module | Language | Changes |
|--------|----------|---------|
| `services/gateway/` | Config/Dockerfile | New — Traefik v3 (Heimdall) |
| `services/secrets/` | Config/Dockerfile | New — OpenBao (Nick Fury) |
| `services/flags/` | Config/Dockerfile | New — Unleash (Mystique) |
| `services/profiles.yaml` | YAML | Add heimdall to `think`; nick-fury + mystique to `reason` |
| `.github/workflows/` | YAML | New — control-images.yml + control-release.yml |
| `Makefile` | Make | Include heimdall.mk, nick-fury.mk, mystique.mk, control.mk + publish-all |
| `services/control.mk` | Make | New — aggregate up/down/health/logs |

## Technical Context

| Aspect | Value |
|--------|-------|
| Language | Config-only (YAML, Dockerfile, Makefile) — no application code |
| Base Images | `traefik:v3`, `openbao/openbao`, `unleashorg/unleash-server` |
| Testing | `traefik healthcheck --ping`, `/v1/sys/health`, `curl /health` |
| Network | `arc_platform_net` (external bridge, pre-existing) |
| Volumes | None for Heimdall/Nick Fury (stateless in dev); none for Mystique (DB-backed) |
| CI Pattern | Mirror `data-images.yml` — amd64-only CI, dorny/paths-filter per service |
| Root handling | Heimdall: port remap to uid 1000; Nick Fury: root documented; Mystique: try uid 1000 |

## Architecture

### Service topology

```mermaid
graph TB
    subgraph Host
        subgraph arc_platform_net
            heimdall["arc-gateway\ntraefik:v3\n:8080 proxy (host→:80)\n:8090 dashboard"]
            fury["arc-vault\nopenbao/openbao\n:8200 API + UI"]
            mystique["arc-flags\nunleash-server\n:4242 UI + REST"]
            oracle["arc-sql-db\npostgres:17\n:5432"]
            sonic["arc-cache\nredis\n:6379"]
        end
        dockersock["/var/run/docker.sock (ro)"]
    end

    dockersock -.->|providers.docker| heimdall
    mystique -->|DATABASE_URL| oracle
    mystique -->|REDIS_HOST| sonic
    apps["Clients"] -->|HTTP :80| heimdall
    apps -->|secrets :8200| fury
    apps -->|flags :4242| mystique
```

### Heimdall port remapping (non-root strategy)

```mermaid
graph LR
    host80["Host :80"] -->|127.0.0.1:80:8080| proxy["Container :8080\nTraefik entrypoint.web"]
    host8090["Host :8090"] -->|127.0.0.1:8090:8090| dash["Container :8090\nTraefik API/dashboard\n--api.insecure=true"]
    docker["Docker socket (ro)"] -->|label discovery| proxy
```

Internal container ports are non-privileged (`≥1024`), so Traefik runs as `uid 1000` without any capabilities.

### File pattern (identical across all three)

```mermaid
graph LR
    A["Dockerfile\nFROM upstream\nOCI + arc labels\nnon-root where possible"] --> B["docker-compose.yml\n127.0.0.1 ports\narc_platform_net external\nenv / command flags"]
    B --> C["service.yaml\nname, codename, image\nports, health, depends_on"]
    C --> D["{service}.mk\nup · down · health · logs\nbuild · push · publish · tag\nclean · nuke · help"]
```

## Key Implementation Decisions

### 1. Heimdall — Non-root via internal port remapping

Traefik in default config binds to privileged port 80 (requires root). Solution: configure internal entrypoints on non-privileged ports; remap at the Docker host layer.

```dockerfile
# services/gateway/Dockerfile
FROM traefik:v3
LABEL org.opencontainers.image.title="ARC Heimdall — API Gateway"
LABEL org.opencontainers.image.description="Traefik v3 ingress gateway for the A.R.C. Platform"
LABEL org.opencontainers.image.source="https://github.com/arc-framework/arc-platform"
LABEL arc.service.name="arc-gateway"
LABEL arc.service.codename="heimdall"
LABEL arc.service.tech="traefik"
# traefik:v3 runs as root by default. We configure internal ports ≥1024
# so the process runs as uid 1000 without CAP_NET_BIND_SERVICE.
USER 1000
```

```yaml
# services/gateway/docker-compose.yml
arc-gateway:
  command:
    - --api.insecure=true                          # dashboard on internal :8090
    - --api.dashboard=true
    - --ping=true                                  # /ping healthcheck endpoint
    - --providers.docker=true
    - --providers.docker.network=arc_platform_net
    - --providers.docker.exposedbydefault=false    # opt-in only via labels
    - --entrypoints.web.address=:8080              # non-privileged proxy port
    - --entrypoints.dashboard.address=:8090        # dashboard port
  ports:
    - "127.0.0.1:80:8080"     # host:80 → container proxy (non-privileged inside)
    - "127.0.0.1:8090:8090"   # dashboard
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock:ro
  user: "1000:1000"
  healthcheck:
    test: ["CMD", "traefik", "healthcheck", "--ping"]
    interval: 5s
    timeout: 3s
    retries: 5
    start_period: 5s
```

**Key**: `--providers.docker.exposedbydefault=false` means services must opt-in via `traefik.enable=true` label. Prevents unintentional routing.

### 2. Nick Fury — Root deviation (documented, dev mode by design)

OpenBao in dev mode (`-dev` flag) is intentionally insecure: in-memory storage, known root token, auto-unsealed. Running as root is consistent with the dev-only posture of this service. Document in Dockerfile and help text.

```dockerfile
# services/secrets/Dockerfile
FROM openbao/openbao
LABEL arc.service.name="arc-vault"
LABEL arc.service.codename="nick-fury"
LABEL arc.service.tech="openbao"
# openbao/openbao runs as root by default.
# Nick Fury uses -dev mode (in-memory, auto-unsealed, known root token).
# This is a DEVELOPMENT-ONLY service. Root is acceptable for dev-mode OpenBao.
# For production: use Raft-backed storage with TLS + non-root user.
```

```yaml
# services/secrets/docker-compose.yml
arc-vault:
  command: server -dev
  environment:
    VAULT_DEV_ROOT_TOKEN_ID: arc-dev-token
    VAULT_DEV_LISTEN_ADDRESS: 0.0.0.0:8200
  ports:
    - "127.0.0.1:8200:8200"
  healthcheck:
    test: ["CMD-SHELL", "wget -qO- http://localhost:8200/v1/sys/health || exit 1"]
    interval: 5s
    timeout: 3s
    retries: 5
    start_period: 5s
```

If `wget` is absent in the openbao image, fall back to bash `/dev/tcp` (same pattern as Cerebro).

### 3. Mystique — Non-root attempt, document if fails

Unleash is a Node.js app. `user: "1000:1000"` in compose is the first attempt. If Unleash writes to privileged paths at startup, document the root deviation in compose comments (like Pulsar in 003).

```yaml
# services/flags/docker-compose.yml
arc-flags:
  user: "1000:1000"   # attempt non-root; remove + add comment if Unleash refuses to start
  environment:
    DATABASE_URL: postgresql://arc:arc@arc-sql-db:5432/unleash
    REDIS_HOST: arc-cache
    REDIS_PORT: 6379
  ports:
    - "127.0.0.1:4242:4242"
  healthcheck:
    test: ["CMD-SHELL", "wget -qO- http://localhost:4242/health || exit 1"]
    interval: 10s
    timeout: 5s
    retries: 10
    start_period: 30s   # Unleash runs DB migrations on first boot — needs longer start_period
```

**Key**: `start_period: 30s` — Unleash auto-runs DB migrations on first startup against Oracle; this takes 10-20s.

### 4. service.yaml pattern (Mystique depends\_on)

```yaml
# services/flags/service.yaml
name: arc-flags
codename: mystique
image: ghcr.io/arc-framework/arc-flags:latest
tech: unleash
upstream: unleashorg/unleash-server
ports:
  - { port: 4242, protocol: http, description: "Unleash UI + REST API" }
health:
  endpoint: http://localhost:4242/health
  interval: 10s
depends_on:
  - oracle   # Postgres — feature flag state storage
  - sonic    # Redis — session cache
```

### 5. control.mk aggregate (mirrors data.mk)

```makefile
## control-up: Start all control plane services (Heimdall + Nick Fury + Mystique)
control-up:
    @docker network create arc_platform_net 2>/dev/null || true
    $(MAKE) heimdall-up --no-print-directory
    $(MAKE) nick-fury-up --no-print-directory
    $(MAKE) mystique-up --no-print-directory

## control-down: Stop all control plane services
control-down:
    $(MAKE) mystique-down --no-print-directory
    $(MAKE) nick-fury-down --no-print-directory
    $(MAKE) heimdall-down --no-print-directory

## control-health: Check health of all control plane services
control-health:
    @$(MAKE) heimdall-health --no-print-directory && \
     $(MAKE) nick-fury-health --no-print-directory && \
     $(MAKE) mystique-health --no-print-directory
```

Note: `control-up` starts in dependency order; `control-down` reverses it (Mystique first, then infra).

### 6. CI/CD — mirrors data-images.yml exactly

`control-images.yml` path filters:

* `services/gateway/**` → build-heimdall
* `services/secrets/**` → build-nick-fury
* `services/flags/**` → build-mystique

`control-release.yml` tag format: `control/vX.Y.Z` → Docker tag `control-vX.Y.Z`

### 7. profiles.yaml update

```yaml
think:
  services:
    - flash
    - sonic
    - strange
    - friday-collector
    - cortex
    - oracle
    - cerebro
    - heimdall       # ← ADD: gateway is foundational for any agent platform

reason:
  services:
    - cortex
    - flash
    - strange
    - sonic
    - otel
    - tardis
    - nick-fury      # ← ADD: secrets management
    - mystique       # ← ADD: feature flags
```

## Constitution Check

| # | Principle | Status | Evidence |
|---|-----------|--------|----------|
| I | Zero-Dep CLI | N/A | No CLI changes |
| II | Platform-in-a-Box | PASS | `make control-up` boots all three; heimdall joins `think` profile |
| III | Modular Services | PASS | Each self-contained in own directory under `services/`; own service.yaml |
| IV | Two-Brain | PASS | Config-only upstream images — no language concern |
| V | Polyglot Standards | PASS | Same Dockerfile/compose/healthcheck structure as 003/005 |
| VI | Local-First | N/A | CLI only |
| VII | Observability | PASS | Traefik dashboard + /ping; OpenBao /v1/sys/health; Unleash /health |
| VIII | Security | PASS† | Heimdall uid 1000; Mystique uid 1000 (attempt); Nick Fury root documented (dev-mode-only) |
| IX | Declarative | N/A | CLI only |
| X | Stateful Ops | N/A | CLI only |
| XI | Resilience | PASS | Health checks with appropriate start\_periods; Mystique depends\_on oracle + sonic |
| XII | Interactive | N/A | CLI only |

†**Security note**: Nick Fury root deviation is intentional — OpenBao `-dev` mode is by-design insecure (ephemeral, known token). Documented in Dockerfile and `nick-fury-help`. Not a constitution violation given the explicit dev-only scope.

## Project Structure

```
arc-platform/
├── services/
│   ├── gateway/                              ← NEW (Heimdall / Traefik v3)
│   │   ├── Dockerfile                        # FROM traefik:v3; OCI + arc labels; USER 1000
│   │   ├── service.yaml                      # name, codename, ports 80+8090, health /ping
│   │   ├── docker-compose.yml                # port remap 80:8080; Docker socket ro; no volume
│   │   └── heimdall.mk                       # heimdall-up/down/health/logs/build/push/publish/...
│   ├── secrets/                              ← NEW (Nick Fury / OpenBao)
│   │   ├── Dockerfile                        # FROM openbao/openbao; OCI + arc labels; root documented
│   │   ├── service.yaml                      # name, codename, port 8200, health /v1/sys/health
│   │   ├── docker-compose.yml                # -dev mode; VAULT_DEV_ROOT_TOKEN_ID; no volume
│   │   └── nick-fury.mk                      # nick-fury-up/down/health/logs/build/push/publish/...
│   ├── flags/                                ← NEW (Mystique / Unleash)
│   │   ├── Dockerfile                        # FROM unleashorg/unleash-server; OCI + arc labels
│   │   ├── service.yaml                      # name, codename, port 4242, depends_on: oracle, sonic
│   │   ├── docker-compose.yml                # DATABASE_URL + REDIS_*; user 1000; start_period 30s
│   │   └── mystique.mk                       # mystique-up/down/health/logs/build/push/publish/...
│   ├── profiles.yaml                         # MODIFY — think += heimdall; reason += nick-fury, mystique
│   └── control.mk                            ← NEW (aggregate)
├── .github/workflows/
│   ├── control-images.yml                    ← NEW
│   └── control-release.yml                   ← NEW
└── Makefile                                  # MODIFY — include *.mk + publish-all
```

## Parallel Execution Strategy

```mermaid
gantt
    title 006-platform-control Implementation Phases
    dateFormat X
    axisFormat %s

    section Phase 1 — Foundation
    Update profiles.yaml (heimdall→think, fury+mystique→reason)  :p1, 0, 1

    section Phase 2 — Services (fully parallel)
    gateway/: Dockerfile + service.yaml + compose                 :p2a, after p1, 2
    secrets/: Dockerfile + service.yaml + compose                 :p2b, after p1, 2
    flags/: Dockerfile + service.yaml + compose                   :p2c, after p1, 2

    section Phase 3 — Make targets (parallel per service)
    heimdall.mk                                                    :p3a, after p2a, 1
    nick-fury.mk                                                   :p3b, after p2b, 1
    mystique.mk                                                    :p3c, after p2c, 1
    control.mk aggregate + Makefile includes                       :p3d, after p2a, 1

    section Phase 4 — CI/CD (parallel)
    control-images.yml workflow                                    :p4a, after p3d, 2
    control-release.yml workflow                                   :p4b, after p3d, 2

    section Phase 5 — Integration
    make control-up + make control-health                          :p5, after p4b, 1
    Docs & links update                                            :p5b, after p5, 1
```

**Parallelizable groups:**

* Phase 2: All three service directories are fully independent
* Phase 3: heimdall.mk + nick-fury.mk + mystique.mk — independent after Phase 2 deps
* Phase 4: control-images.yml + control-release.yml — independent of each other

## Tech Debt

| ID | Item | Rationale |
|----|------|-----------|
| TD-001 | Nick Fury production mode | OpenBao `-dev` mode is stateless and insecure. Production requires Raft-backed storage, TLS, and non-root user. Deferred to a future security hardening spec. |
| TD-002 | Heimdall TLS termination | No HTTPS in dev. Production needs TLS config (cert resolvers, ACME). Deferred. |
| TD-003 | Mystique admin credentials | Unleash auto-creates admin user on first boot; credentials are printed in logs. A future init task should configure an admin API token via env to avoid manual setup. |
| TD-004 | Traefik metrics scraping | Traefik exposes Prometheus metrics at `:8090/metrics`. When a dashboard requirement exists, add scrape job to OTEL collector (see 003 pattern). |

## Reviewer Checklist

* \[ ] `make control-up` exits 0; all three containers in `healthy` state
* \[ ] `make control-health` exits 0
* \[ ] `make control-down` stops all three; no orphaned containers
* \[ ] `make heimdall-up && make heimdall-health` works independently
* \[ ] `make nick-fury-up && make nick-fury-health` works independently
* \[ ] `make mystique-up && make mystique-health` works independently (requires oracle + sonic running)
* \[ ] `curl http://localhost:8090/ping` returns `OK` (Traefik ping endpoint)
* \[ ] Traefik dashboard accessible at `http://localhost:8090/dashboard/`
* \[ ] `curl -H "X-Vault-Token: arc-dev-token" http://localhost:8200/v1/sys/health` returns HTTP 200
* \[ ] `curl http://localhost:4242/health` returns HTTP 200
* \[ ] `docker inspect arc-gateway | jq '.[0].Config.User'` confirms `"1000:1000"` or `"1000"`
* \[ ] `docker inspect arc-flags | jq '.[0].Config.User'` confirms non-root or deviation documented
* \[ ] Nick Fury root deviation documented in Dockerfile comment + `nick-fury-help` output
* \[ ] All ports bind `127.0.0.1` only — verify with `docker compose ps`
* \[ ] No persistent volumes for Heimdall or Nick Fury (stateless dev services)
* \[ ] `services/profiles.yaml` `think` includes `heimdall`; `reason` includes `nick-fury` + `mystique`
* \[ ] Makefile includes `heimdall.mk`, `nick-fury.mk`, `mystique.mk`, `control.mk`
* \[ ] `publish-all` includes `heimdall-build heimdall-publish`, `nick-fury-build nick-fury-publish`, `mystique-build mystique-publish`
* \[ ] `control-images.yml` path filters cover all three service dirs
* \[ ] `control-release.yml` tag format `control/v*`; multi-platform amd64 + arm64
* \[ ] All Dockerfiles have OCI + `arc.service.*` labels
* \[ ] `CLAUDE.md` Service Codenames table updated (Nick Fury: Infisical → OpenBao)
* \[ ] `.specify/config.yaml` `secrets` entry tech updated from `infisical` to `openbao`

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Traefik uid 1000 fails (needs root for some internal operation) | M | Fall back to root + document deviation like Pulsar in 003 |
| Mystique uid 1000 fails (writes to privileged path at startup) | M | Remove `user:` from compose; add comment documenting root deviation |
| Mystique startup fails because Oracle isn't ready | H | `start_period: 30s` + Unleash's built-in retry; `control-up` ensures oracle starts first via ordering |
| `wget` absent in openbao image | M | Fall back to bash `/dev/tcp` pattern (same as arc-vector-db) |
| Port 80 already bound on dev machine (other HTTP server) | M | Document in `heimdall-help`; user can change host port via compose override |
| Mystique DB migration creates tables in `arc` DB instead of separate DB | L | Verify `DATABASE_URL` points to `/unleash` DB path; Oracle Dockerfile may need `CREATE DATABASE unleash` init SQL |
| `arc_platform_net` not created before `make control-up` | H | `control-up` calls `docker network create arc_platform_net 2>/dev/null || true` first |

---

---
url: /arc-platform/specs-site/007-voice-stack/plan.md
---
# Implementation Plan: Realtime Voice Infrastructure Setup

> **Spec**: 007-voice-stack
> **Date**: 2026-03-01

## Summary

Add three co-located realtime services — LiveKit Server (Daredevil), LiveKit Ingress (Sentry), LiveKit Egress (Scribe) — all in `services/realtime/` following the thin-wrapper pattern from 003/005/006: Dockerfiles with OCI labels, shared `docker-compose.yml`, `service.yaml`, `realtime.mk` aggregate, CI/release workflows. All three join `arc_platform_net`; `realtime` added to `think`, `reason` profiles. Sentry and Scribe depend on Daredevil and are always started as a unit via the shared compose. Static dev API keys (`devkey`/`devsecret`) are used in config files; vault integration is deferred.

## Target Modules

| Module | Language | Changes |
|--------|----------|---------|
| `services/realtime/` | Config/Dockerfile | New — 3 Dockerfiles, shared compose, 3 config YAMLs, service.yaml, realtime.mk |
| `services/profiles.yaml` | YAML | Add `realtime` to `think` and `reason` profiles |
| `.github/workflows/` | YAML | New — realtime-images.yml + realtime-release.yml |
| `Makefile` | Make | Include `services/realtime/realtime.mk` |
| `scripts/scripts.mk` | Make | Add realtime build/publish targets to `publish-all` |
| `scripts/lib/check-dev-prereqs.sh` | Bash | Add ports 7880, 7881, 1935 to prerequisite checks |

## Technical Context

| Aspect | Value |
|--------|-------|
| Language | Config-only (YAML, Dockerfile, Makefile) — no application code |
| Base Images | `livekit/livekit-server`, `livekit/ingress`, `livekit/egress` |
| Testing | HTTP GET :7880 (Daredevil), :7888 (Sentry), :7889 (Scribe) |
| Network | `arc_platform_net` (external bridge, pre-existing) |
| Volumes | None — stateless in dev; Egress writes recordings to arc-storage over S3 |
| CI Pattern | Mirror `control-images.yml` — amd64-only CI, dorny/paths-filter; single path `services/realtime/**` |
| Root handling | Verify per image; document deviation in Dockerfile if root required |
| Key config | Static `devkey`/`devsecret` in YAML config files mounted read-only |

## Architecture

### Service topology

```mermaid
graph TB
    subgraph Host
        subgraph arc_platform_net
            realtime["arc-realtime\nlivekit/livekit-server\n:7880 HTTP/signal\n:7881 gRPC\n:7882 TURN/TCP\n:50100-50200/UDP RTP"]
            ingress["arc-realtime-ingress\nlivekit/ingress\n:7888 controller\n:1935 RTMP"]
            egress["arc-realtime-egress\nlivekit/egress\n:7889 controller"]
            cache["arc-cache\nRedis\n:6379"]
            storage["arc-storage\nMinIO\n:9000"]
        end
    end

    webrtc["WebRTC Clients"] -->|7880 signal| realtime
    rtmp["RTMP Sources"] -->|1935 RTMP| ingress
    ingress -->|ws://arc-realtime:7880| realtime
    egress -->|realtime API :7880| realtime
    egress -->|S3 recordings| storage
    realtime -->|pub/sub state| cache
    realtime -.->|"50100-50200/UDP RTP\n(0.0.0.0 — NAT traversal)"| webrtc
```

### Port binding strategy

```mermaid
graph LR
    subgraph "TCP — 127.0.0.1 only"
        p7880["Host :7880 → container :7880\nLiveKit HTTP + signalling"]
        p7881["Host :7881 → container :7881\ngRPC API"]
        p7882["Host :7882 → container :7882\nTURN/TCP"]
        p7888["Host :7888 → container :7888\nIngress controller"]
        p1935["Host :1935 → container :1935\nRTMP ingest"]
        p7889["Host :7889 → container :7889\nEgress controller"]
    end
    subgraph "UDP — 0.0.0.0 (WebRTC exception)"
        pudp["Host :50100-50200 → container :50100-50200\nWebRTC RTP media\nClients outside Docker need direct reach"]
    end
```

### File pattern (co-located variant)

```mermaid
graph TD
    A["services/realtime/"] --> B["Dockerfile\nFROM livekit/livekit-server\narc-realtime"]
    A --> C["Dockerfile.ingress\nFROM livekit/ingress\narc-realtime-ingress"]
    A --> D["Dockerfile.egress\nFROM livekit/egress\narc-realtime-egress"]
    A --> E["livekit.yaml\nkeys, redis, RTC ports\nLIVEKIT_NODE_IP env"]
    A --> F["ingress.yaml\napi_url, key/secret\n:7888 health port"]
    A --> G["egress.yaml\napi_url, key/secret\nS3 endpoint → arc-storage"]
    A --> H["docker-compose.yml\n3 services, shared network\ndepends_on ordering"]
    A --> I["service.yaml\nrole: realtime, codename: daredevil\ndepends_on: cache"]
    A --> J["realtime.mk\nrealtime-* + ingress-* + egress-*"]
```

## Key Implementation Decisions

### 1. Daredevil — LiveKit Server config

```yaml
# services/realtime/livekit.yaml
port: 7880
grpc_port: 7881
rtc:
  tcp_port: 7882
  udp_port_range_start: 50100
  udp_port_range_end: 50200
  use_external_ip: false
  node_ip: ${LIVEKIT_NODE_IP:-127.0.0.1}
redis:
  address: arc-cache:6379
keys:
  devkey: devsecret
```

```yaml
# services/realtime/docker-compose.yml (arc-realtime service)
arc-realtime:
  image: ghcr.io/arc-framework/arc-realtime:latest
  build:
    context: services/realtime
    dockerfile: Dockerfile
  command: --config /etc/livekit.yaml
  volumes:
    - ./services/realtime/livekit.yaml:/etc/livekit.yaml:ro
  ports:
    - "127.0.0.1:7880:7880"
    - "127.0.0.1:7881:7881"
    - "127.0.0.1:7882:7882"
    - "0.0.0.0:50100-50200:50100-50200/udp"  # WebRTC RTP — NAT traversal requires 0.0.0.0
  environment:
    LIVEKIT_NODE_IP: ${LIVEKIT_NODE_IP:-127.0.0.1}
  depends_on:
    arc-cache:
      condition: service_healthy
  healthcheck:
    test: ["CMD-SHELL", "wget -qO- http://localhost:7880 || exit 1"]
    interval: 5s
    timeout: 3s
    retries: 5
    start_period: 10s
  networks:
    - arc_platform_net
```

**Note**: `--config /etc/livekit.yaml` passes the full config as a file; environment variable `LIVEKIT_NODE_IP` is interpolated at compose startup for the node IP. If `wget` is absent, fall back to `curl -sf`.

### 2. Sentry — Ingress config

```yaml
# services/realtime/ingress.yaml
api_url: ws://arc-realtime:7880
api_key: devkey
api_secret: devsecret
health_port: 7888
```

```yaml
# arc-realtime-ingress in docker-compose.yml
arc-realtime-ingress:
  image: ghcr.io/arc-framework/arc-realtime-ingress:latest
  build:
    context: services/realtime
    dockerfile: Dockerfile.ingress
  volumes:
    - ./services/realtime/ingress.yaml:/etc/ingress.yaml:ro
  ports:
    - "127.0.0.1:7888:7888"
    - "127.0.0.1:1935:1935"
  depends_on:
    arc-realtime:
      condition: service_healthy
  healthcheck:
    test: ["CMD-SHELL", "wget -qO- http://localhost:7888 || exit 1"]
    interval: 5s
    timeout: 3s
    retries: 5
    start_period: 10s
  networks:
    - arc_platform_net
```

### 3. Scribe — Egress config

```yaml
# services/realtime/egress.yaml
api_url: ws://arc-realtime:7880
api_key: devkey
api_secret: devsecret
# S3-compatible output (arc-storage / MinIO)
s3:
  endpoint: http://arc-storage:9000
  bucket: recordings
  access_key: arc
  secret: arc-minio-dev
  force_path_style: true
health_port: 7889
```

```yaml
# arc-realtime-egress in docker-compose.yml
arc-realtime-egress:
  image: ghcr.io/arc-framework/arc-realtime-egress:latest
  build:
    context: services/realtime
    dockerfile: Dockerfile.egress
  volumes:
    - ./services/realtime/egress.yaml:/etc/egress.yaml:ro
  ports:
    - "127.0.0.1:7889:7889"
  depends_on:
    arc-realtime:
      condition: service_healthy
  healthcheck:
    test: ["CMD-SHELL", "wget -qO- http://localhost:7889 || exit 1"]
    interval: 5s
    timeout: 3s
    retries: 5
    start_period: 10s
  networks:
    - arc_platform_net
```

**Note**: Scribe doesn't hard-depend on `arc-storage` in compose (storage optional for dev; egress falls back gracefully for recording requests). This matches the spec edge case: egress starts, recording requests fail until storage is healthy.

### 4. Non-root handling

LiveKit images (server, ingress, egress) are Go binaries. Check each image's effective user:

```bash
docker run --rm --entrypoint id livekit/livekit-server
docker run --rm --entrypoint id livekit/ingress
docker run --rm --entrypoint id livekit/egress
```

* If non-root already → add `USER` line in Dockerfile for clarity; no extra steps
* If root → document deviation in Dockerfile comment (same pattern as Nick Fury): *"livekit/livekit-server runs as root by default. This is a DEVELOPMENT-ONLY service. For production, build with non-root user."*

### 5. realtime.mk — multi-image .mk design

Three Dockerfiles in one directory means the `.mk` handles three build targets:

```makefile
COMPOSE_REALTIME     := docker compose -f services/realtime/docker-compose.yml
REALTIME_IMAGE       := $(REGISTRY)/$(ORG)/arc-realtime
REALTIME_INGRESS_IMG := $(REGISTRY)/$(ORG)/arc-realtime-ingress
REALTIME_EGRESS_IMG  := $(REGISTRY)/$(ORG)/arc-realtime-egress

realtime-up:   $(COMPOSE_REALTIME) up -d arc-realtime arc-realtime-ingress arc-realtime-egress
realtime-down: $(COMPOSE_REALTIME) down

realtime-health: check arc-realtime :7880 + arc-realtime-ingress :7888 + arc-realtime-egress :7889

realtime-build:         # builds arc-realtime (Dockerfile)
realtime-ingress-build: # builds arc-realtime-ingress (Dockerfile.ingress)
realtime-egress-build:  # builds arc-realtime-egress (Dockerfile.egress)

realtime-publish:         realtime-build + realtime-push
realtime-ingress-publish: realtime-ingress-build + realtime-ingress-push
realtime-egress-publish:  realtime-egress-build + realtime-egress-push
```

Individual `realtime-ingress-up` and `realtime-egress-up` start only those services (useful for dev restart without cycling all three).

### 6. CI/CD — single path filter, three build jobs

`realtime-images.yml` — one path trigger (`services/realtime/**`), three parallel build steps:

```yaml
on:
  push:
    branches: [main]
    paths: ["services/realtime/**"]

jobs:
  build-realtime:       # builds arc-realtime
  build-realtime-ingress: # builds arc-realtime-ingress
  build-realtime-egress:  # builds arc-realtime-egress
```

`realtime-release.yml` — tag format `realtime/vX.Y.Z` → Docker tag `realtime-vX.Y.Z`; multi-platform amd64 + arm64.

### 7. profiles.yaml update

```yaml
think:
  services:
    - messaging
    - cache
    - streaming
    - friday-collector
    - cortex
    - sql-db
    - vector-db
    - gateway
    - realtime    # ← ADD: voice/WebRTC infrastructure

reason:
  services:
    - cortex
    - messaging
    - streaming
    - cache
    - otel
    - storage
    - vault
    - flags
    - realtime    # ← ADD: voice/WebRTC infrastructure

# ultra-instinct: '*' — already includes all services; no change needed
```

### 8. service.yaml — primary service only

`service.yaml` represents the primary service (Daredevil). Sentry and Scribe are referenced in `docker-compose.yml` depends\_on but not in a separate `service.yaml`:

```yaml
name: arc-realtime
codename: daredevil
role: realtime
image: ghcr.io/arc-framework/arc-realtime:latest
tech: livekit
upstream: livekit/livekit-server
ports:
  - { port: 7880, protocol: http, description: "LiveKit API + WebRTC signalling" }
  - { port: 7881, protocol: grpc, description: "gRPC API" }
  - { port: 7882, protocol: tcp, description: "TURN/TCP" }
  - { port: "50100-50200", protocol: udp, description: "WebRTC RTP media" }
health:
  endpoint: http://localhost:7880
  interval: 5s
depends_on:
  - cache  # Redis — multi-node pub/sub state
sidecars:
  - { name: arc-realtime-ingress, codename: sentry, port: 7888, description: "RTMP ingest" }
  - { name: arc-realtime-egress, codename: scribe, port: 7889, description: "Recording + export" }
```

## Constitution Check

| # | Principle | Status | Evidence |
|---|-----------|--------|----------|
| I | Zero-Dep CLI | N/A | No CLI changes |
| II | Platform-in-a-Box | PASS | `make realtime-up` boots all three; joined to `think` (minimal) profile |
| III | Modular Services | PASS | Self-contained in `services/realtime/`; own service.yaml, compose, .mk |
| IV | Two-Brain | PASS | Config-only upstream images — no Python or Go custom code |
| V | Polyglot Standards | PASS | Same Dockerfile/compose/healthcheck/CI structure as 003/005/006 |
| VI | Local-First | N/A | CLI only |
| VII | Observability | PASS | HTTP health endpoints :7880/:7888/:7889; Docker healthchecks |
| VIII | Security | PASS† | TCP 127.0.0.1; UDP 0.0.0.0 exception documented; no secrets in git; non-root verified per image |
| IX | Declarative | N/A | CLI only |
| X | Stateful Ops | N/A | CLI only |
| XI | Resilience | PASS | `depends_on` ordering: cache → realtime → ingress/egress; start\_periods for healthchecks |
| XII | Interactive | N/A | CLI only |

†**Security note**: UDP `0.0.0.0:50100-50200` is a documented, intentional exception required for WebRTC NAT traversal. All other ports bind `127.0.0.1`.

## Project Structure

```
arc-platform/
├── services/
│   ├── realtime/                               ← NEW (entire directory)
│   │   ├── Dockerfile                          # FROM livekit/livekit-server; OCI + arc labels; non-root verify
│   │   ├── Dockerfile.ingress                  # FROM livekit/ingress; OCI + arc labels
│   │   ├── Dockerfile.egress                   # FROM livekit/egress; OCI + arc labels
│   │   ├── livekit.yaml                        # LiveKit server config; devkey/devsecret; Redis; RTC ports
│   │   ├── ingress.yaml                        # Ingress config; api_url ws://arc-realtime:7880; health :7888
│   │   ├── egress.yaml                         # Egress config; api_url; S3 → arc-storage; health :7889
│   │   ├── docker-compose.yml                  # All 3 services; arc_platform_net external; depends_on
│   │   ├── service.yaml                        # role: realtime, codename: daredevil, depends_on: cache, sidecars
│   │   └── realtime.mk                         # realtime-* + realtime-ingress-* + realtime-egress-* targets
│   └── profiles.yaml                           # MODIFY — think += realtime; reason += realtime
├── .github/workflows/
│   ├── realtime-images.yml                     ← NEW
│   └── realtime-release.yml                    ← NEW
├── Makefile                                    # MODIFY — include services/realtime/realtime.mk
└── scripts/
    ├── scripts.mk                              # MODIFY — add realtime/ingress/egress to publish-all
    └── lib/
        └── check-dev-prereqs.sh                # MODIFY — add ports 7880, 7881, 1935
```

## Parallel Execution Strategy

```mermaid
gantt
    title 007-voice-stack Implementation Phases
    dateFormat X
    axisFormat %s

    section Phase 1 — Foundation
    Update profiles.yaml (realtime → think + reason)    :p1, 0, 1

    section Phase 2 — Dockerfiles + Config (parallel)
    Dockerfile (arc-realtime)                           :p2a, after p1, 2
    Dockerfile.ingress (arc-realtime-ingress)           :p2b, after p1, 2
    Dockerfile.egress (arc-realtime-egress)             :p2c, after p1, 2
    livekit.yaml + ingress.yaml + egress.yaml           :p2d, after p1, 2

    section Phase 3 — Compose + service.yaml
    docker-compose.yml (all 3 services)                 :p3, after p2a, 1
    service.yaml                                        :p3b, after p2a, 1

    section Phase 4 — Make + CI (parallel)
    realtime.mk (all targets)                           :p4a, after p3, 1
    realtime-images.yml                                 :p4b, after p3, 2
    realtime-release.yml                                :p4c, after p3, 2

    section Phase 5 — Integration
    Makefile include + scripts updates                  :p5a, after p4a, 1
    make realtime-up + make realtime-health             :p5b, after p5a, 1
    Docs & links update                                 :p5c, after p5b, 1
```

**Parallelizable groups:**

* Phase 2: All three Dockerfiles and all three config YAML files are fully independent
* Phase 4: `realtime-images.yml` + `realtime-release.yml` are independent of each other
* Phase 3: `docker-compose.yml` + `service.yaml` can be written concurrently (different files)

## Tech Debt

| ID | Item | Rationale |
|----|------|-----------|
| TD-001 | Vault integration for LiveKit API keys | Static `devkey`/`devsecret` in config files. Production requires dynamic secrets from OpenBao. Deferred to security hardening spec. |
| TD-002 | LIVEKIT\_NODE\_IP for remote clients | Default `127.0.0.1` only works locally. Remote/cloud deploy requires setting `LIVEKIT_NODE_IP` to machine's public IP. Document in help target. |
| TD-003 | TURN/STUN for firewall traversal | `node_ip: 127.0.0.1` works on LAN. True TURN server for clients behind strict NAT requires a turnserver sidecar (e.g. coturn). Deferred. |
| TD-004 | MinIO bucket init for recordings | Scribe writes to `recordings` bucket in arc-storage. Bucket must exist. A future init script or MinIO lifecycle policy should pre-create it. |
| TD-005 | LiveKit egress requires `chrome` | LK Egress uses a headless Chromium for composite room recording. The `livekit/egress` image bundles it but is ~1GB. Future: switch to track-based egress (no Chrome) if size is a concern. |

## Reviewer Checklist

* \[ ] `make realtime-up` exits 0; all three containers in `healthy` state
* \[ ] `make realtime-health` exits 0
* \[ ] `make realtime-down` stops all three; no orphaned containers
* \[ ] `make realtime-up` starts `arc-cache` first if not already running (depends\_on)
* \[ ] `curl -s http://localhost:7880` returns response from LiveKit Server
* \[ ] `curl -s http://localhost:7888` returns 200 from Ingress controller
* \[ ] `curl -s http://localhost:7889` returns 200 from Egress controller
* \[ ] TCP ports 7880/7881/7882/7888/1935/7889 bind `127.0.0.1` only — verify with `docker compose ps`
* \[ ] UDP `50100-50200` binds `0.0.0.0` — verify and confirm documented
* \[ ] `livekit.yaml` mounted read-only at `/etc/livekit.yaml` in arc-realtime container
* \[ ] `ingress.yaml` and `egress.yaml` mounted read-only in their respective containers
* \[ ] API key consistency: `devkey`/`devsecret` used identically in livekit.yaml, ingress.yaml, egress.yaml
* \[ ] `LIVEKIT_NODE_IP` documented in `realtime-help` output; defaults to `127.0.0.1`
* \[ ] Non-root status verified for each image; deviation documented if root required
* \[ ] All three Dockerfiles have OCI + `arc.service.*` labels including `arc.service.codename`
* \[ ] `services/realtime/service.yaml` contains role, codename, image, ports, health, depends\_on, sidecars
* \[ ] `services/profiles.yaml` `think` includes `realtime`; `reason` includes `realtime`
* \[ ] Root Makefile includes `services/realtime/realtime.mk`
* \[ ] `publish-all` in scripts/scripts.mk includes all three realtime build/publish steps
* \[ ] `check-dev-prereqs.sh` checks ports 7880, 7881, 1935
* \[ ] `realtime-images.yml` path filter `services/realtime/**`; builds all three images
* \[ ] `realtime-release.yml` tag format `realtime/v*`; multi-platform amd64 + arm64
* \[ ] `CLAUDE.md` Service Codenames table updated — Daredevil (LiveKit), Sentry (LK Ingress), Scribe (LK Egress)
* \[ ] `.specify/config.yaml` `realtime` entry updated with correct codename/role

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| LiveKit images run as root | M | Document deviation in Dockerfile + help text; same approach as Nick Fury in 006 |
| `wget` absent in livekit images | M | Fall back to `curl -sf http://localhost:PORT` in healthcheck — Go images often include curl |
| UDP 50100-50200 blocked by macOS firewall | H | Document in `realtime-help`; user must allow UDP range for WebRTC to function |
| Port 1935 (RTMP) in use by OBS or similar | M | Document in edge cases; user checks `make dev-status` and stops conflicting process |
| Egress `recordings` bucket missing in MinIO | M | Document in TD-004; manual `mc mb` command in `realtime-help` until init script exists |
| LK Egress Chrome binary makes image ~1GB | L | Accepted for dev; noted in TD-005; not a blocker |
| `arc_platform_net` not pre-created | H | `realtime-up` calls `docker network create arc_platform_net 2>/dev/null \|\| true` before compose up |
| `LIVEKIT_NODE_IP` env expansion in livekit.yaml | M | LiveKit supports env var expansion in YAML config — verify `${LIVEKIT_NODE_IP:-127.0.0.1}` syntax is supported; fallback to compose `environment:` injection if not |

---

---
url: /arc-platform/specs-site/008-specs-site/plan.md
---
# Implementation Plan: Specs Documentation Site

> **Spec**: 008-specs-site
> **Date**: 2026-03-01

## Summary

Publish `specs/` as a searchable, mermaid-rendered static site using MkDocs Material, deployed to
GitHub Pages via CI on every push to `main`. Navigation is driven by
`mkdocs-awesome-pages-plugin` — each spec folder gets a `.pages` file with its human-readable
section title; adding a new spec requires no changes to `mkdocs.yml`.

## Target Modules

| Module | Language | Changes |
|--------|----------|---------|
| `docs/specs/` | YAML / Markdown / Python | New — `mkdocs.yml`, `index.md`, `requirements.txt` |
| `specs/` | Markdown / YAML | Additive-only — `.pages` files per folder; no existing files modified |
| `.github/workflows/` | YAML | New — `deploy-specs-site.yml` |
| Root | Text | `.gitignore` + `CLAUDE.md` — 2-line updates |

## Technical Context

| Aspect | Value |
|--------|-------|
| Build tool | MkDocs Material ≥ 9.5 |
| Nav plugin | `mkdocs-awesome-pages-plugin` ≥ 2.9 |
| Mermaid | `pymdownx.superfences` custom fence + mermaid.js CDN (unpkg.com/mermaid@10) |
| Search | MkDocs built-in `search` plugin (`lang: en`) |
| Python | 3.10+ locally; 3.12 in CI |
| Content source | `docs_dir: ../../specs/` — zero duplication |
| Build output | `site_dir: ../../site_build/` — gitignored |
| Deploy target | `gh-pages` branch → GitHub Pages |
| CI trigger | `push` to `main` when `specs/**` or `docs/specs/**` change |

### Key Decision: MkDocs Material vs alternatives

| Option | Verdict |
|--------|---------|
| **MkDocs Material** | ✅ Best-in-class markdown site, mermaid native, Python (consistent with SDK) |
| Docusaurus | Node.js runtime — conflicts with zero-dep philosophy for tooling |
| Sphinx | RST-first; markdown support is bolted on; overkill for spec docs |
| GitHub wiki | No mermaid, no search, no CI control |

### Key Decision: awesome-pages vs hardcoded nav

`mkdocs.yml` `nav:` would need updating every time a spec is added — brittle and error-prone in
a CI context. `awesome-pages` reads `.pages` files from each folder; adding a spec = adding a
`.pages` file in the same PR, no `mkdocs.yml` change needed.

### Key Decision: mermaid rendering

`pymdownx.superfences` custom fence + CDN `mermaid.min.js` is the canonical MkDocs Material
approach. No separate `mkdocs-mermaid2-plugin` needed — that plugin has known version conflicts
with Material ≥ 9.0.

## Architecture

```mermaid
graph TD
    subgraph PR ["PR → main merge"]
        specs_change["specs/** change\nor docs/specs/** change"]
    end

    subgraph CI ["deploy-specs-site.yml"]
        checkout["actions/checkout@v4"]
        python["actions/setup-python@v5\nPython 3.12"]
        install["pip install -r\ndocs/specs/requirements.txt"]
        deploy["mkdocs gh-deploy --force\n--config-file docs/specs/mkdocs.yml"]
    end

    subgraph Output ["GitHub Pages"]
        ghpages["gh-pages branch\n→ arc-framework.github.io\n/arc-platform/specs-site/"]
    end

    specs_change --> checkout
    checkout --> python --> install --> deploy --> ghpages
```

```mermaid
graph LR
    subgraph source ["Content Source"]
        specs["specs/\n(source of truth)"]
        pages[".pages files\n(nav titles)"]
    end

    subgraph config ["docs/specs/"]
        mkdocs["mkdocs.yml\ndocs_dir: ../../specs/"]
        index["index.md\n(landing page)"]
        req["requirements.txt"]
    end

    subgraph site ["Built Site"]
        search["Full-text search"]
        mermaid["Mermaid SVGs"]
        dark["Dark/Light mode"]
        edit["Edit on GitHub links"]
    end

    specs -->|"zero copy"| mkdocs
    pages --> mkdocs
    mkdocs --> site
```

## Constitution Check

| # | Principle | Status | Evidence |
|---|-----------|--------|----------|
| I | Zero-Dep CLI | N/A | No CLI changes |
| II | Platform-in-a-Box | N/A | Documentation only |
| III | Modular Services | N/A | No service changes |
| IV | Two-Brain | N/A | Config and markdown only |
| V | Polyglot Standards | PASS | Python tooling (MkDocs) consistent with Python SDK/services; commenting conventions followed |
| VI | Local-First | PASS | `mkdocs serve` works fully offline after `pip install`; no external API keys needed |
| VII | Observability | N/A | Documentation only |
| VIII | Security | PASS | No secrets committed; CI uses `GITHUB_TOKEN` auto-credential; no sensitive data in built site |
| IX | Declarative | N/A | No CLI changes |
| X | Stateful Ops | N/A | No CLI changes |
| XI | Resilience | N/A | Static site; GitHub Pages 99.9% SLA |
| XII | Interactive | N/A | No CLI changes |

## Project Structure

```
arc-platform/
├── docs/
│   └── specs/                           # New MkDocs site config
│       ├── mkdocs.yml                   # Main config — docs_dir, theme, plugins, mermaid
│       ├── index.md                     # Landing page — overview + spec index table
│       └── requirements.txt             # Pinned Python deps
│
├── specs/
│   ├── .pages                           # Root nav order
│   ├── 001-otel-setup/
│   │   └── .pages                       # title: "001 — OTEL Setup"; file nav map
│   ├── 002-cortex-setup/
│   │   └── .pages
│   ├── 003-messaging-setup/
│   │   └── .pages
│   ├── 004-dev-setup/
│   │   └── .pages
│   ├── 005-data-layer/
│   │   └── .pages
│   ├── 006-platform-control/
│   │   └── .pages
│   └── 007-voice-stack/
│       └── .pages
│
├── .github/
│   └── workflows/
│       └── deploy-specs-site.yml        # New CI workflow
│
├── .gitignore                           # +1 line: site_build/
└── CLAUDE.md                            # +1 line: mkdocs serve command
```

**Total new files**: 12 (mkdocs.yml, index.md, requirements.txt, 7× .pages, 1× .pages root,
deploy-specs-site.yml)
**Modified files**: 2 (.gitignore, CLAUDE.md)

## `.pages` File Pattern

Each spec folder `.pages` follows this structure:

```yaml
# specs/NNN-feature-name/.pages
title: "NNN — Feature Name"
nav:
  - spec.md: Specification
  - plan.md: Implementation Plan
  - tasks.md: Task Breakdown
  - analysis-report.md: Analysis Report
  - ...
```

Only files that **exist in the folder** should be listed. Files not in `nav:` are still
discoverable via search; they just don't appear in the sidebar. Use `...` (ellipsis entry) as a
catch-all for any unlisted files if needed.

## Parallel Execution Strategy

```mermaid
gantt
    title 008-specs-site Implementation
    dateFormat  X
    axisFormat  %s

    section Phase 1 — Setup (serial)
    gitignore + CLAUDE.md         :a1, 0, 1

    section Phase 2 — Core Files (fully parallel)
    mkdocs.yml                    :a2, after a1, 1
    index.md                      :a3, after a1, 1
    requirements.txt              :a4, after a1, 1
    .pages root + 001–007         :a5, after a1, 1

    section Phase 3 — CI (parallel with Phase 2)
    deploy-specs-site.yml         :a6, after a1, 1

    section Phase 4 — Verification
    mkdocs build + serve + strict :a7, after a2, 1
```

Phase 2 tasks (mkdocs.yml, index.md, requirements.txt, all `.pages`) are fully independent —
different files, no shared state. Phase 3 (CI workflow) can run in parallel with Phase 2.

## Tech Debt

None anticipated for this feature. MkDocs Material is stable; the `awesome-pages` plugin is a
mature community standard. If the spec count grows beyond ~50, consider adding
`mkdocs-gen-files` to auto-generate `.pages` files, but that's premature now.

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| GitHub Pages not enabled in repo settings | H | Document as prerequisite (FR-10); CI will fail clearly with 404 |
| `mkdocs-mermaid2-plugin` version conflict | M | Don't use it — use `pymdownx.superfences` + CDN as specified |
| `docs_dir: ../../specs/` relative path wrong when run from subdirectory | M | Always invoke `mkdocs` from repo root; document in CLAUDE.md |
| Spec with broken mermaid syntax fails build | L | Mermaid errors are browser-side; `mkdocs build --strict` won't catch them (not a hard failure) |
| `.pages` nav lists a file that doesn't exist in the folder | M | `mkdocs build --strict` will error; run strict mode in verification |
| CDN mermaid.js unavailable (offline CI) | L | GitHub Actions runners have internet access; CDN is reliable |
| `pr-description.md` or `analysis-report.md` absent from some folders | L | `awesome-pages` only shows files that exist; missing files simply don't appear |

## Reviewer Checklist

* \[ ] `mkdocs build -f docs/specs/mkdocs.yml` exits 0 from repo root
* \[ ] `mkdocs build --strict -f docs/specs/mkdocs.yml` exits 0 (no broken links/warnings)
* \[ ] `site_build/` directory created after build; gitignored
* \[ ] All 7 spec folders have `.pages` files with human-readable titles
* \[ ] Root `specs/.pages` lists all 7 folders in order
* \[ ] `docs/specs/requirements.txt` pins `mkdocs-material`, `awesome-pages`, `pymdown-extensions`
* \[ ] `docs/specs/index.md` has spec index table with all 7 specs
* \[ ] `deploy-specs-site.yml` triggers on `specs/**` and `docs/specs/**` paths
* \[ ] `deploy-specs-site.yml` uses `permissions: contents: write`
* \[ ] `.gitignore` contains `site_build/`
* \[ ] `CLAUDE.md` contains `mkdocs serve -f docs/specs/mkdocs.yml`
* \[ ] Constitution V (Polyglot) and VI (Local-First) pass
* \[ ] No TODO/FIXME without tracking

---

---
url: /arc-platform/specs-site/001-otel-setup/tasks.md
---
# Tasks: 001-otel-setup — Stand Up SigNoz as the Observability Backend

> **Spec**: 001-otel-setup
> **Date**: 2026-02-21

## Dependency Graph

```mermaid
graph TD
    T001["TASK-001\nScaffold directories"] --> T010
    T001 --> T011

    T010["TASK-010 [P]\nservice.yaml — widow"] --> T020
    T010 --> T021
    T011["TASK-011 [P]\nservice.yaml — friday"] --> T022
    T011 --> T023

    T020["TASK-020 [P]\ntelemetry compose"] --> T030
    T021["TASK-021 [P]\notel-collector-config"] --> T030
    T022["TASK-022 [P]\nobservability compose"] --> T030
    T023["TASK-023 [P]\nobservability configs"] --> T030

    T030["TASK-030\nRoot Makefile otel-* targets"] --> T040

    T040["TASK-040\nE2E validation"] --> T900
    T040 --> T901

    T900["TASK-900 [P]\ntelemetry README"] --> T999
    T901["TASK-901 [P]\nobservability README"] --> T999

    T999["TASK-999\nReviewer verification"]

    style T001 fill:#fff3cd
    style T030 fill:#fff3cd
    style T040 fill:#fff3cd
    style T999 fill:#f8d7da
```

**Critical path**: T001 → T010/T011 → T020-T023 → T030 → T040 → T999

***

## Phase 1: Setup

* \[x] **\[TASK-001]** \[SERVICES] \[P1] Scaffold the `services/otel/` directory tree
  * Dependencies: none
  * Module: `services/otel/`
  * Acceptance:
    * `services/otel/telemetry/` exists with empty `service.yaml`, `docker-compose.yml`, `config/` dir
    * `services/otel/observability/` exists with empty `service.yaml`, `docker-compose.yml`, `config/` dir
    * `services/otel/telemetry/config/otel-collector-config.yaml` placeholder exists
    * `services/otel/observability/config/` contains placeholder files for clickhouse-config.xml and zookeeper.properties

***

## Phase 2: Foundational

*Both tasks are parallel-safe — no shared files.*

* \[x] **\[TASK-010]** \[P] \[SERVICES] \[P1] Write `service.yaml` for telemetry (Black Widow / widow)
  * Dependencies: TASK-001
  * Module: `services/otel/telemetry/service.yaml`
  * Acceptance:
    ```yaml
    codename: widow
    tech: signoz-otel-collector
    upstream: signoz/signoz-otel-collector
    ports:
      - 4317   # OTLP gRPC
      - 4318   # OTLP HTTP
    health: http://localhost:13133/
    ```
  * File validates as valid YAML

* \[x] **\[TASK-011]** \[P] \[SERVICES] \[P1] Write `service.yaml` for observability (Friday / friday)
  * Dependencies: TASK-001
  * Module: `services/otel/observability/service.yaml`
  * Acceptance:
    ```yaml
    codename: friday
    tech: signoz
    depends_on:
      - telemetry
    health: http://localhost:3301/api/v1/health
    compose_services:
      - signoz
      - clickhouse
      - zookeeper
    ```
  * File validates as valid YAML

***

## Phase 3: Implementation

### Parallel Batch A — Service Configs

*All four tasks are parallel-safe after Phase 2.*

* \[x] **\[TASK-020]** \[P] \[SERVICES] \[P1] Write `services/otel/telemetry/docker-compose.yml`
  * Dependencies: TASK-010
  * Module: `services/otel/telemetry/docker-compose.yml`
  * Acceptance:
    * Defines service `signoz-otel-collector` using `signoz/signoz-otel-collector` image
    * Collector ports bound to `127.0.0.1`: `127.0.0.1:4317:4317`, `127.0.0.1:4318:4318`
    * Health check: `curl -sf http://localhost:13133/` with `interval: 10s`, `retries: 5`, `start_period: 30s`
    * Mounts `./config/otel-collector-config.yaml` into container
    * Container runs as non-root user
    * Connected to shared Docker network (e.g. `arc_otel_net`)

* \[x] **\[TASK-021]** \[P] \[SERVICES] \[P1] Write `services/otel/telemetry/config/otel-collector-config.yaml`
  * Dependencies: TASK-010
  * Module: `services/otel/telemetry/config/otel-collector-config.yaml`
  * Acceptance:
    * `receivers.otlp` configured with gRPC on `:4317` and HTTP on `:4318`
    * `exporters.otlp` (or `clickhousetraces`) points at ClickHouse endpoint on the shared Docker network
    * `extensions.health_check` bound to `:13133`
    * `service.pipelines.traces` wires receivers → exporters
    * `service.extensions` lists `health_check`
    * Config is valid YAML; collector starts without errors

* \[x] **\[TASK-022]** \[P] \[SERVICES] \[P1] Write `services/otel/observability/docker-compose.yml`
  * Dependencies: TASK-011
  * Module: `services/otel/observability/docker-compose.yml`
  * Acceptance:
    * Three services defined: `zookeeper`, `clickhouse`, `signoz`
    * `clickhouse` depends on `zookeeper` with `condition: service_healthy`
    * `signoz` depends on `clickhouse` with `condition: service_healthy`
    * **Security**: ClickHouse has NO `ports` key (internal network only)
    * **Security**: ZooKeeper has NO `ports` key (internal network only)
    * **Security**: SigNoz UI bound to `127.0.0.1:3301:3301` (not `0.0.0.0`)
    * SigNoz health check: `curl -sf http://localhost:3301/api/v1/health` with `interval: 15s`, `retries: 8`, `start_period: 60s`
    * ClickHouse health check probes readiness (e.g. `clickhouse-client --query "SELECT 1"`)
    * All containers run as non-root user
    * Mounts `./config/clickhouse-config.xml` and `./config/zookeeper.properties`
    * Connected to same shared Docker network as telemetry

* \[x] **\[TASK-023]** \[P] \[SERVICES] \[P1] Write observability config files (ClickHouse + ZooKeeper)
  * Dependencies: TASK-011
  * Module: `services/otel/observability/config/`
  * Acceptance:
    * `clickhouse-config.xml`: sets max memory to 4GB; disables anonymous usage stats; includes comment documenting 4GB minimum requirement
    * `zookeeper.properties`: basic ZK coordination config (dataDir, clientPort, tickTime)
    * Both files are valid format and cause their respective containers to start without errors

### Parallel Batch B — Orchestration

*Single blocking task — waits for all Batch A.*

* \[x] **\[TASK-030]** \[MAKEFILE] \[P1] Write root `Makefile` with `otel-*` targets
  * Dependencies: TASK-020, TASK-021, TASK-022, TASK-023
  * Module: `arc-platform/Makefile` (new file)
  * Acceptance — variables:
    ```makefile
    COMPOSE                    := docker compose
    COMPOSE_OTEL_TELEMETRY     := $(COMPOSE) -f services/otel/telemetry/docker-compose.yml
    COMPOSE_OTEL_OBSERVABILITY := $(COMPOSE) -f services/otel/observability/docker-compose.yml
    COMPOSE_OTEL               := $(COMPOSE_OTEL_TELEMETRY) \
                                  -f services/otel/observability/docker-compose.yml
    ```
  * Acceptance — targets exist and work:
    * `otel-up`: starts full stack via `$(COMPOSE_OTEL) up -d`
    * `otel-up-observability`: starts `$(COMPOSE_OTEL_OBSERVABILITY) up -d`
    * `otel-up-telemetry`: starts `$(COMPOSE_OTEL_TELEMETRY) up -d`
    * `otel-down`: `$(COMPOSE_OTEL) down`
    * `otel-health`: curls `:13133/` and `:3301/api/v1/health`; exits non-zero if either fails; prints ✓/✗ per endpoint
    * `otel-logs`: `$(COMPOSE_OTEL) logs -f`
    * `otel-ps`: `$(COMPOSE_OTEL) ps`
  * All targets declared `.PHONY`
  * No `cd` commands; all paths relative to repo root

***

## Phase 4: Integration

* \[x] **\[TASK-040]** \[SERVICES] \[P1] End-to-end validation — run the full stack and verify all success criteria
  * Dependencies: TASK-030
  * Module: `arc-platform/` (root; validation only, no file changes)
  * Acceptance — all of the following pass:
    1. `make otel-up` exits 0; all four containers reach `healthy` state (verify via `make otel-ps`)
    2. `make otel-health` exits 0
    3. `curl -sf http://localhost:3301/api/v1/health` returns HTTP 200
    4. `curl -sf http://localhost:13133/` returns HTTP 200
    5. `nc -z localhost 9000` fails (ClickHouse NOT exposed)
    6. `nc -z localhost 2181` fails (ZooKeeper NOT exposed)
    7. Test span via telemetrygen appears in SigNoz UI → Traces within 30s:
       ```
       docker run --rm --network host \
         ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:latest \
         traces --otlp-insecure --otlp-endpoint localhost:4317
       ```
    8. `docker compose exec <service> whoami` returns non-root for all containers
    9. `make otel-down` exits 0 and removes all containers

***

## Phase 5: Polish

*Both README tasks are parallel-safe.*

* \[x] **\[TASK-900]** \[P] \[DOCS] \[P1] Write `services/otel/telemetry/README.md`
  * Dependencies: TASK-040
  * Module: `services/otel/telemetry/README.md`
  * Acceptance:
    * Quickstart: `make otel-up-telemetry` command
    * Open ports documented: 4317 (OTLP gRPC), 4318 (OTLP HTTP), 13133 (health)
    * Health check URL: `http://localhost:13133/`
    * Collector config reference: where to find it, what receivers are open, what the export target is
    * No dead links

* \[x] **\[TASK-901]** \[P] \[DOCS] \[P1] Write `services/otel/observability/README.md`
  * Dependencies: TASK-040
  * Module: `services/otel/observability/README.md`
  * Acceptance:
    * **Prerequisites section** — documents: Docker Desktop with ≥ 4GB RAM allocated to ClickHouse
    * Quickstart: `make otel-up` from repo root
    * UI URL: `http://localhost:3301`
    * Health check URL: `http://localhost:3301/api/v1/health`
    * Note: ClickHouse and ZooKeeper are internal-only (not accessible from host)
    * No dead links

* \[x] **\[TASK-999]** \[REVIEW] \[P1] Reviewer agent verification
  * Dependencies: TASK-900, TASK-901 (all tasks)
  * Module: all affected modules
  * Acceptance — reviewer agent confirms all of the following:
    * All tasks in this file marked `[x]` complete
    * Directory structure matches plan: `services/otel/telemetry/`, `services/otel/observability/`
    * `make otel-up` starts all 4 containers and reaches healthy state
    * `make otel-health` exits 0
    * All success criteria SC-1 through SC-7 from spec.md verified
    * All containers confirmed non-root
    * SigNoz UI bound to `127.0.0.1` (confirmed in docker-compose.yml)
    * ClickHouse and ZooKeeper have no `ports` key in their compose config
    * No secrets or credentials in any committed file
    * Both READMEs exist with required sections
    * Constitution Principles II, III, VII, VIII — all PASS
    * No TODO/FIXME without a tracking issue

***

## Phase 6: Image Strategy & Naming

*Codifies the arc-friday-* naming convention and Approach C image strategy from platform-spike.\*

* \[x] **\[TASK-050]** \[SERVICES] \[P1] Fix broken hostname references baked into collector image configs
  * Dependencies: none (prerequisite for otel-build to produce working images)
  * Module: `services/otel/telemetry/config/`, `services/otel/observability/config/`
  * Acceptance:
    * `otel-collector-config.yaml`: `clickhouse:9000` → `arc-friday-clickhouse:9000`
    * `otel-collector-opamp-config.yaml`: `signoz:4320` → `arc-friday:4320`
    * `clickhouse-cluster.xml`: `<host>clickhouse</host>` → `<host>arc-friday-clickhouse</host>`; `<host>zookeeper</host>` → `<host>arc-friday-zookeeper</host>`
    * `make otel-build` produces images; `make otel-up` shows all 6 containers reach healthy/exited-0 state

* \[x] **\[TASK-051]** \[SERVICES] \[P1] Rename all containers and images to `arc-friday-*` in docker-compose.yml and Makefile
  * Dependencies: TASK-050
  * Module: `services/otel/docker-compose.yml`, `Makefile`
  * Acceptance:
    * `docker ps` shows: `arc-friday`, `arc-friday-collector`, `arc-friday-clickhouse`, `arc-friday-zookeeper`, `arc-friday-migrator-sync` (exited 0), `arc-friday-migrator-async` (exited 0)
    * Images: `ghcr.io/arc-framework/arc-friday:latest`, `ghcr.io/arc-framework/arc-friday-collector:latest`, etc.
    * All DSNs use `tcp://arc-friday-clickhouse:9000`
    * Volumes named `arc-friday-clickhouse`, `arc-friday-zookeeper`, `arc-friday-sqlite`

* \[x] **\[TASK-052]** \[CI] \[P1] Update `.github/workflows/otel-images.yml` matrix to use new image names
  * Dependencies: TASK-051
  * Module: `.github/workflows/otel-images.yml`
  * Acceptance:
    * Matrix contains: `arc-friday`, `arc-friday-collector`, `arc-friday-clickhouse`, `arc-friday-zookeeper`
    * Each entry has correct `context` and `dockerfile` path
    * Workflow produces images at `ghcr.io/arc-framework/arc-friday-*`

* \[x] **\[TASK-053]** \[CI] \[P] Create `.github/config/publish-observability.json`
  * Dependencies: TASK-051
  * Module: `.github/config/publish-observability.json`
  * Acceptance:
    * JSON has `images` array with 4 entries: `arc-friday`, `arc-friday-collector`, `arc-friday-clickhouse`, `arc-friday-zookeeper`
    * Each entry has `source`, `target`, `description`, `platforms`, `required` fields
    * `settings.rate_limit_delay_seconds` is set
    * Consumed by `_reusable-publish-group.yml` without errors

* \[x] **\[TASK-054]** \[CI] \[P] Port reusable CI workflows from platform-spike
  * Dependencies: none (parallel-safe)
  * Module: `.github/workflows/`
  * Acceptance:
    * `_reusable-build.yml`: reusable Docker build with GHA caching, SBOM generation, size + duration tracking
    * `_reusable-security.yml`: reusable Trivy scan (fs + image), SARIF upload, non-blocking by default
    * `_reusable-publish-group.yml`: reusable vendor re-tag, reads JSON config, sequential publish with rate limit delay
    * All three pass YAML linting (`yamllint` or GitHub Actions validation)

* \[x] **\[TASK-055]** \[DOCS] \[P] Update `spec.md` with FR-13..16 and Image Strategy section
  * Dependencies: TASK-051
  * Module: `specs/001-otel-setup/spec.md`
  * Acceptance:
    * FR-13 through FR-16 added to Requirements section
    * Image Strategy section added before Edge Cases
    * Table of 4 images with source, Dockerfile, and baked-in config
    * No dead file references

***

## Progress Summary

| Phase | Total | Done | Parallel |
|-------|-------|------|----------|
| Setup | 1 | 1 | 0 |
| Foundational | 2 | 2 | 2 |
| Implementation | 5 | 5 | 4 |
| Integration | 1 | 1 | 0 |
| Polish | 3 | 3 | 2 |
| Image Strategy | 6 | 6 | 3 |
| **Total** | **18** | **18** | **11** |

---

---
url: /arc-platform/specs-site/002-cortex-setup/tasks.md
---
# Tasks: Cortex Bootstrap Service

> **Spec**: 002-cortex-setup
> **Date**: 2026-02-22

## Task Format

```
[TASK-NNN] [P?] [MODULE] [PRIORITY] Description
  Dependencies: [TASK-XXX] or none
  Module: services/cortex/...
  Acceptance: Testable criteria
  Status: [ ] pending | [~] in-progress | [x] done
```

* `[P]` = Safe for parallel agent execution
* Priority: P1 (must), P2 (should), P3 (nice)

## Dependency Graph

```mermaid
graph TD
    T001 --> T010
    T010 --> T011
    T011 --> T012
    T012 --> T013

    T013 --> T020
    T013 --> T021
    T013 --> T022
    T013 --> T023

    T020 --> T030
    T021 --> T030
    T022 --> T030
    T023 --> T030

    T030 --> T031

    T031 --> T040
    T031 --> T041

    T040 --> T900
    T041 --> T900
    T900 --> T999
    T041 --> T999
```

## Quality Requirements

| Module | Coverage | Lint | Notes |
|--------|----------|------|-------|
| `internal/orchestrator` | 75% | golangci-lint | Critical — P1 |
| `internal/clients` | 75% | golangci-lint | Critical — P1 |
| `internal/api` | 60% | golangci-lint | Core — P1 |
| `internal/config` | 60% | golangci-lint | Core — P2 |
| `internal/telemetry` | 40% | golangci-lint | Infra — P2 |

***

## Phase 1: Setup

* \[x] \[TASK-001] \[SERVICES] \[P1] Scaffold `services/cortex/` directory structure and `go.mod`
  * Dependencies: none
  * Module: `services/cortex/`
  * Acceptance: All directories from spec exist; `go mod init arc-framework/cortex` succeeds; `go build ./...` compiles with no errors on an empty `main.go`

* \[x] \[TASK-010] \[SERVICES] \[P1] Implement `internal/config/config.go` — Viper config with typed structs
  * Dependencies: TASK-001
  * Module: `services/cortex/internal/config/config.go`
  * Acceptance: `Config` struct covers all four infra clients + OTEL + server; env var mapping matches `CORTEX_*` prefix; `LoadConfig()` returns typed struct; unit tests for defaults and env override pass

* \[x] \[TASK-011] \[SERVICES] \[P1] Implement `internal/telemetry/friday.go` — OTEL TracerProvider + MeterProvider
  * Dependencies: TASK-010
  * Module: `services/cortex/internal/telemetry/friday.go`
  * Acceptance: `InitProvider(ctx, cfg)` exports traces and metrics via OTLP gRPC to `arc-widow:4317`; shutdown is graceful; OTEL dial failure is non-fatal (`WithFailFast(false)`); unit test verifies provider initialises without panic when collector is unreachable

* \[x] \[TASK-012] \[SERVICES] \[P1] Implement Cobra CLI entry point — `cmd/cortex/` (root, server, bootstrap commands)
  * Dependencies: TASK-010
  * Module: `services/cortex/cmd/cortex/`
  * Acceptance: `cortex server` starts Gin on `:8081`; `cortex bootstrap` executes one-shot bootstrap and exits 0/non-zero; `--config`, `--log-level` global flags wired; `main.go` wires DI with functional options; compiles cleanly

* \[x] \[TASK-013] \[SERVICES] \[P1] Define orchestrator types — `internal/orchestrator/types.go`
  * Dependencies: TASK-001
  * Module: `services/cortex/internal/orchestrator/types.go`
  * Acceptance: `BootstrapResult`, `PhaseResult`, `ProbeResult` structs defined; JSON tags present; `sync.Mutex` embedded in `BootstrapResult` for concurrent phase writes; zero-value is safe to use

***

## Phase 2: Foundational — Infra Clients (Parallel Batch)

### Parallel Batch A — 4 agents, no inter-dependency

* \[x] \[TASK-020] \[P] \[SERVICES] \[P1] Implement `internal/clients/postgres.go` — pgx pool + schema validation + circuit breaker
  * Dependencies: TASK-013
  * Module: `services/cortex/internal/clients/postgres.go`
  * Acceptance: `NewPostgresClient(cfg, cb)` opens pgx pool; `Probe(ctx)` pings and validates schema version table exists; `gobreaker` trips after 3 failures, resets after 30s; table-driven tests with mocked pgx cover success, failure, and open-breaker cases; `golangci-lint` passes

* \[x] \[TASK-021] \[P] \[SERVICES] \[P1] Implement `internal/clients/nats.go` — JetStream stream creation + circuit breaker
  * Dependencies: TASK-013
  * Module: `services/cortex/internal/clients/nats.go`
  * Acceptance: `NewNATSClient(cfg, cb)` connects via NATS; `ProvisionStreams(ctx)` creates 3 streams with correct subjects, retention, and max-age; existing stream is updated not errored; `gobreaker` circuit breaker applied; unit tests pass; `golangci-lint` passes
  * Note: Stream names use SCREAMING\_SNAKE\_CASE convention (`AGENT_COMMANDS`, `AGENT_EVENTS`, `SYSTEM_METRICS`) instead of the original spec placeholder names (`arc-commands`, `arc-events`, `arc-results`). NATS JetStream streams are conventionally uppercased; this is an intentional naming improvement.

* \[x] \[TASK-022] \[P] \[SERVICES] \[P1] Implement `internal/clients/pulsar.go` — Pulsar admin REST + circuit breaker
  * Dependencies: TASK-013
  * Module: `services/cortex/internal/clients/pulsar.go`
  * Acceptance: `NewPulsarClient(cfg, cb)` uses `net/http` to Pulsar admin REST at `:8080`; provisions tenant `arc-system`, 3 namespaces, 3 partitioned topics; 409 on topic creation treated as success (idempotent); `gobreaker` applied; unit tests mock HTTP responses; `golangci-lint` passes

* \[x] \[TASK-023] \[P] \[SERVICES] \[P1] Implement `internal/clients/redis.go` — go-redis PING + circuit breaker
  * Dependencies: TASK-013
  * Module: `services/cortex/internal/clients/redis.go`
  * Acceptance: `NewRedisClient(cfg, cb)` creates go-redis client; `Probe(ctx)` sends `PING` and validates `PONG`; `gobreaker` applied; unit tests cover success and circuit-open cases; `golangci-lint` passes

***

## Phase 3: Implementation

* \[x] \[TASK-030] \[SERVICES] \[P1] Implement `internal/orchestrator/service.go` — `RunBootstrap()` + `RunDeepHealth()`
  * Dependencies: TASK-020, TASK-021, TASK-022, TASK-023
  * Module: `services/cortex/internal/orchestrator/service.go`
  * Acceptance: `RunBootstrap(ctx)` runs all 4 phases via `errgroup` in parallel; each phase writes to `BootstrapResult` under mutex; phase error does not cancel others; `atomic.Bool` gates concurrent bootstrap requests (returns sentinel error when in-progress); OTEL span wraps full execution; table-driven tests achieve ≥ 75% coverage; `golangci-lint` passes

* \[x] \[TASK-031] \[SERVICES] \[P1] Implement `internal/api/` — Gin router, handlers, and OTEL middleware
  * Dependencies: TASK-030
  * Module: `services/cortex/internal/api/`
  * Acceptance: `POST /api/v1/bootstrap` returns 202 on start, 409 if in-progress; `GET /health` always 200; `GET /health/deep` probes all 4 clients; `GET /ready` returns 503 until bootstrap completes, then 200; OTEL trace propagation middleware applied; panic recovery middleware present; ≥ 60% test coverage on handlers; `golangci-lint` passes

***

## Phase 4: Integration

* \[x] \[TASK-040] \[SERVICES] \[P1] Wire `main.go` DI + integration test for bootstrap flow
  * Dependencies: TASK-031
  * Module: `services/cortex/cmd/cortex/main.go`
  * Acceptance: `main.go` constructs all dependencies via functional options and wires them into the Cobra command tree; `go build ./...` produces a valid binary; integration test spins up Gin with mock clients and verifies full `POST /api/v1/bootstrap → 202 → GET /ready → 200` flow; `golangci-lint` passes

* \[x] \[TASK-041] \[P] \[SERVICES] \[P1] Add `service.yaml` and `Dockerfile` for Cortex
  * Dependencies: TASK-031
  * Module: `services/cortex/`
  * Acceptance: `service.yaml` declares `codename: cortex`, `depends_on: [flash, strange, oracle, sonic, widow]`, health endpoint `/health`; Dockerfile uses multi-stage build, non-root `USER cortex`, and exposes `:8081`; `docker build` succeeds; `docker inspect` confirms non-root user

***

## Phase 5: Polish

* \[x] \[TASK-900] \[P] \[DOCS] \[P1] Update platform references — profiles.yaml, config.yaml, codename map, architecture doc
  * Dependencies: TASK-040, TASK-041
  * Module: `services/profiles.yaml`, `.specify/config.yaml`, `.specify/meta/service-codename-map.md`, `.specify/docs/architecture/cortex.md`
  * Acceptance: `cortex` added to `think` and `reason` profiles in `profiles.yaml`; `.specify/config.yaml` updated from `bootstrap/raymond` → `cortex/cortex`; `service-codename-map.md` has Cortex row; `cortex.md` architecture doc links back to `spec.md` and `plan.md`; all internal links resolve

* \[x] \[TASK-999] \[REVIEW] \[P1] Reviewer agent verification — all tasks complete, tests pass, constitution compliant
  * Dependencies: ALL
  * Module: all affected modules
  * Acceptance: All tasks marked `[x]`; `go test ./...` passes with ≥ 75% on orchestrator+clients, ≥ 60% on api; `golangci-lint run` zero errors; `GET /health` responds < 50ms; `docker inspect` confirms non-root user; OTEL service name is `arc-cortex`; `POST /api/v1/bootstrap` returns 409 when in-progress; `cortex bootstrap` exits 0 on success; constitution principles II, III, IV, V, VII, VIII, XI all verified

***

## Progress Summary

| Phase | Total | Done | Parallel |
|-------|-------|------|----------|
| Setup | 5 | 5 | 0 |
| Foundational (Clients) | 4 | 4 | 4 |
| Implementation | 2 | 2 | 0 |
| Integration | 2 | 2 | 1 |
| Polish | 2 | 2 | 1 |
| **Total** | **15** | **15** | **6** |

---

---
url: /arc-platform/specs-site/005-data-layer/tasks.md
---
# Tasks: Data Layer Services Setup

> **Spec**: 005-data-layer
> **Date**: 2026-02-28

## Task Format

```
[TASK-NNN] [P?] [MODULE] [PRIORITY] Description
  Dependencies: [TASK-XXX] or none
  Module: services/{role}
  Acceptance: Testable criteria
  Status: [ ] pending | [~] in-progress | [x] done
```

* `[P]` = Safe for parallel agent execution
* Priority: P1 (must), P2 (should), P3 (nice)

## Dependency Graph

```mermaid
graph TD
    T001["TASK-001\nprofiles.yaml"] --> T011
    T001 --> T012
    T001 --> T013

    T011["TASK-011 [P]\nservices/persistence/\nOracle"] --> T021
    T012["TASK-012 [P]\nservices/vector/\nCerebro"] --> T022
    T013["TASK-013 [P]\nservices/storage/\nTardis"] --> T023

    T021["TASK-021 [P]\noracle.mk"] --> T024
    T022["TASK-022 [P]\ncerebro.mk"] --> T024
    T023["TASK-023 [P]\ntardis.mk"] --> T024

    T024["TASK-024\ndata.mk +\nMakefile includes"] --> T031
    T024 --> T032

    T031["TASK-031 [P]\ndata-images.yml"] --> T041
    T032["TASK-032 [P]\ndata-release.yml"] --> T041

    T041["TASK-041\nE2E verification"] --> T900
    T041 --> T999

    T900["TASK-900\nDocs & links"] --> T999
    T999["TASK-999\nReviewer"]
```

## Quality Requirements

| Module | Coverage | Lint | Notes |
|--------|----------|------|-------|
| Config/YAML | n/a | `docker compose config` (no syntax errors) | |
| Makefile | n/a | `make -n {target}` dry-run passes | |
| CI/CD YAML | n/a | GitHub Actions YAML schema valid | |

***

## Phase 1: Setup

* \[x] \[TASK-001] \[SERVICES] \[P1] Update profiles.yaml — oracle + cerebro → `think`; tardis → `reason`
  * Dependencies: none
  * Module: `services/profiles.yaml`
  * Acceptance:
    * `grep "oracle" services/profiles.yaml` matches a line under `think.services`
    * `grep "cerebro" services/profiles.yaml` matches a line under `think.services`
    * `grep "tardis" services/profiles.yaml` matches a line under `reason.services`
    * `ultra-instinct` entry unchanged (`services: '*'`)
    * File parses as valid YAML: `python3 -c "import yaml,sys; yaml.safe_load(open('services/profiles.yaml'))"` exits 0

***

## Phase 2: Foundational — Service Directories (parallel)

All three tasks are fully independent. Each creates 4 files: `Dockerfile`, `service.yaml`, `docker-compose.yml`, and the `.mk` file is handled in Phase 3.

* \[x] \[TASK-011] \[P] \[SERVICES] \[P1] Create `services/persistence/` — Oracle (Postgres 17)
  * Dependencies: TASK-001
  * Module: `services/persistence/`
  * Acceptance:
    * `Dockerfile`: `FROM postgres:17-alpine`; OCI labels (`org.opencontainers.image.title`, `source`); `arc.service.name=arc-sql-db`, `arc.service.codename=oracle`, `arc.service.tech=postgres`; no USER directive (postgres user uid 70 is default)
    * `service.yaml`: name `arc-sql-db`, codename `oracle`, image `ghcr.io/arc-framework/arc-sql-db:latest`, tech `postgres`, upstream `postgres:17-alpine`, ports `[5432]`, health `pg_isready -U arc`, depends\_on `[]`
    * `docker-compose.yml`: service `arc-sql-db`; env `POSTGRES_USER=arc POSTGRES_PASSWORD=arc POSTGRES_DB=arc`; port `127.0.0.1:5432:5432`; volume `arc-sql-db-data:/var/lib/postgresql/data`; healthcheck `pg_isready -U arc || exit 1` interval 5s, timeout 3s, retries 10, start\_period 10s; network `arc_platform_net` (external); `restart: unless-stopped`
    * `docker compose config` passes without errors
    * `docker build -f services/persistence/Dockerfile services/persistence/` succeeds
  * Reviewer notes (2026-02-28): All acceptance criteria verified. `docker exec arc-sql-db ps aux` confirms `postgres` user (uid 70) runs the postgres process. PASS.

* \[x] \[TASK-012] \[P] \[SERVICES] \[P1] Create `services/vector/` — Cerebro (Qdrant)
  * Dependencies: TASK-001
  * Module: `services/vector/`
  * Acceptance:
    * `Dockerfile`: `FROM qdrant/qdrant`; OCI labels; `arc.service.name=arc-vector-db`, `arc.service.codename=cerebro`, `arc.service.tech=qdrant`; no USER directive (qdrant image runs as uid 1000)
    * `service.yaml`: name `arc-vector-db`, codename `cerebro`, image `ghcr.io/arc-framework/arc-vector-db:latest`, tech `qdrant`, upstream `qdrant/qdrant`, ports `[6333, 6334]`, health `http://localhost:6333/readyz`, depends\_on `[]`
    * `docker-compose.yml`: service `arc-vector-db`; ports `127.0.0.1:6333:6333` and `127.0.0.1:6334:6334`; volume `arc-vector-db-data:/qdrant/storage`; healthcheck `wget -qO- http://localhost:6333/readyz || exit 1` interval 5s, timeout 3s, retries 5, start\_period 5s; network `arc_platform_net` (external); `restart: unless-stopped`
    * `docker compose config` passes without errors
    * `docker build -f services/vector/Dockerfile services/vector/` succeeds
  * Reviewer notes (2026-02-28): BLOCKED — Constitution Principle VIII violation.
    * The `qdrant/qdrant` upstream image declares `USER 0:0` (root) by default. The `USER_ID` build arg defaults to 0. `docker inspect arc-vector-db --format '{{.Config.User}}'` returns `0:0` and `docker exec arc-vector-db whoami` returns `root`.
    * The Dockerfile comment "qdrant image runs as uid 1000 by default" is factually incorrect — the upstream image runs as root unless the image is rebuilt with `--build-arg USER_ID=1000`.
    * Acceptance criterion says "no USER directive (qdrant image runs as uid 1000)" — this is wrong. A `user: "1000:1000"` entry must be added to `services/vector/docker-compose.yml` (same approach as Tardis), or the Dockerfile must rebuild with `--build-arg USER_ID=1000` and add `USER 1000`.
    * Acceptable healthcheck deviation (pre-approved): compose healthcheck uses `bash -c 'exec 3<>/dev/tcp/...'` instead of `wget` because `wget` is not present in the qdrant image. Functional result is identical (HTTP 200 check on /readyz).
  * Re-verification (2026-02-28): PASS. Dockerfile now adds USER root, pre-creates /qdrant/storage + /qdrant/snapshots with chown 1000:1000, then drops to USER 1000. docker-compose.yml adds `user: "1000:1000"`. Verified: `docker inspect arc-vector-db --format '{{.Config.User}}'` = `1000:1000`; `docker exec arc-vector-db id` = `uid=1000 gid=1000 groups=1000`. Principle VIII satisfied. PASS.

* \[x] \[TASK-013] \[P] \[SERVICES] \[P1] Create `services/storage/` — Tardis (MinIO)
  * Dependencies: TASK-001
  * Module: `services/storage/`
  * Acceptance:
    * `Dockerfile`: `FROM minio/minio`; OCI labels; `arc.service.name=arc-storage`, `arc.service.codename=tardis`, `arc.service.tech=minio`; document uid in comment (non-root if supported, otherwise note deviation per 003-Pulsar pattern)
    * `service.yaml`: name `arc-storage`, codename `tardis`, image `ghcr.io/arc-framework/arc-storage:latest`, tech `minio`, upstream `minio/minio`, ports `[9000, 9001]`, health `http://localhost:9000/minio/health/live`, depends\_on `[]`
    * `docker-compose.yml`: service `arc-storage`; command `server /data --console-address ":9001"`; env `MINIO_ROOT_USER=arc MINIO_ROOT_PASSWORD=arc-minio-dev`; ports `127.0.0.1:9000:9000` and `127.0.0.1:9001:9001`; volume `arc-storage-data:/data`; healthcheck `curl -f http://localhost:9000/minio/health/live || exit 1` interval 10s, timeout 5s, retries 5, start\_period 10s; network `arc_platform_net` (external); `restart: unless-stopped`
    * `docker compose config` passes without errors
    * `docker build -f services/storage/Dockerfile services/storage/` succeeds
    * Non-root uid: attempt `user: "1000:1000"` in docker-compose first; if MinIO fails to start, try `USER 1000` in Dockerfile; if both fail, add inline comment `# NOTE: minio/minio requires root — upstream constraint` (same as Pulsar in 003)
  * Reviewer notes (2026-02-28): `user: "1000:1000"` confirmed in compose; `docker inspect arc-storage --format '{{.Config.User}}'` returns `1000:1000`; `docker exec arc-storage id` returns `uid=1000 gid=1000`. Dockerfile pre-creates `/data` owned by uid 1000 and drops to `USER 1000`. PASS.

***

## Phase 3: Make Targets

Batch A (parallel — each .mk is independent):

* \[x] \[TASK-021] \[P] \[SERVICES] \[P1] Create `services/persistence/oracle.mk`
  * Dependencies: TASK-011
  * Module: `services/persistence/oracle.mk`
  * Acceptance:
    * Targets present: `oracle-help`, `oracle-build`, `oracle-build-fresh`, `oracle-up`, `oracle-down`, `oracle-health`, `oracle-logs`, `oracle-push`, `oracle-publish`, `oracle-tag`, `oracle-clean`, `oracle-nuke`
    * `oracle-health`: probes `pg_isready -U arc -h localhost -p 5432`; exits 0 if healthy, 1 if not
    * `oracle-clean` / `oracle-nuke`: require typed confirmation (`yes` / `nuke`) before destructive action
    * `oracle-publish`: pushes image then prints `https://github.com/orgs/$(ORG)/packages/container/arc-sql-db/settings`
    * `make oracle-help` lists all targets with descriptions
    * All targets use `COLOR_INFO`, `COLOR_OK`, `COLOR_ERR` inherited from root Makefile
    * All paths relative to repo root

* \[x] \[TASK-022] \[P] \[SERVICES] \[P1] Create `services/vector/cerebro.mk`
  * Dependencies: TASK-012
  * Module: `services/vector/cerebro.mk`
  * Acceptance:
    * Targets present: `cerebro-help`, `cerebro-build`, `cerebro-build-fresh`, `cerebro-up`, `cerebro-down`, `cerebro-health`, `cerebro-logs`, `cerebro-push`, `cerebro-publish`, `cerebro-tag`, `cerebro-clean`, `cerebro-nuke`
    * `cerebro-health`: probes `wget -qO- http://localhost:6333/readyz`; exits 0 if healthy, 1 if not
    * `cerebro-clean` / `cerebro-nuke`: require typed confirmation before destructive action
    * `cerebro-publish`: pushes then prints settings URL
    * `make cerebro-help` lists all targets
  * Reviewer notes (2026-02-28): All targets present and function correctly. Minor acceptable deviation: `cerebro-health` in the .mk uses `curl -sf` (not `wget`) to probe from the host — functionally identical. PASS.

* \[x] \[TASK-023] \[P] \[SERVICES] \[P1] Create `services/storage/tardis.mk`
  * Dependencies: TASK-013
  * Module: `services/storage/tardis.mk`
  * Acceptance:
    * Targets present: `tardis-help`, `tardis-build`, `tardis-build-fresh`, `tardis-up`, `tardis-down`, `tardis-health`, `tardis-logs`, `tardis-push`, `tardis-publish`, `tardis-tag`, `tardis-clean`, `tardis-nuke`
    * `tardis-health`: probes `curl -f http://localhost:9000/minio/health/live`; exits 0 if healthy, 1 if not
    * `tardis-clean` / `tardis-nuke`: require typed confirmation before destructive action
    * `tardis-publish`: pushes then prints settings URL
    * `make tardis-help` lists all targets

Batch B (sequential — depends on all Batch A):

* \[x] \[TASK-024] \[SERVICES] \[P1] Create `services/data.mk` + update root `Makefile` includes
  * Dependencies: TASK-021, TASK-022, TASK-023
  * Module: `services/data.mk`, `Makefile`
  * Acceptance:
    * `services/data.mk` targets: `data-help`, `data-up`, `data-down`, `data-health`, `data-logs`
    * `data-up`: calls `docker network create arc_platform_net 2>/dev/null || true`, then `oracle-up`, `cerebro-up`, `tardis-up` sequentially
    * `data-down`: calls `oracle-down`, `cerebro-down`, `tardis-down` sequentially
    * `data-health`: calls all three health targets; exits non-zero if any fails
    * `data-logs`: fans out logs from all three containers simultaneously with service name prefixes
    * Root `Makefile`: `include services/persistence/oracle.mk`, `include services/vector/cerebro.mk`, `include services/storage/tardis.mk`, `include services/data.mk` added after existing service includes
    * `make data-help` lists data-\* targets
    * `make -n data-up` dry-run shows correct chain

***

## Phase 4: CI/CD (parallel)

Both workflows are independent and can be implemented concurrently.

* \[x] \[TASK-031] \[P] \[CI] \[P1] Create `.github/workflows/data-images.yml`
  * Dependencies: TASK-024
  * Module: `.github/workflows/data-images.yml`
  * Acceptance:
    * Mirrors `messaging-images.yml` structure exactly
    * `on.push.paths`: `services/persistence/**`, `services/vector/**`, `services/storage/**`, `.github/workflows/data-images.yml`
    * `on.pull_request.paths`: same service paths
    * `on.workflow_dispatch` with `mode` input (`ci` / `release`)
    * `changes` job uses `dorny/paths-filter@v3` with filters `oracle`, `cerebro`, `tardis`
    * `build-oracle`, `build-cerebro`, `build-tardis` jobs: parallel, each uses `_reusable-build.yml`
      * `platforms: linux/amd64`
      * `service-path: services/persistence` / `services/vector` / `services/storage`
    * `security-oracle`, `security-cerebro`, `security-tardis` jobs: run after respective builds; `block-on-failure: false` in CI; `_reusable-security.yml`
    * YAML is valid; `act` dry-run passes if available

* \[x] \[TASK-032] \[P] \[CI] \[P1] Create `.github/workflows/data-release.yml`
  * Dependencies: TASK-024
  * Module: `.github/workflows/data-release.yml`
  * Acceptance:
    * Mirrors `messaging-release.yml` structure exactly
    * `on.push.tags`: `data/v*`
    * `prepare` job: derives `image-tag` (`data/v0.1.0` → `data-v0.1.0`), `version`, `prerelease` outputs
    * `build-oracle`, `build-cerebro`, `build-tardis`: parallel after `prepare`; `platforms: linux/amd64,linux/arm64`; `push-image: true`, `latest-tag: true`, `image-tag: ${{ needs.prepare.outputs.image-tag }}`
    * `security-oracle`, `security-cerebro`, `security-tardis`: `block-on-failure: true`; `create-issues: true` (CRITICAL CVEs block release)
    * `release` job: generates release notes with image table (arc-sql-db, arc-vector-db, arc-storage); creates GitHub release via `softprops/action-gh-release@v2`
    * Release notes include `make data-up` / `make data-health` quick-start
    * YAML is valid

***

## Phase 5: Integration

* \[x] \[TASK-041] \[SERVICES] \[P1] End-to-end verification — data layer up + health
  * Dependencies: TASK-031, TASK-032
  * Module: `services/persistence/`, `services/vector/`, `services/storage/`
  * Acceptance:
    * `docker network create arc_platform_net 2>/dev/null || true` runs without error
    * `make data-up` exits 0; `docker compose ps` shows arc-sql-db, arc-vector-db, arc-storage all in `healthy` state
    * `make data-health` exits 0 (all three health probes pass)
    * `make data-down` exits 0; all three containers stop; no orphaned containers
    * Independent health checks pass: `make oracle-health`, `make cerebro-health`, `make tardis-health`
    * `curl -s http://localhost:6333/readyz` returns HTTP 200
    * `curl -s http://localhost:9000/minio/health/live` returns HTTP 200
    * `docker exec arc-sql-db pg_isready -U arc` exits 0
    * All ports bound to `127.0.0.1`: `docker compose ps` confirms
    * All volumes are named: `docker volume ls | grep arc` shows `arc-sql-db-data`, `arc-vector-db-data`, `arc-storage-data`
    * After `make data-up` + `make cortex-docker-up`: `curl -s http://localhost:8081/health/deep | jq .oracle.status` returns `"ok"`
  * Reviewer notes (2026-02-28): 10 of 11 criteria PASS. BLOCKED on TASK-012 (Cerebro runs as root).
    * Criteria 1-10 all verified and passing in live test.
    * Criterion 11 (Cortex oracle health): `"ok": false` — FAIL. Root cause is a pre-existing mismatch in Cortex config (commit 1d15b07): Cortex defaults `bootstrap.postgres.db=arc_db` and has no password configured, while Oracle compose sets `POSTGRES_DB=arc` and `POSTGRES_PASSWORD=arc`. This is not introduced by 005-data-layer. The implementer must align either Cortex's config or Oracle's env (or both) and document in Cortex's compose. This criterion blocks TASK-041 until resolved.
  * Re-verification (2026-02-28): ALL 11 criteria PASS.
    * Criterion 11 fix: `services/cortex/internal/config/config.go` now sets default `bootstrap.postgres.password=""` and `bootstrap.postgres.db="arc"` (was `arc_db`). `services/cortex/docker-compose.yml` injects `CORTEX_BOOTSTRAP_POSTGRES_PASSWORD: arc`. `curl -s http://localhost:8081/health/deep | jq .dependencies.postgres` = `{"name":"arc-sql-db","ok":true,"latencyMs":17}`. PASS.

***

## Phase 6: Polish

* \[x] \[TASK-900] \[P] \[DOCS] \[P1] Docs & links update
  * Dependencies: TASK-041
  * Module: `services/profiles.yaml`, `CLAUDE.md`, `services/cortex/service.yaml`
  * Acceptance:
    * `services/profiles.yaml` `think` profile includes `oracle` and `cerebro` (already done in TASK-001 — verify final state)
    * `services/profiles.yaml` `reason` profile includes `tardis`
    * `services/cortex/service.yaml` `depends_on` field references `oracle` codename (add if missing)
    * `CLAUDE.md` monorepo layout section references `persistence/`, `vector/`, `storage/` directories (add if missing)
    * No broken internal references in modified files
  * Reviewer notes (2026-02-28): All docs criteria pass. CLAUDE.md monorepo layout lists all three directories; service codenames table updated. cortex/service.yaml depends\_on includes `oracle`. PASS.

* \[x] \[TASK-999] \[REVIEW] \[P1] Reviewer agent verification
  * Dependencies: ALL
  * Module: all affected modules
  * Acceptance (reviewer runs all items from plan.md Reviewer Checklist):
    * All tasks TASK-001 through TASK-900 marked complete
    * `make data-up && make data-health` exits 0
    * `make data-down` clean shutdown
    * Cortex `/health/deep` shows `oracle: ok`
    * All ports bind `127.0.0.1`
    * All volumes are named Docker volumes
    * `profiles.yaml` think includes oracle + cerebro; reason includes tardis
    * `Makefile` includes oracle.mk, cerebro.mk, tardis.mk, data.mk
    * `data-images.yml` path filters cover all three service directories
    * `data-release.yml` tag format `data/v*`; multi-platform builds
    * All Dockerfiles have OCI + `arc.service.*` labels
    * No credentials in any compose file
    * `docker inspect arc-sql-db` confirms postgres uid 70 (non-root default)
    * `docker inspect arc-vector-db` confirms uid 1000 (non-root)
    * MinIO uid documented (non-root or deviation noted in compose comments)
    * Constitution compliance: II, III, IV, V, VII, VIII, XI all PASS
  * Reviewer notes (2026-02-28): BLOCKED — two issues must be resolved before this can be marked done.
    * ISSUE-1 (CRITICAL): `docker inspect arc-vector-db` shows `Config.User=0:0`; runtime `id` returns `uid=0(root)`. The upstream `qdrant/qdrant` image defaults to `USER 0:0`. Principle VIII (non-root containers) is violated. Fix: add `user: "1000:1000"` to `services/vector/docker-compose.yml` and rebuild (same pattern as Tardis), or rebuild Dockerfile with `--build-arg USER_ID=1000` and add `USER 1000`. Correct the Dockerfile comment from "qdrant image runs as uid 1000 by default" to reflect the actual fix.
    * ISSUE-2 (BLOCKER): `curl .../health/deep | jq '.dependencies.postgres.ok'` returns `false`. Root cause: pre-existing mismatch between Cortex defaults (`arc_db`, no password) and Oracle compose (`POSTGRES_DB=arc`, `POSTGRES_PASSWORD=arc`). Fix: align Oracle `POSTGRES_DB` to `arc_db`, or configure `POSTGRES_DB=arc_db` and add `BOOTSTRAP_POSTGRES_PASSWORD=arc` env in `services/cortex/docker-compose.yml`. Needs coordination with Cortex maintainer.
  * Re-verification (2026-02-28): ALL checklist items PASS. Both issues resolved. See TASK-012 and TASK-041 re-verification notes. APPROVED.

***

## Progress Summary

| Phase | Total | Done | Blocked |
|-------|-------|------|---------|
| Setup | 1 | 1 | 0 |
| Foundational | 3 | 3 | 0 |
| Implementation (Make + CI) | 6 | 6 | 0 |
| Integration | 1 | 1 | 0 |
| Polish | 2 | 2 | 0 |
| **Total** | **13** | **13** | **0** |

All tasks complete. Re-verified 2026-02-28.

---

---
url: /arc-platform/specs-site/004-dev-setup/tasks.md
---
# Tasks: Dev Setup Orchestration

> **Spec**: 004-dev-setup
> **Date**: 2026-02-28

## Dependency Graph

```mermaid
graph TD
    T001["T001 · Setup: .make/ + .gitignore"]
    T010["T010 · YAML: profiles.yaml patch"]
    T011["T011 · YAML: cortex/service.yaml patch"]
    T012["T012 · YAML: cache/service.yaml patch"]
    T013["T013 · YAML: otel/service.yaml (new)"]
    T014["T014 · YAML: otel/telemetry/service.yaml (new)"]
    T015["T015 · YAML: otel/observability/service.yaml fix depends_on"]
    T020["T020 · Script: parse-profiles.sh"]
    T021["T021 · Script: parse-registry.sh"]
    T022["T022 · Script: resolve-deps.sh"]
    T023["T023 · Script: wait-for-health.sh"]
    T024["T024 · Script: check-dev-prereqs.sh"]
    T030["T030 · .mk: otel.mk friday-collector aliases"]
    T031["T031 · .mk: cortex.mk cortex-up/down aliases"]
    T040["T040 · Makefile: -include + dev-* targets"]
    T050["T050 · Integration: make dev smoke test"]
    T900["T900 · Docs & links update"]
    T999["T999 · Reviewer agent verification"]

    T001 --> T040
    T010 --> T040
    T011 --> T040
    T012 --> T040
    T013 --> T040
    T014 --> T040
    T015 --> T040
    T020 --> T040
    T021 --> T040
    T022 --> T040
    T023 --> T040
    T024 --> T040
    T030 --> T040
    T031 --> T040
    T040 --> T050
    T050 --> T900
    T050 --> T999
    T900 --> T999
```

## Quality Requirements

| Module | Coverage | Lint | Notes |
|--------|----------|------|-------|
| scripts/lib/ | bash -n syntax check | shellcheck (if available) | All 5 scripts must be executable |
| services/ | n/a | YAML valid (awk-parseable) | Matches expected service.yaml schema |
| Makefile | n/a | make -n dry-run | All dev-\* targets declared .PHONY |

***

## Phase 1: Setup

* \[x] \[TASK-001] \[P] \[ROOT] \[P1] Create `.make/` directory placeholder + add to `.gitignore`
  * Dependencies: none
  * Module: `.gitignore`, `Makefile` (`.make` dir rule only)
  * Acceptance: `.make/` appears in `.gitignore`; `mkdir -p .make` rule exists in Makefile

***

## Phase 2: Foundational

*No blocking type/interface definitions needed — this feature is scripts + YAML + Make. All Phase 2 items run in parallel.*

### Parallel Batch A — YAML Patches

* \[x] \[TASK-010] \[P] \[SERVICES] \[P1] Add `friday-collector` to `think` profile in `services/profiles.yaml`
  * Dependencies: none
  * Module: `services/profiles.yaml`
  * Acceptance: `think` profile services list includes `friday-collector`; `parse-profiles.sh` emits `friday-collector` in `PROFILE_THINK_SERVICES`

* \[x] \[TASK-011] \[P] \[SERVICES] \[P1] Remove `oracle` from `services/cortex/service.yaml` `depends_on`
  * Dependencies: none
  * Module: `services/cortex/service.yaml`
  * Acceptance: `oracle` no longer listed in `depends_on`; remaining deps: `flash`, `strange`, `sonic`, `friday-collector`

* \[x] \[TASK-012] \[P] \[SERVICES] \[P1] Update `services/cache/service.yaml` health to `docker exec arc-cache redis-cli ping`
  * Dependencies: none
  * Module: `services/cache/service.yaml`
  * Acceptance: `health` field reads `docker exec arc-cache redis-cli ping`; `wait-for-health.sh` can use this without host redis-cli

* \[x] \[TASK-015] \[P] \[SERVICES] \[P1] Fix `services/otel/observability/service.yaml` — correct broken `depends_on` codename
  * Dependencies: none
  * Module: `services/otel/observability/service.yaml`
  * Acceptance: `depends_on` changed from `[telemetry]` to `[friday-collector]`; codename `friday-collector` matches the actual entry in `services/otel/telemetry/service.yaml`; `resolve-deps.sh` can resolve `friday` → `friday-collector` without "unregistered dependency" error

* \[x] \[TASK-013] \[P] \[SERVICES] \[P2] Create `services/otel/service.yaml` — full OTEL stack metadata
  * Dependencies: none
  * Module: `services/otel/service.yaml`
  * Acceptance: File exists with `codename: otel`, ports 3301/8080/9000, health endpoint for full SigNoz stack

* \[x] \[TASK-014] \[P] \[SERVICES] \[P2] Ensure `services/otel/telemetry/service.yaml` has correct metadata (add `name`, `timeout` fields if missing)
  * Dependencies: none
  * Module: `services/otel/telemetry/service.yaml`
  * Acceptance: File has `codename: friday-collector`, `health: http://localhost:13133/`, `timeout: 60`; valid for `parse-registry.sh`

### Parallel Batch B — Parsing Scripts

* \[x] \[TASK-020] \[P] \[SCRIPTS] \[P1] Write `scripts/lib/parse-profiles.sh` — `services/profiles.yaml` → `.make/profiles.mk`
  * Dependencies: none
  * Module: `scripts/lib/parse-profiles.sh`
  * Acceptance: `bash -n scripts/lib/parse-profiles.sh` passes; script is executable; output contains `PROFILE_THINK_SERVICES := flash sonic strange friday-collector cortex` and `ALL_PROFILES := think reason ultra-instinct`; `ultra-instinct` `services: '*'` expands to all registered services

* \[x] \[TASK-021] \[P] \[SCRIPTS] \[P1] Write `scripts/lib/parse-registry.sh` — `service.yaml` glob → `.make/registry.mk`
  * Dependencies: none
  * Module: `scripts/lib/parse-registry.sh`
  * Acceptance: `bash -n` passes; executable; output contains `ALL_SERVICES`, `SERVICE_<n>_HEALTH`, `SERVICE_<n>_DEPENDS`, `SERVICE_<n>_TIMEOUT` for all `service.yaml` files found under `services/`

### Parallel Batch C — Orchestration Scripts

* \[x] \[TASK-022] \[P] \[SCRIPTS] \[P1] Write `scripts/lib/resolve-deps.sh` — Kahn's topological sort
  * Dependencies: none
  * Module: `scripts/lib/resolve-deps.sh`
  * Acceptance: `bash -n` passes; executable; given `flash sonic strange friday-collector cortex` outputs two layers (`flash sonic strange friday-collector` then `cortex`); exits 1 with message on unregistered dep; exits 1 with message on cycle

* \[x] \[TASK-023] \[P] \[SCRIPTS] \[P1] Write `scripts/lib/wait-for-health.sh` — HTTP + command health poller
  * Dependencies: none
  * Module: `scripts/lib/wait-for-health.sh`
  * Acceptance: `bash -n` passes; executable; HTTP endpoints polled via `curl -sf`; command endpoints via `eval`; times out after `[timeout]` seconds and exits 1 with service name in message; prints `✓ <codename> healthy` on success

* \[x] \[TASK-024] \[P] \[SCRIPTS] \[P1] Write `scripts/lib/check-dev-prereqs.sh` — Docker + port prereq checks
  * Dependencies: none
  * Module: `scripts/lib/check-dev-prereqs.sh`
  * Acceptance: `bash -n` passes; executable; checks Docker daemon running, `docker compose` v2 available, ports 4222 6379 6650 8082 13133 8081 free; prints colored checklist; exits 1 with `✗ <check> failed` for first failure; exits 0 when all pass

### Parallel Batch D — .mk Alias Targets

* \[x] \[TASK-030] \[P] \[SERVICES] \[P1] Add `friday-collector-up/down/health` alias targets to `services/otel/otel.mk`
  * Dependencies: none
  * Module: `services/otel/otel.mk`
  * Acceptance: `friday-collector-up` chains to `otel-up-telemetry`; `friday-collector-down` stops only `arc-friday-collector` container; `friday-collector-health` curls `http://localhost:13133/` and prints `✓`/`✗`; all three declared `.PHONY`

* \[x] \[TASK-031] \[P] \[SERVICES] \[P1] Add `cortex-up/down` alias targets to `services/cortex/cortex.mk`
  * Dependencies: none
  * Module: `services/cortex/cortex.mk`
  * Acceptance: `cortex-up` chains to `cortex-docker-up` as Make prerequisite (not shell); `cortex-down` chains to `cortex-docker-down`; both declared `.PHONY`

***

## Phase 3: Implementation

* \[x] \[TASK-040] \[MAKEFILE] \[P1] Update root `Makefile` — `-include`, `.make/` generation rules, `dev-*` targets
  * Dependencies: TASK-001, TASK-010, TASK-011, TASK-012, TASK-013, TASK-014, TASK-020, TASK-021, TASK-022, TASK-023, TASK-024, TASK-030, TASK-031
  * Module: `Makefile`
  * Acceptance:
    * `-include .make/profiles.mk` and `-include .make/registry.mk` present
    * `.make/profiles.mk` rule: prerequisite on `services/profiles.yaml`, recipe calls `scripts/lib/parse-profiles.sh > $@`
    * `.make/registry.mk` rule: prerequisite on `$(shell find services -name service.yaml)`, recipe calls `scripts/lib/parse-registry.sh > $@`
    * `.make` directory rule present
    * `dev` target chain: `dev-prereqs dev-networks .make/profiles.mk .make/registry.mk dev-up dev-wait`
    * `dev-up`: iterates layers from `resolve-deps.sh`, starts each service in layer via `<codename>-up`
    * `dev-wait`: calls `wait-for-health.sh` for each service in profile
    * `dev-down`, `dev-health`, `dev-logs`, `dev-status`, `dev-clean`, `dev-prereqs`, `dev-networks`, `dev-regen` all implemented and declared `.PHONY`
    * `PROFILE ?= think` default set
    * All existing targets (`flash-up`, `otel-up`, `cortex-docker-up`) still work — `make -n flash-up` succeeds

***

## Phase 4: Integration

* \[x] \[TASK-050] \[INTEGRATION] \[P1] Smoke test: `make dev` on `think` profile
  * Dependencies: TASK-040
  * Module: All (`Makefile`, `scripts/lib/`, `services/`)
  * Acceptance (Reviewer Checklist SC-1 through SC-9):
    * SC-1: `make dev` exits 0 for `think` profile
    * SC-2: `make dev-health` exits 0, shows 5 `✓` services
    * SC-3: `make dev-down` leaves no `arc-*` containers running
    * SC-4: `make dev PROFILE=reason` starts SigNoz at `:3301`
    * SC-5: Injecting unregistered `depends_on` causes `make dev` to exit 1 with correct message
    * SC-6: `make flash-up`, `make otel-up`, `make cortex-docker-up` still work independently
    * SC-7: Touching `profiles.yaml` causes `.make/profiles.mk` to rebuild on next `make dev`
    * SC-8: Touching any `service.yaml` causes `.make/registry.mk` to rebuild on next `make dev`
    * SC-9: With Docker stopped, `make dev-prereqs` exits 1 with `✗ Docker daemon not running`

***

## Phase 5: Polish

* \[x] \[TASK-900] \[P] \[DOCS] \[P1] Docs & links update
  * Dependencies: TASK-050
  * Module: `specs/004-dev-setup/`, `README.md` (if dev workflow section exists)
  * Acceptance: Any internal spec cross-links valid; if root `README.md` has a "Getting Started" or "Development" section, `make dev` usage is documented there

* \[x] \[TASK-999] \[REVIEW] \[P1] Reviewer agent verification
  * Dependencies: ALL
  * Module: All affected modules
  * Acceptance: All tasks complete and marked `[x]`; all 5 scripts pass `bash -n`; all 5 scripts are executable; `.make/` in `.gitignore`; `make dev` exits 0; `make dev-health` shows 5 ✓; constitution compliance confirmed (Principles II, III, V, VII, VIII, IX); no regressions on existing Make targets

***

## Progress Summary

| Phase | Total | Done | Parallel |
|-------|-------|------|----------|
| Setup | 1 | 1 | 1 |
| Foundational | 11 | 11 | 11 |
| Implementation | 1 | 1 | 0 |
| Integration | 1 | 1 | 0 |
| Polish | 2 | 2 | 1 |
| **Total** | **16** | **16** | **13** |

---

---
url: /arc-platform/specs-site/003-messaging-setup/tasks.md
---
# Tasks: Messaging & Cache Services Setup

> **Spec**: 003-messaging-setup
> **Date**: 2026-02-23

## Task Format

```
[TASK-NNN] [P?] [MODULE] [PRIORITY] Description
  Dependencies: [TASK-XXX] or none
  Module: services/{role}
  Acceptance: Testable criteria
  Status: [ ] pending | [~] in-progress | [x] done
```

* `[P]` = Safe for parallel agent execution
* Priority: P1 (must), P2 (should), P3 (nice)

## Dependency Graph

```mermaid
graph TD
    T001 --> T011
    T001 --> T012
    T001 --> T013

    T011 --> T021
    T012 --> T022
    T013 --> T023

    T021 --> T024
    T022 --> T024
    T023 --> T024

    T011 --> T031
    T012 --> T031
    T031 --> T032
    T032 --> T033

    T024 --> T041
    T024 --> T042

    T033 --> T051
    T041 --> T051
    T042 --> T051

    T051 --> T900
    T051 --> T999
    T900 --> T999
```

## Quality Requirements

| Module | Coverage | Lint | Notes |
|--------|----------|------|-------|
| Services (config) | n/a | `docker compose config` validates | No Go/Python code |
| Dockerfiles | n/a | `docker build` succeeds | Security: trivy scan |
| Workflows | n/a | `actionlint` or manual review | |

***

## Phase 1: Setup

### TASK-001 — Update profiles.yaml

* \[x] \[TASK-001] \[SERVICES] \[P1] Add flash, strange, sonic to `think` profile in profiles.yaml
  * Dependencies: none
  * Module: `services/profiles.yaml`
  * Acceptance:
    * `think` profile lists `flash`, `strange`, `sonic` alongside `cortex`
    * `reason` profile unchanged (it extends think implicitly via the CLI)
    * `cat services/profiles.yaml` shows all three codenames under think
  * Status: \[x] done

***

## Phase 2: Service Directories (fully parallel)

> All three tasks are independent. Dispatch as parallel agents.

### TASK-011 \[P] — Flash (NATS) service files

* \[x] \[TASK-011] \[P] \[SERVICES] \[P1] Create `services/messaging/` — NATS Dockerfile, service.yaml, docker-compose.yml
  * Dependencies: TASK-001
  * Module: `services/messaging/`
  * Files to create:
    * `services/messaging/Dockerfile`
    * `services/messaging/service.yaml`
    * `services/messaging/docker-compose.yml`
  * Acceptance:
    * Dockerfile `FROM nats:alpine`
    * `docker build -t arc-messaging:test services/messaging/` exits 0
    * `docker inspect arc-messaging:test` — User field is non-root (nats user, uid 1000)
    * `docker inspect arc-messaging:test` — Labels include `arc.service.codename=flash`, `arc.service.tech=nats`
    * `docker compose -f services/messaging/docker-compose.yml config` exits 0
    * `service.yaml` contains: `name: arc-messaging`, `codename: flash`, `port: 4222`, `health: http://localhost:8222/healthz`
    * Container `arc-messaging` in compose binds ports to `127.0.0.1` only
    * Named volume `arc-messaging-jetstream` declared (not bind mount)
    * Network declared as `arc_platform_net` with `external: true`
  * Status: \[x] done

### TASK-012 \[P] — Strange (Pulsar) service files

* \[x] \[TASK-012] \[P] \[SERVICES] \[P1] Create `services/streaming/` — Pulsar Dockerfile, service.yaml, docker-compose.yml
  * Dependencies: TASK-001
  * Module: `services/streaming/`
  * Files to create:
    * `services/streaming/Dockerfile`
    * `services/streaming/service.yaml`
    * `services/streaming/docker-compose.yml`
  * Acceptance:
    * Dockerfile `FROM apachepulsar/pulsar:latest`
    * `docker build -t arc-streaming:test services/streaming/` exits 0
    * `docker inspect arc-streaming:test` — Labels include `arc.service.codename=strange`, `arc.service.tech=pulsar`
    * `docker compose -f services/streaming/docker-compose.yml config` exits 0
    * `service.yaml` contains: `name: arc-streaming`, `codename: strange`, `port: 6650`, health endpoint
    * Compose: `PULSAR_MEM: "-Xms512m -Xmx1024m -XX:MaxDirectMemorySize=512m"` and `--no-functions-worker`
    * Compose: port `6650` binds `127.0.0.1:6650`, admin `8080` maps to `127.0.0.1:8082`
    * Named volume `arc-streaming-data` declared
    * Health check uses `curl -f http://localhost:8080/admin/v2/brokers/health` with `start_period: 90s`
    * Pulsar non-root deviation documented in compose comment (upstream constraint)
    * Network `arc_platform_net` with `external: true`
  * Status: \[x] done

### TASK-013 \[P] — Sonic (Redis) service files

* \[x] \[TASK-013] \[P] \[SERVICES] \[P1] Create `services/cache/` — Redis Dockerfile, service.yaml, docker-compose.yml
  * Dependencies: TASK-001
  * Module: `services/cache/`
  * Files to create:
    * `services/cache/Dockerfile`
    * `services/cache/service.yaml`
    * `services/cache/docker-compose.yml`
  * Acceptance:
    * Dockerfile `FROM redis:alpine`
    * `docker build -t arc-cache:test services/cache/` exits 0
    * `docker inspect arc-cache:test` — User field is non-root
    * `docker inspect arc-cache:test` — Labels include `arc.service.codename=sonic`, `arc.service.tech=redis`
    * `docker compose -f services/cache/docker-compose.yml config` exits 0
    * `service.yaml` contains: `name: arc-cache`, `codename: sonic`, `port: 6379`, health endpoint
    * Compose command includes: `--appendonly yes`, `--appendfsync everysec`, `--maxmemory 512mb`, `--maxmemory-policy noeviction`
    * Port `6379` binds `127.0.0.1:6379`
    * Named volume `arc-cache-data` declared
    * Health check: `redis-cli ping` with `interval: 5s`, `start_period: 5s`
    * Network `arc_platform_net` with `external: true`
  * Status: \[x] done

***

## Phase 3: Make Targets (parallel per service, then aggregate)

> TASK-021, 022, 023 are independent. Dispatch in parallel. TASK-024 waits for all three.

### TASK-021 \[P] — flash.mk

* \[x] \[TASK-021] \[P] \[SERVICES] \[P1] Create `services/messaging/flash.mk` with all make targets
  * Dependencies: TASK-011
  * Module: `services/messaging/flash.mk`
  * Acceptance:
    * `make flash-build` — builds `arc-messaging:latest` image
    * `make flash-up` — starts `arc-messaging` container; exits 0
    * `make flash-health` — probes `http://localhost:8222/healthz`; exits 0 when healthy, non-zero when down
    * `make flash-logs` — tails arc-messaging container logs
    * `make flash-down` — stops and removes arc-messaging container
    * `make flash-clean` — stops container + removes named volume (with confirmation prompt)
    * All targets listed in `.PHONY`
    * Target comments follow `## flash-<target>: description` pattern (used by `make flash-help`)
    * `make flash-help` prints all flash targets
  * Status: \[x] done

### TASK-022 \[P] — strange.mk

* \[x] \[TASK-022] \[P] \[SERVICES] \[P1] Create `services/streaming/strange.mk` with all make targets
  * Dependencies: TASK-012
  * Module: `services/streaming/strange.mk`
  * Acceptance:
    * `make strange-build` — builds `arc-streaming:latest` image
    * `make strange-up` — starts `arc-streaming`; waits for health (Pulsar takes up to 90s on cold start); prints progress dots
    * `make strange-health` — probes `http://localhost:8082/admin/v2/brokers/health`; exits 0 when healthy
    * `make strange-logs` — tails arc-streaming container logs
    * `make strange-down` — stops container
    * `make strange-clean` — stops container + removes named volume (with confirmation)
    * `make strange-help` prints all strange targets
    * All targets in `.PHONY`
  * Status: \[x] done

### TASK-023 \[P] — sonic.mk

* \[x] \[TASK-023] \[P] \[SERVICES] \[P1] Create `services/cache/sonic.mk` with all make targets
  * Dependencies: TASK-013
  * Module: `services/cache/sonic.mk`
  * Acceptance:
    * `make sonic-build` — builds `arc-cache:latest` image
    * `make sonic-up` — starts `arc-cache`; exits 0
    * `make sonic-health` — runs `docker exec arc-cache redis-cli ping`; exits 0 when healthy
    * `make sonic-logs` — tails arc-cache container logs
    * `make sonic-down` — stops container
    * `make sonic-clean` — stops + removes volume (with confirmation)
    * `make sonic-help` prints all sonic targets
    * All targets in `.PHONY`
  * Status: \[x] done

### TASK-024 — messaging.mk aggregates + Makefile wiring

* \[x] \[TASK-024] \[SERVICES] \[P1] Create `services/messaging.mk` aggregates and wire all .mk files into root Makefile
  * Dependencies: TASK-021, TASK-022, TASK-023
  * Module: `services/messaging.mk`, `Makefile`
  * Files to create/modify:
    * Create: `services/messaging.mk`
    * Modify: `Makefile`
  * Acceptance:
    * `services/messaging.mk` contains:
      * `messaging-up`: creates `arc_platform_net` if missing (`docker network create arc_platform_net 2>/dev/null || true`), then calls `flash-up`, `strange-up`, `sonic-up`
      * `messaging-down`: calls `flash-down`, `strange-down`, `sonic-down`
      * `messaging-health`: calls `flash-health`, `strange-health`, `sonic-health`; exits non-zero if any fails
      * `messaging-logs`: fans out `docker logs -f` for all three containers simultaneously
      * `messaging-help`: prints all messaging targets
    * `Makefile` includes:
      * `include services/messaging/flash.mk`
      * `include services/streaming/strange.mk`
      * `include services/cache/sonic.mk`
      * `include services/messaging.mk`
    * `make help` shows `messaging-help` entry
    * `make messaging-help` works from repo root
  * Status: \[x] done

***

## Phase 4: OTEL Integration (sequential)

### TASK-031 — Update otel-collector-config.yaml

* \[x] \[TASK-031] \[SERVICES] \[P1] Add prometheus receiver to `services/otel/telemetry/config/otel-collector-config.yaml`
  * Dependencies: TASK-011, TASK-012
  * Module: `services/otel/telemetry/config/otel-collector-config.yaml`
  * Acceptance:
    * `receivers` section contains a `prometheus` block with two scrape jobs:
      * `job_name: arc-messaging` targeting `arc-messaging:8222`
      * `job_name: arc-streaming` targeting `arc-streaming:8080`
      * Both with `scrape_interval: 15s`
    * `service.pipelines.metrics.receivers` updated from `[otlp]` to `[otlp, prometheus]`
    * File is valid YAML: `python3 -c "import yaml; yaml.safe_load(open('services/otel/telemetry/config/otel-collector-config.yaml'))"` exits 0
    * No other pipelines changed (traces and logs pipelines untouched)
  * Status: \[x] done

### TASK-032 — Add arc\_platform\_net to otel collector compose

* \[x] \[TASK-032] \[SERVICES] \[P1] Add `arc_platform_net` to `arc-friday-collector` in `services/otel/docker-compose.yml`
  * Dependencies: TASK-031
  * Module: `services/otel/docker-compose.yml`
  * Acceptance:
    * `arc-friday-collector` service has both `arc_otel_net` and `arc_platform_net` in its `networks` list
    * `arc_platform_net` declared at the bottom of the file as `external: true`
    * `arc_otel_net` definition unchanged (still internal, not external)
    * `docker compose -f services/otel/docker-compose.yml config` exits 0
    * No other services in the otel compose gain `arc_platform_net` (only the collector bridges both)
  * Status: \[x] done

### TASK-033 — Rebuild and verify otel collector image

* \[x] \[TASK-033] \[SERVICES] \[P1] Rebuild `arc-friday-collector` image to bake in updated config; verify it starts
  * Dependencies: TASK-032
  * Module: `services/otel/`
  * Acceptance:
    * `make otel-build` (or `docker compose -f services/otel/docker-compose.yml build arc-friday-collector`) exits 0
    * `docker run --rm ghcr.io/arc-framework/arc-friday-collector:latest --config=/etc/otelcol/config.yaml validate` exits 0 (or equivalent config validation)
    * If otel stack is running locally: `make otel-down && make otel-up` brings collector back healthy with new config
    * `docker inspect ghcr.io/arc-framework/arc-friday-collector:latest` shows updated image (new layer for config)
  * Status: \[x] done

***

## Phase 5: CI/CD Workflows (parallel)

### TASK-041 \[P] — messaging-images.yml

* \[x] \[TASK-041] \[P] \[CI] \[P1] Create `.github/workflows/messaging-images.yml` — CI pipeline for all three services
  * Dependencies: TASK-024
  * Module: `.github/workflows/messaging-images.yml`
  * Acceptance:
    * Triggers on `push` to `main`/`dev` with path filters per service (`services/messaging/**`, `services/streaming/**`, `services/cache/**`)
    * Triggers on `pull_request` to `main` (build only, no push)
    * Triggers on `workflow_dispatch` with `mode: ci | release`
    * `changes` job uses `dorny/paths-filter@v3` — outputs `flash`, `strange`, `sonic`, `push-image`
    * Three parallel build jobs (`build-flash`, `build-strange`, `build-sonic`), each gated on its respective change output
    * All three jobs use `_reusable-build.yml` with `platforms: linux/amd64` (no QEMU)
    * `push-image: true` only on `main` push or `mode=release`
    * `security` job runs after all three builds; `block-on-failure: false` in CI
    * File is valid YAML; mirrors `cortex-images.yml` structure
  * Status: \[x] done

### TASK-042 \[P] — messaging-release.yml

* \[x] \[TASK-042] \[P] \[CI] \[P1] Create `.github/workflows/messaging-release.yml` — release pipeline on `messaging/v*` tags
  * Dependencies: TASK-024
  * Module: `.github/workflows/messaging-release.yml`
  * Acceptance:
    * Triggers on `push` with tag pattern `messaging/v*`
    * `prepare` job derives `image-tag` (`messaging/v0.1.0` → `messaging-v0.1.0`), `version`, `prerelease` flag
    * Three build jobs (`build-flash`, `build-strange`, `build-sonic`) run in parallel after `prepare`
    * All three use `_reusable-build.yml` with `platforms: linux/amd64,linux/arm64`, `push-image: true`, `latest-tag: true`
    * `security` job runs after all three builds; `block-on-failure: true`, `create-issues: true`
    * `release` job creates GitHub release with image reference table (mirrors `cortex-release.yml` release notes format)
    * `permissions: contents: write, packages: write, security-events: write, issues: write`
    * File is valid YAML
  * Status: \[x] done

***

## Phase 6: Integration

### TASK-051 — End-to-end health verification

* \[x] \[TASK-051] \[SERVICES] \[P1] Verify full stack: messaging-up, health checks, otel metrics flow
  * Dependencies: TASK-033, TASK-041, TASK-042
  * Module: `services/` (runtime verification, no file changes)
  * Acceptance:
    * `docker network create arc_platform_net 2>/dev/null || true` — network exists
    * `make messaging-up` exits 0; all three containers in `healthy` state (`docker compose ps` or `docker inspect`)
    * `make messaging-health` exits 0
    * `make flash-health` exits 0 independently
    * `make strange-health` exits 0 independently (allow up to 90s on cold start)
    * `make sonic-health` exits 0 independently
    * `make messaging-down` stops all three; no orphaned containers (`docker ps -a | grep arc-messaging` etc.)
    * `make messaging-up` is idempotent (run twice, no error on second run)
    * If otel stack running: `curl -s http://localhost:8222/metrics` returns NATS Prometheus data; collector scrape visible in SigNoz
  * Status: \[x] done

***

## Phase 7: Polish

### TASK-900 — Docs & links update

* \[x] \[TASK-900] \[P] \[DOCS] \[P1] Docs & links update
  * Dependencies: TASK-051
  * Module: `services/profiles.yaml`, `services/cortex/service.yaml`, `CLAUDE.md`
  * Acceptance:
    * `services/cortex/service.yaml` `depends_on` codenames (`flash`, `strange`, `sonic`) match the `name` fields in each new `service.yaml`
    * `CLAUDE.md` service codename table reflects: messaging=flash/nats, streaming=strange/pulsar, cache=sonic/redis
    * No broken cross-references introduced
  * Status: \[x] done

### TASK-999 — Reviewer agent verification

* \[x] \[TASK-999] \[REVIEW] \[P1] Reviewer agent verifies all tasks complete, quality gates met
  * Dependencies: ALL
  * Module: all affected modules
  * Acceptance:
    * All TASK-001 through TASK-900 marked `[x]` done
    * `make messaging-up && make messaging-health` exits 0
    * All Dockerfiles build without error
    * `docker compose config` exits 0 for all three new compose files and the updated otel compose
    * `messaging-images.yml` and `messaging-release.yml` are valid YAML
    * No secrets or credentials in any committed file
    * Pulsar non-root deviation is documented inline (not a silent violation)
    * Constitution compliance: II, III, VII, VIII, XI all passing per plan.md checklist
  * Status: \[x] done

***

## Progress Summary

| Phase | Total | Done | Parallel |
|-------|-------|------|----------|
| Setup | 1 | 1 | 0 |
| Service Directories | 3 | 3 | 3 |
| Make Targets | 4 | 4 | 3 |
| OTEL Integration | 3 | 3 | 0 |
| CI/CD Workflows | 2 | 2 | 2 |
| Integration | 1 | 1 | 0 |
| Polish | 2 | 2 | 1 |
| **Total** | **16** | **16** | **9** |

---

---
url: /arc-platform/specs-site/006-platform-control/tasks.md
---
# Tasks: Platform Control Plane Services Setup

> **Spec**: 006-platform-control
> **Date**: 2026-02-28

## Task Format

```
[TASK-NNN] [P?] [MODULE] [PRIORITY] Description
  Dependencies: [TASK-XXX] or none
  Module: services/{role}
  Acceptance: Testable criteria
  Status: [ ] pending | [~] in-progress | [x] done
```

* `[P]` = Safe for parallel agent execution
* Priority: P1 (must), P2 (should), P3 (nice)

## Dependency Graph

```mermaid
graph TD
    T001["TASK-001\nprofiles.yaml"] --> T011
    T001 --> T012
    T001 --> T013

    T011["TASK-011 [P]\nservices/gateway/\nHeimdall"] --> T021
    T012["TASK-012 [P]\nservices/secrets/\nNick Fury"] --> T022
    T013["TASK-013 [P]\nservices/flags/\nMystique"] --> T023

    T021["TASK-021 [P]\nheimdall.mk"] --> T024
    T022["TASK-022 [P]\nnick-fury.mk"] --> T024
    T023["TASK-023 [P]\nmystique.mk"] --> T024

    T024["TASK-024\ncontrol.mk +\nMakefile includes"] --> T031
    T024 --> T032

    T031["TASK-031 [P]\ncontrol-images.yml"] --> T041
    T032["TASK-032 [P]\ncontrol-release.yml"] --> T041

    T041["TASK-041\nE2E verification"] --> T900
    T041 --> T999

    T900["TASK-900\nDocs & links"] --> T999
    T999["TASK-999\nReviewer"]
```

## Quality Requirements

| Module | Coverage | Lint | Notes |
|--------|----------|------|-------|
| Config/YAML | n/a | `docker compose config` (no syntax errors) | |
| Makefile | n/a | `make -n {target}` dry-run passes | |
| CI/CD YAML | n/a | GitHub Actions YAML schema valid | |

***

## Phase 1: Setup

* \[x] \[TASK-001] \[SERVICES] \[P1] Update profiles.yaml — heimdall → `think`; nick-fury + mystique → `reason`
  * Dependencies: none
  * Module: `services/profiles.yaml`
  * Acceptance:
    * Python YAML validation confirms structure (not just text presence):
      ```
      python3 -c "
      import yaml
      p = yaml.safe_load(open('services/profiles.yaml'))
      assert 'heimdall' in p['think']['services'], 'heimdall missing from think'
      assert 'nick-fury' in p['reason']['services'], 'nick-fury missing from reason'
      assert 'mystique' in p['reason']['services'], 'mystique missing from reason'
      assert p['ultra-instinct']['services'] == '*', 'ultra-instinct changed'
      print('profiles.yaml OK')
      "
      ```
    * Existing services in `think` preserved: flash, sonic, strange, friday-collector, cortex, oracle, cerebro
    * Existing services in `reason` preserved: cortex, flash, strange, sonic, otel, tardis

***

## Phase 2: Foundational — Service Directories (parallel)

All three tasks are fully independent. Each creates: `Dockerfile`, `service.yaml`, `docker-compose.yml`. Make targets handled in Phase 3.

* \[x] \[TASK-011] \[P] \[SERVICES] \[P1] Create `services/gateway/` — Heimdall (Traefik v3)
  * Dependencies: TASK-001
  * Module: `services/gateway/`
  * Acceptance:
    * `Dockerfile`: `FROM traefik:v3`; OCI labels (`org.opencontainers.image.title`, `description`, `source`); `arc.service.name=arc-gateway`, `arc.service.codename=heimdall`, `arc.service.tech=traefik`; `USER 1000` (non-root via non-privileged internal ports)
    * `service.yaml`: name `arc-gateway`, codename `heimdall`, image `ghcr.io/arc-framework/arc-gateway:latest`, tech `traefik`, upstream `traefik:v3`, ports `[80, 8090]`, health `http://localhost:8090/ping`, depends\_on `[]`
    * `docker-compose.yml`: service `arc-gateway`; command includes `--api.insecure=true`, `--api.dashboard=true`, `--ping=true`, `--providers.docker=true`, `--providers.docker.network=arc_platform_net`, `--providers.docker.exposedbydefault=false`, `--entrypoints.web.address=:8080`, `--entrypoints.dashboard.address=:8090`; ports `127.0.0.1:80:8080` and `127.0.0.1:8090:8090`; volumes `/var/run/docker.sock:/var/run/docker.sock:ro`; `user: "1000:1000"`; healthcheck `traefik healthcheck --ping` interval 5s, timeout 3s, retries 5, start\_period 5s; no persistent volume; network `arc_platform_net` (external); `restart: unless-stopped`
    * `docker compose config` passes without errors
    * `docker build -f services/gateway/Dockerfile services/gateway/` succeeds
    * Note: if `USER 1000` causes Traefik to fail at runtime, remove `USER 1000` and document root deviation in Dockerfile comment (same Pulsar pattern from 003)

* \[x] \[TASK-012] \[P] \[SERVICES] \[P1] Create `services/secrets/` — Nick Fury (OpenBao)
  * Dependencies: TASK-001
  * Module: `services/secrets/`
  * Acceptance:
    * `Dockerfile`: `FROM openbao/openbao`; OCI labels; `arc.service.name=arc-vault`, `arc.service.codename=nick-fury`, `arc.service.tech=openbao`; **Dockerfile MUST contain** this exact comment (or equivalent): `# openbao/openbao runs as root by default. Nick Fury uses -dev mode: in-memory, auto-unsealed, known root token. Root is acceptable for this DEVELOPMENT-ONLY service. For production: Raft-backed storage + TLS + non-root user.`
    * `service.yaml`: name `arc-vault`, codename `nick-fury`, image `ghcr.io/arc-framework/arc-vault:latest`, tech `openbao`, upstream `openbao/openbao`, ports `[8200]`, health `http://localhost:8200/v1/sys/health`, depends\_on `[]`
    * `docker-compose.yml`: service `arc-vault`; command `server -dev`; env `VAULT_DEV_ROOT_TOKEN_ID=arc-dev-token`, `VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200`; port `127.0.0.1:8200:8200`; no volume (stateless dev); healthcheck: attempt `wget -qO- http://localhost:8200/v1/sys/health || exit 1`; if `wget` absent use bash `/dev/tcp` pattern (same as arc-vector-db); interval 5s, timeout 3s, retries 5, start\_period 5s; network `arc_platform_net` (external); `restart: unless-stopped`
    * `docker compose config` passes without errors
    * `docker build -f services/secrets/Dockerfile services/secrets/` succeeds
    * No volume declared (Nick Fury is stateless in dev — data lost on restart is expected and documented)

* \[x] \[TASK-013] \[P] \[SERVICES] \[P1] Create `services/flags/` — Mystique (Unleash)
  * Dependencies: TASK-001
  * Module: `services/flags/`, `services/persistence/initdb/`
  * Acceptance:
    * `Dockerfile`: `FROM unleashorg/unleash-server`; OCI labels; `arc.service.name=arc-flags`, `arc.service.codename=mystique`, `arc.service.tech=unleash`; attempt `USER 1000` (non-root); if Unleash fails to start, remove and add comment: "# NOTE: unleashorg/unleash-server requires root — upstream constraint"
    * `service.yaml`: name `arc-flags`, codename `mystique`, image `ghcr.io/arc-framework/arc-flags:latest`, tech `unleash`, upstream `unleashorg/unleash-server`, ports `[4242]`, health `http://localhost:4242/health`, depends\_on `[oracle, sonic]`
    * `docker-compose.yml`: service `arc-flags`; env `DATABASE_URL=postgresql://arc:arc@arc-sql-db:5432/unleash`, `REDIS_HOST=arc-cache`, `REDIS_PORT=6379`; port `127.0.0.1:4242:4242`; no persistent volume; healthcheck `wget -qO- http://localhost:4242/health || exit 1` interval 10s, timeout 5s, retries 10, **start\_period 30s** (Unleash runs DB migrations on first boot); network `arc_platform_net` (external); `restart: unless-stopped`
    * **Oracle init SQL** (cross-service, justified): create `services/persistence/initdb/002_create_unleash_db.sql` containing `CREATE DATABASE unleash;`
      * Justification: Oracle (the database service) is the rightful owner of "what databases exist". Mystique declares the need; Oracle satisfies it via initdb. This follows the same pattern as `001_schema_migrations.sql` (created in 005 for Cortex's needs). Not a Principle III violation — initdb is Oracle's extension point for dependent services.
      * This file runs on Oracle's first boot only (Postgres skips initdb if data dir exists). To force re-creation: `make oracle-nuke && make oracle-up`
    * `docker compose config` passes without errors
    * `docker build -f services/flags/Dockerfile services/flags/` succeeds

***

## Phase 3: Make Targets

### Batch A — parallel (each .mk is independent)

* \[x] \[TASK-021] \[P] \[SERVICES] \[P1] Create `services/gateway/heimdall.mk`
  * Dependencies: TASK-011
  * Module: `services/gateway/heimdall.mk`
  * Acceptance:
    * Targets present: `heimdall-help`, `heimdall-build`, `heimdall-build-fresh`, `heimdall-up`, `heimdall-down`, `heimdall-health`, `heimdall-logs`, `heimdall-push`, `heimdall-publish`, `heimdall-tag`, `heimdall-clean`, `heimdall-nuke`
    * `heimdall-health`: probes `curl -sf http://localhost:8090/ping`; exits 0 if healthy, 1 if not
    * `heimdall-clean` / `heimdall-nuke`: require typed confirmation (`yes` / `nuke`) before destructive action
    * `heimdall-publish`: pushes image then prints `https://github.com/orgs/$(ORG)/packages/container/arc-gateway/settings`
    * `make heimdall-help` lists all targets with descriptions; output includes note: "Port 80 binding requires Docker privilege. If startup fails with 'permission denied', change host port in a docker-compose.override.yml"
    * All targets use `COLOR_INFO`, `COLOR_OK`, `COLOR_ERR` from root Makefile
    * `make -n heimdall-up` dry-run passes

* \[x] \[TASK-022] \[P] \[SERVICES] \[P1] Create `services/secrets/nick-fury.mk`
  * Dependencies: TASK-012
  * Module: `services/secrets/nick-fury.mk`
  * Acceptance:
    * Targets present: `nick-fury-help`, `nick-fury-build`, `nick-fury-build-fresh`, `nick-fury-up`, `nick-fury-down`, `nick-fury-health`, `nick-fury-logs`, `nick-fury-push`, `nick-fury-publish`, `nick-fury-tag`, `nick-fury-clean`, `nick-fury-nuke`
    * `nick-fury-health`: probes `curl -sf http://localhost:8200/v1/sys/health`; exits 0 if healthy, 1 if not
    * `nick-fury-nuke`: warns "All secrets will be lost (stateless dev service)" before destructive action; requires typed confirmation
    * `nick-fury-help` output mentions "Dev mode only — all secrets lost on restart"
    * `make nick-fury-help` lists all targets
    * `make -n nick-fury-up` dry-run passes

* \[x] \[TASK-023] \[P] \[SERVICES] \[P1] Create `services/flags/mystique.mk`
  * Dependencies: TASK-013
  * Module: `services/flags/mystique.mk`
  * Acceptance:
    * Targets present: `mystique-help`, `mystique-build`, `mystique-build-fresh`, `mystique-up`, `mystique-down`, `mystique-health`, `mystique-logs`, `mystique-push`, `mystique-publish`, `mystique-tag`, `mystique-clean`, `mystique-nuke`
    * `mystique-health`: probes `curl -sf http://localhost:4242/health`; exits 0 if healthy, 1 if not
    * `mystique-help` output includes note: "Requires arc-sql-db and arc-cache to be running"
    * `mystique-clean` / `mystique-nuke`: require typed confirmation before destructive action
    * `make mystique-help` lists all targets
    * `make -n mystique-up` dry-run passes

### Batch B — sequential (depends on all Batch A)

* \[x] \[TASK-024] \[SERVICES] \[P1] Create `services/control.mk` + update root `Makefile`
  * Dependencies: TASK-021, TASK-022, TASK-023
  * Module: `services/control.mk`, `Makefile`
  * Acceptance:
    * `services/control.mk` targets: `control-help`, `control-up`, `control-down`, `control-health`, `control-logs`
    * `control-up`: calls `docker network create arc_platform_net 2>/dev/null || true`, then `heimdall-up`, `nick-fury-up`, `mystique-up` sequentially
    * `control-down`: calls `mystique-down`, `nick-fury-down`, `heimdall-down` (reverse order)
    * `control-health`: calls all three health targets; exits non-zero if any fails
    * `control-logs`: fans out logs from all three containers with service name prefixes
    * Root `Makefile` additions:
      * `include services/gateway/heimdall.mk`
      * `include services/secrets/nick-fury.mk`
      * `include services/flags/mystique.mk`
      * `include services/control.mk`
    * `publish-all` target updated to include `heimdall-build heimdall-publish`, `nick-fury-build nick-fury-publish`, `mystique-build mystique-publish`
    * `make control-help` lists control-\* targets
    * `make -n control-up` dry-run shows correct chain

***

## Phase 4: CI/CD (parallel)

Both workflows are independent and can be implemented concurrently.

* \[x] \[TASK-031] \[P] \[CI] \[P1] Create `.github/workflows/control-images.yml`
  * Dependencies: TASK-024
  * Module: `.github/workflows/control-images.yml`
  * Acceptance:
    * Mirrors `data-images.yml` structure exactly
    * `on.push.paths`: `services/gateway/**`, `services/secrets/**`, `services/flags/**`, `.github/workflows/control-images.yml`
    * `on.pull_request.paths`: same service paths
    * `on.workflow_dispatch` with `mode` input (`ci` / `release`)
    * `changes` job uses `dorny/paths-filter@v3` with filters `heimdall`, `nick-fury`, `mystique`
    * `build-heimdall`, `build-nick-fury`, `build-mystique` jobs: parallel, each uses `_reusable-build.yml`
      * `platforms: linux/amd64`
      * `service-path: services/gateway` / `services/secrets` / `services/flags`
    * `security-heimdall`, `security-nick-fury`, `security-mystique` jobs: run after respective builds; `block-on-failure: false` in CI
    * YAML parses without errors: `python3 -c "import yaml,sys; yaml.safe_load(open('.github/workflows/control-images.yml'))"` exits 0

* \[x] \[TASK-032] \[P] \[CI] \[P1] Create `.github/workflows/control-release.yml`
  * Dependencies: TASK-024
  * Module: `.github/workflows/control-release.yml`
  * Acceptance:
    * Mirrors `data-release.yml` structure exactly
    * `on.push.tags`: `control/v*`
    * `prepare` job: derives `image-tag` (`control/v0.1.0` → `control-v0.1.0`), `version`, `prerelease` outputs
    * `build-heimdall`, `build-nick-fury`, `build-mystique`: parallel after `prepare`; `platforms: linux/amd64,linux/arm64`; `push-image: true`, `latest-tag: true`, `image-tag: ${{ needs.prepare.outputs.image-tag }}`
    * `security-*` jobs: `block-on-failure: true`; `create-issues: true` (CRITICAL CVEs block release)
    * `release` job: image table (arc-gateway, arc-vault, arc-flags); creates GitHub release via `softprops/action-gh-release@v2`
    * Release notes include `make control-up` / `make control-health` quickstart
    * YAML parses without errors

***

## Phase 5: Integration

* \[x] \[TASK-041] \[SERVICES] \[P1] End-to-end verification — control plane up + health
  * Dependencies: TASK-031, TASK-032
  * Module: `services/gateway/`, `services/secrets/`, `services/flags/`
  * Pre-condition: `arc-sql-db` and `arc-cache` must be running (`make oracle-up sonic-up` first)
  * Acceptance:
    * `docker network create arc_platform_net 2>/dev/null || true` runs without error
    * `make control-up` exits 0; `docker compose ps` shows arc-gateway, arc-vault, arc-flags all in `healthy` state
    * `make control-health` exits 0 (all three health probes pass)
    * `make control-down` exits 0; all three containers stop; no orphaned containers
    * Independent health checks pass: `make heimdall-health`, `make nick-fury-health`, `make mystique-health`
    * `curl -sf http://localhost:8090/ping` returns `OK`
    * Traefik dashboard accessible: `curl -sf http://localhost:8090/dashboard/` returns HTTP 200
    * `curl -H "X-Vault-Token: arc-dev-token" http://localhost:8200/v1/sys/health` returns HTTP 200 with `"sealed":false`
    * `curl http://localhost:4242/health` returns HTTP 200
    * All ports bound to `127.0.0.1`: `docker compose ps` confirms
    * No persistent volumes for Heimdall or Nick Fury: `docker volume ls | grep arc` shows no `arc-gateway-*` or `arc-vault-*`
    * Heimdall uid verified: `docker inspect arc-gateway --format '{{.Config.User}}'` returns `1000:1000` or `1000` (or root documented)
    * `services/persistence/initdb/002_create_unleash_db.sql` exists; Mystique connects to Unleash DB without error

***

## Phase 6: Polish

* \[x] \[TASK-900] \[P] \[DOCS] \[P1] Docs & links update
  * Dependencies: TASK-041
  * Module: `services/profiles.yaml`, `CLAUDE.md`, `.specify/config.yaml`
  * Acceptance:
    * `services/profiles.yaml` `think` profile includes `heimdall` (already done in TASK-001 — verify final state)
    * `services/profiles.yaml` `reason` profile includes `nick-fury` and `mystique`
    * `CLAUDE.md` monorepo layout section references `gateway/`, `secrets/`, `flags/` directories
    * `CLAUDE.md` Service Codenames table updated: Nick Fury row shows `OpenBao` (not `Infisical`)
    * `.specify/config.yaml` `secrets` service entry: `tech` changed from `infisical` to `openbao`
    * No broken internal references in modified files

* \[x] \[TASK-999] \[REVIEW] \[P1] Reviewer agent verification
  * Dependencies: ALL
  * Module: all affected modules
  * Acceptance (full checklist — reviewer runs all items without needing to open plan.md):
    * All tasks TASK-001 through TASK-900 marked `[x]` complete
    * `make control-up` exits 0 (pre-condition: oracle + sonic running via `make oracle-up sonic-up`)
    * `docker compose ps` shows arc-gateway, arc-vault, arc-flags all in `healthy` state
    * `make control-health` exits 0
    * `make control-down` exits 0; no orphaned containers
    * `make heimdall-up && make heimdall-health` works independently
    * `make nick-fury-up && make nick-fury-health` works independently
    * `make mystique-up && make mystique-health` works independently (requires oracle + sonic)
    * `curl http://localhost:8090/ping` returns `OK`
    * `curl http://localhost:8090/dashboard/` returns HTTP 200
    * `curl -H "X-Vault-Token: arc-dev-token" http://localhost:8200/v1/sys/health` returns HTTP 200 with `"sealed":false`
    * `curl http://localhost:4242/health` returns HTTP 200
    * `docker inspect arc-gateway --format '{{.Config.User}}'` confirms uid 1000 (or root deviation documented)
    * `docker inspect arc-flags --format '{{.Config.User}}'` confirms non-root or deviation documented
    * Nick Fury Dockerfile contains comment explaining root deviation + dev-only scope
    * All ports bind `127.0.0.1`: verify with `docker compose ps`
    * No persistent volumes for Heimdall or Nick Fury: `docker volume ls | grep arc` shows no `arc-gateway-*` or `arc-vault-*`
    * `services/profiles.yaml` `think` includes `heimdall`; `reason` includes `nick-fury` + `mystique`
    * `Makefile` includes `heimdall.mk`, `nick-fury.mk`, `mystique.mk`, `control.mk`
    * `publish-all` includes `heimdall-build heimdall-publish`, `nick-fury-build nick-fury-publish`, `mystique-build mystique-publish`
    * `control-images.yml` path filters: `services/gateway/**`, `services/secrets/**`, `services/flags/**`
    * `control-release.yml` tag format `control/v*`; platforms `linux/amd64,linux/arm64`
    * All three Dockerfiles have OCI (`org.opencontainers.*`) + `arc.service.*` labels
    * No plaintext credentials in any compose file (env values are dev defaults, not production secrets)
    * `CLAUDE.md` Service Codenames table: Nick Fury row shows `OpenBao` (not `Infisical`)
    * `.specify/config.yaml` `secrets` service entry: `tech: openbao`
    * `services/persistence/initdb/002_create_unleash_db.sql` exists
    * Constitution compliance verified: II PASS, III PASS, IV PASS, V PASS, VII PASS, VIII PASS†, XI PASS
      * VIII†: Nick Fury root deviation justified (dev-only, documented)

***

## Progress Summary

| Phase | Total | Done | Parallel |
|-------|-------|------|----------|
| Setup | 1 | 1 | 0 |
| Foundational | 3 | 3 | 3 |
| Implementation (Make + CI) | 6 | 6 | 5 |
| Integration | 1 | 1 | 0 |
| Polish | 2 | 2 | 0 |
| **Total** | **13** | **13** | **8** |

---

---
url: /arc-platform/specs-site/007-voice-stack/tasks.md
---
# Tasks: Realtime Voice Infrastructure Setup

> **Spec**: 007-voice-stack
> **Date**: 2026-03-01

## Task Format

```
[TASK-NNN] [P?] [MODULE] [PRIORITY] Description
  Dependencies: [TASK-XXX] or none
  Module: services/{role}
  Acceptance: Testable criteria
  Status: [ ] pending | [~] in-progress | [x] done
```

* `[P]` = Safe for parallel agent execution
* Priority: P1 (must), P2 (should), P3 (nice)

## Dependency Graph

```mermaid
graph TD
    T001["TASK-001\nprofiles.yaml"] --> T011
    T001 --> T012
    T001 --> T013
    T001 --> T014
    T001 --> T015

    T011["TASK-011 [P]\nDockerfile\narc-realtime"]
    T012["TASK-012 [P]\nDockerfile.ingress\narc-realtime-ingress"]
    T013["TASK-013 [P]\nDockerfile.egress\narc-realtime-egress"]
    T014["TASK-014 [P]\nConfig YAMLs\nlivekit / ingress / egress"]
    T015["TASK-015 [P]\nservice.yaml"]

    T011 --> T021
    T012 --> T021
    T013 --> T021
    T014 --> T021
    T015 --> T021

    T021["TASK-021 [P]\ndocker-compose.yml\n(all 3 services)"] --> T031
    T021 --> T022

    T022["TASK-022 [P]\nrealtime.mk"] --> T031

    T031["TASK-031\nMakefile include\n+ scripts updates"] --> T041
    T031 --> T042

    T041["TASK-041 [P]\nrealtime-images.yml"] --> T051
    T042["TASK-042 [P]\nrealtime-release.yml"] --> T051

    T051["TASK-051\nE2E verification"] --> T900
    T051 --> T999

    T900["TASK-900\nDocs & links"] --> T999
    T999["TASK-999\nReviewer"]
```

## Quality Requirements

| Module | Coverage | Lint | Notes |
|--------|----------|------|-------|
| Config/YAML | n/a | `docker compose config` (no syntax errors) | |
| Makefile | n/a | `make -n {target}` dry-run passes | |
| CI/CD YAML | n/a | `python3 -c "import yaml; yaml.safe_load(open(...))"` exits 0 | |

***

## Phase 1: Setup

* \[x] \[TASK-001] \[SERVICES] \[P1] Update `services/profiles.yaml` — add `realtime` to `think` and `reason`
  * Dependencies: none
  * Module: `services/profiles.yaml`
  * Acceptance:
    * Python YAML validation confirms structure:
      ```
      python3 -c "
      import yaml
      p = yaml.safe_load(open('services/profiles.yaml'))
      assert 'realtime' in p['think']['services'], 'realtime missing from think'
      assert 'realtime' in p['reason']['services'], 'realtime missing from reason'
      assert p['ultra-instinct']['services'] == '*', 'ultra-instinct changed'
      print('profiles.yaml OK')
      "
      ```
    * `ultra-instinct` remains `'*'` (unchanged — wildcard already includes realtime)
    * All existing entries in `think` and `reason` preserved (no accidental removal)
  * Status: \[x]

***

## Phase 2: Core Files (fully parallel)

All five tasks are independent — different files in the same directory. Run concurrently.

* \[x] \[TASK-011] \[P] \[SERVICES] \[P1] Create `services/realtime/Dockerfile` — arc-realtime (LiveKit Server)
  * Dependencies: TASK-001
  * Module: `services/realtime/Dockerfile`
  * Acceptance:
    * `FROM livekit/livekit-server`
    * OCI labels: `org.opencontainers.image.title`, `org.opencontainers.image.description`, `org.opencontainers.image.source=https://github.com/arc-framework/arc-platform`
    * Arc service labels: `arc.service.name=arc-realtime`, `arc.service.codename=daredevil`, `arc.service.tech=livekit`, `arc.service.role=realtime`
    * Run `docker run --rm --entrypoint id livekit/livekit-server` to verify user:
      * If non-root → add `USER` directive for explicit declaration
      * If root → add comment: `# livekit/livekit-server runs as root by default. This is a DEVELOPMENT-ONLY service. For production: build with non-root user.`
    * `docker build -f services/realtime/Dockerfile services/realtime/` succeeds locally
  * Status: \[x]

* \[x] \[TASK-012] \[P] \[SERVICES] \[P1] Create `services/realtime/Dockerfile.ingress` — arc-realtime-ingress (LK Ingress)
  * Dependencies: TASK-001
  * Module: `services/realtime/Dockerfile.ingress`
  * Acceptance:
    * `FROM livekit/ingress`
    * OCI labels: `org.opencontainers.image.title`, `org.opencontainers.image.description`, `org.opencontainers.image.source`
    * Arc service labels: `arc.service.name=arc-realtime-ingress`, `arc.service.codename=sentry`, `arc.service.tech=livekit-ingress`, `arc.service.role=realtime-ingress`
    * Non-root handling: same verify-and-document pattern as TASK-011
    * `docker build -f services/realtime/Dockerfile.ingress services/realtime/` succeeds locally
  * Status: \[x]

* \[x] \[TASK-013] \[P] \[SERVICES] \[P1] Create `services/realtime/Dockerfile.egress` — arc-realtime-egress (LK Egress)
  * Dependencies: TASK-001
  * Module: `services/realtime/Dockerfile.egress`
  * Acceptance:
    * `FROM livekit/egress`
    * OCI labels: `org.opencontainers.image.title`, `org.opencontainers.image.description`, `org.opencontainers.image.source`
    * Arc service labels: `arc.service.name=arc-realtime-egress`, `arc.service.codename=scribe`, `arc.service.tech=livekit-egress`, `arc.service.role=realtime-egress`
    * Non-root handling: same verify-and-document pattern as TASK-011
    * Dockerfile comment: `# NOTE: livekit/egress bundles headless Chromium (~1GB). Track-based egress (no Chrome) is a future optimization (see plan.md TD-005).`
    * `docker build -f services/realtime/Dockerfile.egress services/realtime/` succeeds locally
  * Status: \[x]

* \[x] \[TASK-014] \[P] \[SERVICES] \[P1] Create `services/realtime/livekit.yaml`, `ingress.yaml`, `egress.yaml`
  * Dependencies: TASK-001
  * Module: `services/realtime/`
  * Acceptance:
    * `livekit.yaml`:
      * `port: 7880`, `grpc_port: 7881`
      * `rtc.tcp_port: 7882`, `rtc.udp_port_range_start: 50100`, `rtc.udp_port_range_end: 50200`
      * `rtc.use_external_ip: false`
      * `rtc.node_ip` sourced from env (LiveKit supports `${LIVEKIT_NODE_IP}` or compose env injection)
      * `redis.address: arc-cache:6379`
      * `keys:` block with `devkey: devsecret`
      * Comment: `# API keys are static dev values. Production: replace with dynamic secrets from OpenBao.`
    * `ingress.yaml`:
      * `api_url: ws://arc-realtime:7880`
      * `api_key: devkey`, `api_secret: devsecret`
      * Health port: 7888 (either via config key or LK Ingress env `HEALTH_PORT=7888`)
    * `egress.yaml`:
      * `api_url: ws://arc-realtime:7880`
      * `api_key: devkey`, `api_secret: devsecret`
      * S3/MinIO output config pointing to `http://arc-storage:9000`, bucket `recordings`, credentials `arc`/`arc-minio-dev`, `force_path_style: true`
      * Comment: `# 'recordings' bucket must exist in arc-storage. Run: docker exec arc-storage mc mb /data/recordings`
    * All three files are valid YAML: `python3 -c "import yaml; [yaml.safe_load(open(f)) for f in ['services/realtime/livekit.yaml','services/realtime/ingress.yaml','services/realtime/egress.yaml']]"` exits 0
  * Status: \[x]

* \[x] \[TASK-015] \[P] \[SERVICES] \[P1] Create `services/realtime/service.yaml`
  * Dependencies: TASK-001
  * Module: `services/realtime/service.yaml`
  * Acceptance:
    * Fields present: `name: arc-realtime`, `codename: daredevil`, `role: realtime`, `image: ghcr.io/arc-framework/arc-realtime:latest`, `tech: livekit`, `upstream: livekit/livekit-server`
    * `ports` list includes: 7880 (http, LiveKit API + WebRTC signalling), 7881 (grpc, gRPC API), 7882 (tcp, TURN), 50100-50200 (udp, WebRTC RTP media)
    * `health.endpoint: http://localhost:7880`
    * `depends_on: [cache]`
    * `sidecars` list includes:
      * `{ name: arc-realtime-ingress, codename: sentry, port: 7888, description: "RTMP ingest" }`
      * `{ name: arc-realtime-egress, codename: scribe, port: 7889, description: "Recording + export" }`
    * Valid YAML: `python3 -c "import yaml; yaml.safe_load(open('services/realtime/service.yaml'))"` exits 0
  * Status: \[x]

***

## Phase 3: Assembly

Both compose and .mk can be written in parallel (different files).

* \[x] \[TASK-021] \[P] \[SERVICES] \[P1] Create `services/realtime/docker-compose.yml` — all 3 services
  * Dependencies: TASK-011, TASK-012, TASK-013, TASK-014
  * Module: `services/realtime/docker-compose.yml`
  * Acceptance:
    * **arc-realtime** service:
      * `image: ghcr.io/arc-framework/arc-realtime:latest`; `build: {context: services/realtime, dockerfile: Dockerfile}`
      * `command: --config /etc/livekit.yaml`
      * Volume: `./services/realtime/livekit.yaml:/etc/livekit.yaml:ro`
      * `environment: LIVEKIT_NODE_IP: ${LIVEKIT_NODE_IP:-127.0.0.1}`
      * Ports: `127.0.0.1:7880:7880`, `127.0.0.1:7881:7881`, `127.0.0.1:7882:7882`, `0.0.0.0:50100-50200:50100-50200/udp`
      * Port comment: `# UDP 0.0.0.0 — WebRTC RTP media requires host-reachable port range for NAT traversal`
      * `depends_on: {arc-cache: {condition: service_healthy}}`
      * Healthcheck: `wget -qO- http://localhost:7880 || exit 1` (fallback: `curl -sf http://localhost:7880`); interval 5s, timeout 3s, retries 5, start\_period 10s
      * `restart: unless-stopped`
    * **arc-realtime-ingress** service:
      * `image: ghcr.io/arc-framework/arc-realtime-ingress:latest`; dockerfile: `Dockerfile.ingress`
      * Volume: `./services/realtime/ingress.yaml:/etc/ingress.yaml:ro`
      * Ports: `127.0.0.1:7888:7888`, `127.0.0.1:1935:1935`
      * `depends_on: {arc-realtime: {condition: service_healthy}}`
      * Healthcheck: `:7888`; interval 5s, timeout 3s, retries 5, start\_period 10s
    * **arc-realtime-egress** service:
      * `image: ghcr.io/arc-framework/arc-realtime-egress:latest`; dockerfile: `Dockerfile.egress`
      * Volume: `./services/realtime/egress.yaml:/etc/egress.yaml:ro`
      * Ports: `127.0.0.1:7889:7889`
      * `depends_on: {arc-realtime: {condition: service_healthy}}`
      * Healthcheck: `:7889`; interval 5s, timeout 3s, retries 5, start\_period 15s
    * `networks: {arc_platform_net: {external: true}}` at file bottom
    * `docker compose -f services/realtime/docker-compose.yml config` passes without errors
  * **Deviations (documented):**
    * `depends_on: arc-cache` omitted — Docker Compose v2 cannot express `condition: service_healthy`
      across separate compose files. The `arc-cache` service lives in `services/cache/docker-compose.yml`.
      Cross-compose ordering is delegated to the `.mk` layer: `make realtime-up` must be called after
      `make cache-up` (enforced by `SERVICE_realtime_DEPENDS := cache` in registry.mk).
      Comment added to compose file documents this: `# arc-cache (Redis) must be running for multi-node pub/sub.`
    * Healthchecks use `nc -z localhost <port>` instead of `wget`/`curl` — LiveKit images are Alpine-based
      with busybox netcat but no wget/curl in PATH. `nc -z` is more reliable for TCP port-open checks.
    * Retries increased from 5 → 10 (LK Server), start\_period from 10s/15s → 15s/20s — LiveKit Server
      and Egress (Chromium bundle ~1GB) have slow cold-start; higher values prevent false-unhealthy on
      first launch. Conservative values; can be tuned down once baseline startup time is measured.
    * `NODE_IP` env var name used in compose (matching LiveKit docs); spec said `LIVEKIT_NODE_IP`.
      Both work — the compose file maps `NODE_IP: ${LIVEKIT_NODE_IP:-127.0.0.1}`, so the override env
      var is still `LIVEKIT_NODE_IP` as documented in `realtime-help`.
    * `ws_url` used in ingress.yaml/egress.yaml instead of `api_url` — `ws_url` is the correct LiveKit
      SDK config key for the WebSocket connection URL. Spec used `api_url` which is not a valid LK field.
  * Status: \[x]

* \[x] \[TASK-022] \[P] \[SERVICES] \[P1] Create `services/realtime/realtime.mk`
  * Dependencies: TASK-021
  * Module: `services/realtime/realtime.mk`
  * Acceptance:
    * Image variables declared: `REALTIME_IMAGE`, `REALTIME_INGRESS_IMG`, `REALTIME_EGRESS_IMG`
    * `COMPOSE_REALTIME := docker compose -f services/realtime/docker-compose.yml`
    * **Core targets**: `realtime-help`, `realtime-up`, `realtime-down`, `realtime-health`, `realtime-logs`, `realtime-clean`, `realtime-nuke`
    * **Build targets**: `realtime-build` (Dockerfile), `realtime-build-fresh`, `realtime-ingress-build` (Dockerfile.ingress), `realtime-ingress-build-fresh`, `realtime-egress-build` (Dockerfile.egress), `realtime-egress-build-fresh`
    * **Push/publish targets**: `realtime-push`, `realtime-publish`, `realtime-tag`; `realtime-ingress-push`, `realtime-ingress-publish`; `realtime-egress-push`, `realtime-egress-publish`
    * **Individual service targets**: `realtime-ingress-up`, `realtime-ingress-down`, `realtime-ingress-health`, `realtime-ingress-logs`; `realtime-egress-up`, `realtime-egress-down`, `realtime-egress-health`, `realtime-egress-logs`
    * `realtime-up`: calls `docker network create arc_platform_net 2>/dev/null || true` before compose up
    * `realtime-health`: probes all three — `curl -sf http://localhost:7880`, `curl -sf http://localhost:7888`, `curl -sf http://localhost:7889`; exits 0 only if all pass
    * `realtime-ingress-health`: probes `:7888`; `realtime-egress-health`: probes `:7889`
    * `realtime-help`: lists all targets; includes notes: "UDP 50100-50200 must be open on host firewall for WebRTC" and "Set LIVEKIT\_NODE\_IP= for non-local clients"
    * `realtime-clean` / `realtime-nuke`: confirm before destructive action; nuke warns "active WebRTC rooms will be disconnected"
    * `make -n realtime-up` dry-run passes
    * `make -n realtime-health` dry-run passes
    * All targets declare `.PHONY`
    * All targets use `COLOR_INFO`, `COLOR_OK`, `COLOR_ERR` from root Makefile
  * Status: \[x]

***

## Phase 4: Wiring

* \[x] \[TASK-031] \[SERVICES] \[P1] Update root `Makefile`, `scripts/scripts.mk`, and `scripts/lib/check-dev-prereqs.sh`
  * Dependencies: TASK-022
  * Module: `Makefile`, `scripts/scripts.mk`, `scripts/lib/check-dev-prereqs.sh`
  * Acceptance:
    * Root `Makefile`: `include services/realtime/realtime.mk` added to Services section (after control.mk include)
    * `scripts/scripts.mk` `publish-all` target: three new lines added:
      ```makefile
      $(MAKE) realtime-build         realtime-publish         --no-print-directory
      $(MAKE) realtime-ingress-build realtime-ingress-publish --no-print-directory
      $(MAKE) realtime-egress-build  realtime-egress-publish  --no-print-directory
      ```
    * `scripts/lib/check-dev-prereqs.sh` `port_to_service()` function: add port mappings:
      * `7880` → `daredevil` (or equivalent realtime service label)
      * `7881` → `daredevil-grpc`
      * `1935` → `sentry-rtmp`
    * `make -n publish-all` dry-run includes realtime build/publish steps
    * `make help` lists `realtime-help`
  * Status: \[x]

***

## Phase 5: CI/CD (parallel)

Both workflows are independent and can be implemented concurrently.

* \[x] \[TASK-041] \[P] \[CI] \[P2] Create `.github/workflows/realtime-images.yml`
  * Dependencies: TASK-031
  * Module: `.github/workflows/realtime-images.yml`
  * Acceptance:
    * Mirrors `control-images.yml` / `data-images.yml` structure exactly
    * `on.push.branches: [main]` + `on.push.paths: ["services/realtime/**", ".github/workflows/realtime-images.yml"]`
    * `on.pull_request.paths`: same path
    * `on.workflow_dispatch` with `mode` input (`ci` / `release`)
    * `changes` job: `dorny/paths-filter@v3` with filter `realtime: services/realtime/**`
    * `build-realtime`, `build-realtime-ingress`, `build-realtime-egress` jobs:
      * Parallel; each uses `_reusable-build.yml`
      * `platforms: linux/amd64` (amd64-only in CI — no QEMU for speed)
      * `service-path: services/realtime`; `dockerfile: Dockerfile` / `Dockerfile.ingress` / `Dockerfile.egress`
      * `image-name: arc-realtime` / `arc-realtime-ingress` / `arc-realtime-egress`
    * `security-*` jobs after each build; `block-on-failure: false` in CI
    * YAML parses: `python3 -c "import yaml,sys; yaml.safe_load(open('.github/workflows/realtime-images.yml'))"` exits 0
    * Three images built: `arc-realtime`, `arc-realtime-ingress`, `arc-realtime-egress`
  * Status: \[x]

* \[x] \[TASK-042] \[P] \[CI] \[P2] Create `.github/workflows/realtime-release.yml`
  * Dependencies: TASK-031
  * Module: `.github/workflows/realtime-release.yml`
  * Acceptance:
    * Mirrors `control-release.yml` / `data-release.yml` structure exactly
    * `on.push.tags: ["realtime/v*"]`
    * `prepare` job: derives `image-tag` (`realtime/v0.1.0` → `realtime-v0.1.0`), `version`, `prerelease` outputs
    * `build-realtime`, `build-realtime-ingress`, `build-realtime-egress` jobs: parallel after `prepare`
      * `platforms: linux/amd64,linux/arm64`; `push-image: true`; `latest-tag: true`; `image-tag: ${{ needs.prepare.outputs.image-tag }}`
    * `security-*` jobs: `block-on-failure: true`; `create-issues: true` (CRITICAL CVEs block release)
    * `release` job: image table listing `arc-realtime`, `arc-realtime-ingress`, `arc-realtime-egress` with GHCR links; creates GitHub release via `softprops/action-gh-release@v2`
    * Release notes include: `make realtime-up` / `make realtime-health` quickstart; `LIVEKIT_NODE_IP` env doc
    * YAML parses without errors
  * Status: \[x]

***

## Phase 6: Integration

* \[x] \[TASK-051] \[SERVICES] \[P1] End-to-end verification — realtime stack up + health
  * Dependencies: TASK-041, TASK-042
  * Module: `services/realtime/`
  * Pre-condition: `arc-cache` must be running (`make cache-up` first)
  * Acceptance:
    * `docker network create arc_platform_net 2>/dev/null || true` runs without error
    * `make realtime-up` exits 0
    * `docker compose -f services/realtime/docker-compose.yml ps` shows all three containers in `healthy` state: `arc-realtime`, `arc-realtime-ingress`, `arc-realtime-egress`
    * `make realtime-health` exits 0 (all three probes pass)
    * `curl -s http://localhost:7880` returns a LiveKit response (2xx or redirect)
    * `curl -sf http://localhost:7888` returns HTTP 200 (Ingress controller alive)
    * `curl -sf http://localhost:7889` returns HTTP 200 (Egress controller alive)
    * TCP ports 7880/7881/7882/7888/1935/7889 bind `127.0.0.1` — confirm with `docker compose -f services/realtime/docker-compose.yml ps`
    * UDP 50100-50200 binds `0.0.0.0` — confirm and document exception is intentional
    * Config files mounted read-only: `docker inspect arc-realtime --format '{{json .Mounts}}'` shows `/etc/livekit.yaml` with `RW:false`
    * `make realtime-down` exits 0; no orphaned containers
    * Individual targets work: `make realtime-ingress-health` exits 0; `make realtime-egress-health` exits 0
    * `make dev PROFILE=think` starts `realtime` along with other think-profile services
  * Status: \[x]

***

## Phase 7: Polish

* \[x] \[TASK-900] \[P] \[DOCS] \[P1] Docs & links update
  * Dependencies: TASK-051
  * Module: `CLAUDE.md`, `.specify/config.yaml`, `services/profiles.yaml`
  * Acceptance:
    * `CLAUDE.md` monorepo layout: `services/realtime/` entry added (Daredevil/Sentry/Scribe LiveKit)
    * `CLAUDE.md` Service Codenames table: three new rows added:
      * Daredevil | LiveKit Server (realtime)
      * Sentry | LiveKit Ingress (realtime-ingress)
      * Scribe | LiveKit Egress (realtime-egress)
    * `.specify/config.yaml` `services` list: `realtime` entry updated to `{ dir: "realtime", codename: "daredevil", tech: "livekit", lang: "config" }`; confirm `realtime-ingress` (sentry) and `realtime-egress` (scribe) entries added if needed
    * `services/profiles.yaml` `think` includes `realtime`; `reason` includes `realtime` (verify final state from TASK-001)
    * No broken internal references in modified files
  * Status: \[x]

* \[x] \[TASK-999] \[REVIEW] \[P1] Reviewer agent verification
  * Dependencies: ALL
  * Module: all affected modules
  * Acceptance (reviewer runs all items without opening plan.md):
    * All tasks TASK-001 through TASK-900 marked `[x]` complete
    * **Stack health** (pre-condition: `make cache-up`):
      * `make realtime-up` exits 0
      * `docker compose -f services/realtime/docker-compose.yml ps` shows all three containers `healthy`
      * `make realtime-health` exits 0
      * `make realtime-down` exits 0; no orphaned containers
    * **Individual service health**:
      * `curl -s http://localhost:7880` → LiveKit response
      * `curl -sf http://localhost:7888` → HTTP 200 (Ingress)
      * `curl -sf http://localhost:7889` → HTTP 200 (Egress)
    * **Port binding**:
      * TCP ports bind `127.0.0.1` only — verified via `docker compose ps`
      * UDP 50100-50200 binds `0.0.0.0` — documented as intentional WebRTC exception
    * **Config mounts**: livekit.yaml, ingress.yaml, egress.yaml all mounted read-only
    * **API key consistency**: `devkey`/`devsecret` identical across all three config files
    * **Dockerfiles**:
      * All three have OCI (`org.opencontainers.*`) + `arc.service.*` labels including `arc.service.codename`
      * Non-root status verified per image; root deviation documented if required
      * Egress Dockerfile has note about Chromium bundle
    * **service.yaml**: contains role, codename, image, ports, health, depends\_on, sidecars
    * **realtime.mk**:
      * `make -n realtime-up` dry-run passes
      * `make -n realtime-health` dry-run passes
      * `make realtime-help` includes `LIVEKIT_NODE_IP` and UDP firewall notes
    * **Makefile**: `include services/realtime/realtime.mk` present
    * **scripts/scripts.mk**: `publish-all` includes all three realtime build/publish steps
    * **check-dev-prereqs.sh**: ports 7880, 7881, 1935 added
    * **profiles.yaml**: `think` and `reason` both include `realtime`; `ultra-instinct` unchanged (`'*'`)
    * **CI**:
      * `realtime-images.yml` path filter `services/realtime/**`; 3 image builds; amd64 only
      * `realtime-release.yml` tag format `realtime/v*`; platforms `linux/amd64,linux/arm64`
    * **Docs**: CLAUDE.md codenames table has Daredevil/Sentry/Scribe; layout has `services/realtime/`
    * **Constitution compliance verified**:
      * II PASS: `make realtime-up` boots all three; joined to `think` (minimal profile)
      * III PASS: self-contained in `services/realtime/`; own service.yaml + compose + .mk
      * IV PASS: config-only upstream images, no custom Python/Go code
      * V PASS: same Dockerfile/compose/healthcheck/CI structure as 003/005/006
      * VII PASS: HTTP health endpoints :7880/:7888/:7889 with Docker healthchecks
      * VIII PASS†: TCP 127.0.0.1; UDP 0.0.0.0 exception documented; no secrets in git
      * XI PASS: depends\_on ordering (cache → realtime → ingress/egress); start\_periods set
    * No TODO/FIXME in any file without a corresponding plan.md TD-\* entry
  * Status: \[x]

***

## Progress Summary

| Phase | Total | Done | Parallel |
|-------|-------|------|----------|
| Setup | 1 | 1 | 0 |
| Core Files | 5 | 5 | 5 |
| Assembly | 2 | 2 | 2 |
| Wiring | 1 | 1 | 0 |
| CI/CD | 2 | 2 | 2 |
| Integration | 1 | 1 | 0 |
| Polish | 2 | 2 | 0 |
| **Total** | **14** | **14** | **9** |

---

---
url: /arc-platform/specs-site/008-specs-site/tasks.md
---
# Tasks: Specs Documentation Site

> **Spec**: 008-specs-site
> **Date**: 2026-03-01

## Dependency Graph

```mermaid
graph TD
    T001["TASK-001\nSetup: .gitignore + CLAUDE.md"]

    T010["TASK-010 [P]\nmkdocs.yml"]
    T011["TASK-011 [P]\nindex.md"]
    T012["TASK-012 [P]\nrequirements.txt"]
    T013["TASK-013 [P]\nspecs/.pages (root)"]
    T014["TASK-014 [P]\n001-otel-setup/.pages"]
    T015["TASK-015 [P]\n002-cortex-setup/.pages"]
    T016["TASK-016 [P]\n003-messaging-setup/.pages"]
    T017["TASK-017 [P]\n004-dev-setup/.pages"]
    T018["TASK-018 [P]\n005-data-layer/.pages"]
    T019["TASK-019 [P]\n006-platform-control/.pages"]
    T020["TASK-020 [P]\n007-voice-stack/.pages"]
    T021["TASK-021 [P]\ndeploy-specs-site.yml"]

    T030["TASK-030\nmkdocs build verification"]

    T900["TASK-900\nDocs & links update"]
    T999["TASK-999\nReviewer agent"]

    T001 --> T010
    T001 --> T011
    T001 --> T012
    T001 --> T013
    T001 --> T014
    T001 --> T015
    T001 --> T016
    T001 --> T017
    T001 --> T018
    T001 --> T019
    T001 --> T020
    T001 --> T021

    T010 --> T030
    T011 --> T030
    T012 --> T030
    T013 --> T030
    T014 --> T030
    T015 --> T030
    T016 --> T030
    T017 --> T030
    T018 --> T030
    T019 --> T030
    T020 --> T030
    T021 --> T030

    T030 --> T900
    T900 --> T999
```

***

## Phase 1: Setup

* \[x] \[TASK-001] \[ROOT] \[P1] Update `.gitignore` and `CLAUDE.md`
  * Dependencies: none
  * Module: `.gitignore`, `CLAUDE.md`
  * Acceptance:
    * `.gitignore` contains `site_build/` line (FR-11)
    * `CLAUDE.md` Commands section contains `mkdocs serve -f docs/specs/mkdocs.yml  # preview specs site locally` (FR-12)

***

## Phase 2: Implementation (Fully Parallel)

All tasks in this phase are independent — different files, no shared state.

### Batch A — MkDocs Config Files

* \[x] \[TASK-010] \[P] \[DOCS] \[P1] Create `docs/specs/mkdocs.yml`
  * Dependencies: TASK-001
  * Module: `docs/specs/mkdocs.yml`
  * Acceptance:
    * `site_name: "A.R.C. Platform — Specs"` (FR-1, FR-10)
    * `site_url: "https://arc-framework.github.io/arc-platform/specs-site/"` (FR-10)
    * `docs_dir: "../../specs/"` (FR-1)
    * `site_dir: "../../site_build/"` (FR-1)
    * `theme: material` with two-palette dark/light toggle (FR-7)
    * `features: [navigation.instant, navigation.sections, search.highlight, search.suggest, content.action.edit]`
    * `pymdownx.superfences` with mermaid custom fence (FR-5)
    * `plugins: [search, awesome-pages]` (FR-1, FR-6)
    * `extra_javascript: [https://unpkg.com/mermaid@10/dist/mermaid.min.js]` (FR-5)
    * `edit_uri: "edit/main/specs/"` (FR-8)

* \[x] \[TASK-011] \[P] \[DOCS] \[P1] Create `docs/specs/index.md`
  * Dependencies: TASK-001
  * Module: `docs/specs/index.md`
  * Acceptance:
    * Landing page with project overview paragraph (FR-2)
    * Spec index table: columns = Feature, Name, Status, Link — 7 rows for 001–007 (FR-2)
    * Local dev instructions section with `pip install` + `mkdocs serve` commands (FR-2)

* \[x] \[TASK-012] \[P] \[DOCS] \[P1] Create `docs/specs/requirements.txt`
  * Dependencies: TASK-001
  * Module: `docs/specs/requirements.txt`
  * Acceptance:
    * `mkdocs-material>=9.5` (FR-3)
    * `mkdocs-awesome-pages-plugin>=2.9` (FR-3)
    * `pymdown-extensions>=10.0` (FR-3)
    * Comment line documenting Python 3.10+ requirement (FR-3)

### Batch B — `.pages` Nav Files

* \[x] \[TASK-013] \[P] \[SPECS] \[P1] Create `specs/.pages` (root nav order)
  * Dependencies: TASK-001
  * Module: `specs/.pages`
  * Acceptance:
    * `nav:` lists `index.md` first, then `001-otel-setup` through `007-voice-stack` in order (FR-4)

* \[x] \[TASK-014] \[P] \[SPECS] \[P1] Create `specs/001-otel-setup/.pages`
  * Dependencies: TASK-001
  * Module: `specs/001-otel-setup/.pages`
  * Acceptance:
    * `title: "001 — OTEL Setup"` (FR-4)
    * `nav:` lists only files that exist: `spec.md`, `plan.md`, `tasks.md`, `pr-description.md`

* \[x] \[TASK-015] \[P] \[SPECS] \[P1] Create `specs/002-cortex-setup/.pages`
  * Dependencies: TASK-001
  * Module: `specs/002-cortex-setup/.pages`
  * Acceptance:
    * `title: "002 — Cortex Setup"` (FR-4)
    * `nav:` lists only files that exist: `spec.md`, `plan.md`, `tasks.md`, `pr-description.md`

* \[x] \[TASK-016] \[P] \[SPECS] \[P1] Create `specs/003-messaging-setup/.pages`
  * Dependencies: TASK-001
  * Module: `specs/003-messaging-setup/.pages`
  * Acceptance:
    * `title: "003 — Messaging Setup"` (FR-4)
    * `nav:` lists only files that exist: `spec.md`, `plan.md`, `tasks.md`, `pr-description.md`

* \[x] \[TASK-017] \[P] \[SPECS] \[P1] Create `specs/004-dev-setup/.pages`
  * Dependencies: TASK-001
  * Module: `specs/004-dev-setup/.pages`
  * Acceptance:
    * `title: "004 — Dev Setup"` (FR-4)
    * `nav:` lists only files that exist: `spec.md`, `plan.md`, `tasks.md`, `analysis-report.md`

* \[x] \[TASK-018] \[P] \[SPECS] \[P1] Create `specs/005-data-layer/.pages`
  * Dependencies: TASK-001
  * Module: `specs/005-data-layer/.pages`
  * Acceptance:
    * `title: "005 — Data Layer"` (FR-4)
    * `nav:` lists only files that exist: `spec.md`, `plan.md`, `tasks.md`, `analysis-report.md`, `pr-description.md`

* \[x] \[TASK-019] \[P] \[SPECS] \[P1] Create `specs/006-platform-control/.pages`
  * Dependencies: TASK-001
  * Module: `specs/006-platform-control/.pages`
  * Acceptance:
    * `title: "006 — Platform Control"` (FR-4)
    * `nav:` lists only files that exist: `spec.md`, `plan.md`, `tasks.md`, `analysis-report.md`, `pr-description.md`

* \[x] \[TASK-020] \[P] \[SPECS] \[P1] Create `specs/007-voice-stack/.pages`
  * Dependencies: TASK-001
  * Module: `specs/007-voice-stack/.pages`
  * Acceptance:
    * `title: "007 — Voice Stack"` (FR-4)
    * `nav:` lists only files that exist: `spec.md`, `plan.md`, `tasks.md`, `analysis-report.md`, `pr-description.md`

### Batch C — CI Workflow

* \[x] \[TASK-021] \[P] \[CI] \[P1] Create `.github/workflows/deploy-specs-site.yml`
  * Dependencies: TASK-001
  * Module: `.github/workflows/deploy-specs-site.yml`
  * Acceptance:
    * Triggers on `push` to `main` with path filters `specs/**` and `docs/specs/**` (FR-9)
    * Job uses `ubuntu-latest`, `permissions: contents: write` (FR-9, NFR-5)
    * Steps: `actions/checkout@v4` → `actions/setup-python@v5` (Python 3.12) → `pip install -r docs/specs/requirements.txt` → `mkdocs gh-deploy --force --config-file docs/specs/mkdocs.yml` (FR-9)

***

## Phase 3: Verification

* \[x] \[TASK-030] \[ROOT] \[P1] Run `mkdocs build` and verify output
  * Dependencies: TASK-010, TASK-011, TASK-012, TASK-013, TASK-014, TASK-015, TASK-016, TASK-017, TASK-018, TASK-019, TASK-020, TASK-021
  * Module: repo root
  * Acceptance:
    * `pip install -r docs/specs/requirements.txt` completes without error (NFR-1)
    * `mkdocs build -f docs/specs/mkdocs.yml` exits 0 from repo root (SC-1)
    * `site_build/` directory created and gitignored (SC-1, FR-11)
    * `mkdocs build --strict -f docs/specs/mkdocs.yml` exits 0 — no broken links or nav warnings (SC-9)
    * All 7 spec sections visible in `site_build/` output directory

***

## Phase 4: Polish

* \[x] \[TASK-900] \[DOCS] \[P1] Docs & links update
  * Dependencies: TASK-030
  * Module: `specs/008-specs-site/spec.md`
  * Acceptance:
    * `spec.md` FR and SC checkboxes reflect implementation state
    * `specs/008-specs-site/.pages` file created with correct nav (this spec's own nav file)

* \[x] \[TASK-999] \[REVIEW] \[P1] Reviewer agent verification
  * Dependencies: ALL
  * Module: all affected modules
  * Acceptance:
    * All 14 tasks above marked `[x]` complete
    * `mkdocs build --strict` exits 0 (no warnings or errors)
    * `site_build/` produced and not committed (gitignored)
    * All 7 spec folders have `.pages` files with human-readable titles
    * Root `specs/.pages` lists all 7 folders in order
    * `docs/specs/requirements.txt` pins all three dependencies
    * `docs/specs/index.md` has spec index table with all 7 specs
    * `deploy-specs-site.yml` has correct path filters and `contents: write` permission
    * `.gitignore` contains `site_build/`
    * `CLAUDE.md` contains `mkdocs serve` command
    * Constitution V (Polyglot) and VI (Local-First) compliant
    * No TODO/FIXME without tracking

***

## Progress Summary

| Phase | Total | Done | Parallel |
|-------|-------|------|----------|
| Setup | 1 | 1 | 0 |
| Implementation | 12 | 12 | 12 |
| Verification | 1 | 1 | 0 |
| Polish | 2 | 2 | 0 |
| **Total** | **16** | **16** | **12** |
